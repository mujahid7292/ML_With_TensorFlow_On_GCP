{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enable Virtual Environment For This Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will go to the location of the directory, where we will create our virtual environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>`$ cd /media/mujahid7292/Data/GoogleDriveSandCorp2014/ML_With_TensorFlow_On_GCP/06. End_To_End_ML_With_TensorFlow_On_GCP/Week_2/Lab_3/Practice`</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deactivate conda environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>`$ conda deactivate`</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activate newly created virtual environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>`$ source Venv/bin/activate`</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook <a href=\"https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/courses/machine_learning/deepdive/06_structured/3_tensorflow_wd.ipynb\">Link</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Necessary import of python package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/mujahid7292/Data/GoogleDriveSandCorp2014/ML_With_TensorFlow_On_GCP/06. End_To_End_ML_With_TensorFlow_On_GCP/Week_2/Lab_3/Practice/Venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/media/mujahid7292/Data/GoogleDriveSandCorp2014/ML_With_TensorFlow_On_GCP/06. End_To_End_ML_With_TensorFlow_On_GCP/Week_2/Lab_3/Practice/Venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/media/mujahid7292/Data/GoogleDriveSandCorp2014/ML_With_TensorFlow_On_GCP/06. End_To_End_ML_With_TensorFlow_On_GCP/Week_2/Lab_3/Practice/Venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/media/mujahid7292/Data/GoogleDriveSandCorp2014/ML_With_TensorFlow_On_GCP/06. End_To_End_ML_With_TensorFlow_On_GCP/Week_2/Lab_3/Practice/Venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:522: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/media/mujahid7292/Data/GoogleDriveSandCorp2014/ML_With_TensorFlow_On_GCP/06. End_To_End_ML_With_TensorFlow_On_GCP/Week_2/Lab_3/Practice/Venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/media/mujahid7292/Data/GoogleDriveSandCorp2014/ML_With_TensorFlow_On_GCP/06. End_To_End_ML_With_TensorFlow_On_GCP/Week_2/Lab_3/Practice/Venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard module is not an IPython extension.\n",
      "1.8.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change these to try this notebook out\n",
    "ACCOUNT = 'student-03-ec074aee9d21@qwiklabs.net'\n",
    "SAC = 'jupyter-notebook-sac-d'\n",
    "SAC_KEY_DESTINATION = '/media/mujahid7292/Data/Gcloud_Tem_SAC'\n",
    "PROJECT = 'qwiklabs-gcp-03-d8cb7ba1a6f4'\n",
    "BUCKET = 'bucket-qwiklabs-gcp-03-d8cb7ba1a6f4'\n",
    "REGION = 'us-central1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bash Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['ACCOUNT'] = ACCOUNT\n",
    "os.environ['SAC'] = SAC\n",
    "os.environ['SAC_KEY_DESTINATION'] = SAC_KEY_DESTINATION\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['REGION'] = REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Google Application Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]='{}/{}.json'.format(SAC_KEY_DESTINATION,SAC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Whether Google Application Credential Was Set Successfully Outside Virtual Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GOOGLE_APPLICATION_CREDENTIALS=/media/mujahid7292/Data/Gcloud_Tem_SAC/jupyter-notebook-sac-d.json\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "set | grep GOOGLE_APPLICATION_CREDENTIALS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Default Project And Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated property [core/account].\n",
      "Updated property [core/project].\n",
      "Updated property [compute/region].\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud config set account $ACCOUNT\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Create TensorFlow model using TensorFlow's Estimator API </h2>\n",
    "<p>\n",
    "First, write an input_fn to read the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine CSV, label, and key columns\n",
    "CSV_COLUMNS = 'weight_pounds,is_male,mother_age,plurality,gestation_weeks,key'.split(',')\n",
    "LABEL_COLUMN = 'weight_pounds'\n",
    "KEY_COLUMN = 'key'\n",
    "\n",
    "# Set default values for each CSV column\n",
    "DEFAULTS = [[0.0], ['null'], [0.0], ['null'], [0.0], ['nokey']]\n",
    "TRAIN_STEPS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an input function reading a file using the Dataset API\n",
    "# Then provide the results to the Estimator API\n",
    "def read_dataset(filename, mode, batch_size = 512):\n",
    "  def _input_fn():\n",
    "    def decode_csv(value_column):\n",
    "      columns = tf.decode_csv(value_column, record_defaults=DEFAULTS)\n",
    "      features = dict(zip(CSV_COLUMNS, columns))\n",
    "      label = features.pop(LABEL_COLUMN)\n",
    "      return features, label\n",
    "    \n",
    "    # Create list of files that match pattern\n",
    "    file_list = tf.gfile.Glob(filename)\n",
    "\n",
    "    # Create dataset from file list\n",
    "    dataset = (tf.data.TextLineDataset(file_list)  # Read text file\n",
    "                 .map(decode_csv))  # Transform each elem by applying decode_csv fn\n",
    "      \n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        num_epochs = None # indefinitely\n",
    "        dataset = dataset.shuffle(buffer_size=10*batch_size)\n",
    "    else:\n",
    "        num_epochs = 1 # end-of-input after this\n",
    " \n",
    "    dataset = dataset.repeat(num_epochs).batch(batch_size)\n",
    "    return dataset\n",
    "  return _input_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, define the feature columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wide_and_deep():\n",
    "    \"\"\"\n",
    "    Define both wide and deep type feature column.\n",
    "    \"\"\"\n",
    "    is_male,mother_age,plurality,gestation_weeks= \\\n",
    "        [\\\n",
    "         tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "            key='is_male',\n",
    "            vocabulary_list=['True', 'False', 'Unknown']\n",
    "         ),\n",
    "         tf.feature_column.numeric_column('mother_age'),\n",
    "         tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "             key='plurality',\n",
    "             vocabulary_list=['Single(1)', 'Twins(2)', 'Triplets(3)',\n",
    "                       'Quadruplets(4)', 'Quintuplets(5)','Multiple(2+)']\n",
    "         ),\n",
    "         tf.feature_column.numeric_column('gestation_weeks')\n",
    "        ]\n",
    "    \n",
    "    # Discretize\n",
    "    age_buckets = tf.feature_column.bucketized_column(\n",
    "        source_column=mother_age,\n",
    "        boundaries=np.arange(start=15, stop=45, step=1).tolist()\n",
    "    )\n",
    "    \n",
    "    gestation_buckets = tf.feature_column.bucketized_column(\n",
    "        source_column=gestation_weeks,\n",
    "        boundaries=np.arange(start=17, stop=47, step=1).tolist()\n",
    "    )\n",
    "    \n",
    "    # Sparse columns are wide and have a linear relationship with the output.\n",
    "    wide = [\n",
    "        is_male,\n",
    "        plurality,\n",
    "        age_buckets,\n",
    "        gestation_buckets\n",
    "    ]\n",
    "    \n",
    "    # Feature cross all the wide column and embed into lower dimension.\n",
    "    crossed = tf.feature_column.crossed_column(keys=wide, hash_bucket_size=20000)\n",
    "    embed = tf.feature_column.embedding_column(categorical_column=crossed, dimension=3)\n",
    "    \n",
    "    # Continous columns are deep and have a complex relationship with the output.\n",
    "    deep =[\n",
    "        mother_age,\n",
    "        gestation_weeks,\n",
    "        embed\n",
    "    ]\n",
    "    \n",
    "    return wide, deep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To predict with the TensorFlow model, we also need a serving input function. We will want all the inputs from our user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create serving input function to be able to serve predictions later using provided inputs\n",
    "def serving_input_fn():\n",
    "    feature_placeholders = {\n",
    "        'is_male': tf.placeholder(tf.string, [None]),\n",
    "        'mother_age': tf.placeholder(tf.float32, [None]),\n",
    "        'plurality': tf.placeholder(tf.string, [None]),\n",
    "        'gestation_weeks': tf.placeholder(tf.float32, [None])\n",
    "    }\n",
    "    features = {\n",
    "        key: tf.expand_dims(tensor, -1)\n",
    "        for key, tensor in feature_placeholders.items()\n",
    "    }\n",
    "    return tf.estimator.export.ServingInputReceiver(features, feature_placeholders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train And Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(output_dir):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    wide, deep = get_wide_and_deep()\n",
    "    EVAL_INTERVAL = 300\n",
    "    \n",
    "    run_config = tf.estimator.RunConfig(\n",
    "        save_checkpoints_secs=EVAL_INTERVAL,\n",
    "        keep_checkpoint_max=3\n",
    "    )\n",
    "    \n",
    "    estimator = tf.estimator.DNNLinearCombinedRegressor(\n",
    "        model_dir=output_dir,\n",
    "        linear_feature_columns=wide,\n",
    "        dnn_feature_columns=deep,\n",
    "        dnn_hidden_units=[64,32],\n",
    "        config=run_config\n",
    "    )\n",
    "    \n",
    "    train_spec = tf.estimator.TrainSpec(\n",
    "        input_fn=read_dataset('train.csv',tf.estimator.ModeKeys.TRAIN),\n",
    "        max_steps=TRAIN_STEPS\n",
    "    )\n",
    "    \n",
    "    exporter = tf.estimator.LatestExporter(\n",
    "        name='exporter',\n",
    "        serving_input_receiver_fn=serving_input_fn\n",
    "    )\n",
    "    \n",
    "    eval_spec = tf.estimator.EvalSpec(\n",
    "        input_fn=read_dataset('eval.csv',tf.estimator.ModeKeys.EVAL),\n",
    "        steps=None,\n",
    "        start_delay_secs=60, # Start evaluating after N seconds\n",
    "        throttle_secs=EVAL_INTERVAL, # Evaluate every N seconds\n",
    "        exporters=exporter\n",
    "    )\n",
    "    \n",
    "    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monitor The Training In Tensorboard\n",
    "Run Below Command in terminal "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>$ `tensorboard --logdir babyweight_trained_tf_wd`</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open <a href='http://localhost:6006/'>Tensoarboard</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally, train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': 'babyweight_trained_tf_wd', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 300, '_session_config': None, '_keep_checkpoint_max': 3, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f5c135b5c18>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after 300 secs (eval_spec.throttle_secs) or training is finished.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into babyweight_trained_tf_wd/model.ckpt.\n",
      "INFO:tensorflow:loss = 16288.873, step = 1\n",
      "INFO:tensorflow:global_step/sec: 62.346\n",
      "INFO:tensorflow:loss = 633.91003, step = 101 (1.605 sec)\n",
      "INFO:tensorflow:global_step/sec: 64.0904\n",
      "INFO:tensorflow:loss = 663.922, step = 201 (1.560 sec)\n",
      "INFO:tensorflow:global_step/sec: 63.6573\n",
      "INFO:tensorflow:loss = 677.90137, step = 301 (1.571 sec)\n",
      "INFO:tensorflow:global_step/sec: 65.1114\n",
      "INFO:tensorflow:loss = 665.5498, step = 401 (1.536 sec)\n",
      "INFO:tensorflow:global_step/sec: 66.1384\n",
      "INFO:tensorflow:loss = 610.4074, step = 501 (1.512 sec)\n",
      "INFO:tensorflow:global_step/sec: 66.1297\n",
      "INFO:tensorflow:loss = 743.9429, step = 601 (1.512 sec)\n",
      "INFO:tensorflow:global_step/sec: 65.6486\n",
      "INFO:tensorflow:loss = 546.66254, step = 701 (1.523 sec)\n",
      "INFO:tensorflow:global_step/sec: 65.8215\n",
      "INFO:tensorflow:loss = 634.69324, step = 801 (1.519 sec)\n",
      "INFO:tensorflow:global_step/sec: 62.4565\n",
      "INFO:tensorflow:loss = 552.28937, step = 901 (1.601 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into babyweight_trained_tf_wd/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 658.9146.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2020-04-18-13:28:23\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from babyweight_trained_tf_wd/model.ckpt-1000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2020-04-18-13:28:23\n",
      "INFO:tensorflow:Saving dict for global step 1000: average_loss = 1.1862346, global_step = 1000, loss = 593.8473\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict']\n",
      "INFO:tensorflow:Signatures EXCLUDED from export because they cannot be be served via TensorFlow Serving APIs:\n",
      "INFO:tensorflow:'serving_default' : Regression input must be a single string Tensor; got {'is_male': <tf.Tensor 'Placeholder:0' shape=(?,) dtype=string>, 'mother_age': <tf.Tensor 'Placeholder_1:0' shape=(?,) dtype=float32>, 'plurality': <tf.Tensor 'Placeholder_2:0' shape=(?,) dtype=string>, 'gestation_weeks': <tf.Tensor 'Placeholder_3:0' shape=(?,) dtype=float32>}\n",
      "INFO:tensorflow:'regression' : Regression input must be a single string Tensor; got {'is_male': <tf.Tensor 'Placeholder:0' shape=(?,) dtype=string>, 'mother_age': <tf.Tensor 'Placeholder_1:0' shape=(?,) dtype=float32>, 'plurality': <tf.Tensor 'Placeholder_2:0' shape=(?,) dtype=string>, 'gestation_weeks': <tf.Tensor 'Placeholder_3:0' shape=(?,) dtype=float32>}\n",
      "WARNING:tensorflow:Export includes no default signature!\n",
      "INFO:tensorflow:Restoring parameters from babyweight_trained_tf_wd/model.ckpt-1000\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: b\"babyweight_trained_tf_wd/export/exporter/temp-b'1587216503'/saved_model.pb\"\n"
     ]
    }
   ],
   "source": [
    "# Run the model\n",
    "OUTPUT_DIR='babyweight_trained_tf_wd'\n",
    "shutil.rmtree(path=OUTPUT_DIR, ignore_errors=True) # Start fresh each time\n",
    "tf.summary.FileWriterCache.clear() # Ensure file writer cache is clear for tensorboard event file\n",
    "train_and_evaluate(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Monitor and experiment with training </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin TensorBoard from within AI Platform Notebooks, click the + symbol in the top left corner and select the **Tensorboard** icon to create a new TensorBoard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In TensorBoard, look at the learned embeddings. Are they getting clustered? How about the weights for the hidden layers? What if you run this longer? What happens if you change the batchsize?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
