{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enable Virtual Environment For This Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will go to the location of the directory, where we will enable our virtual environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>`$ cd /media/mujahid7292/Data/GoogleDriveSandCorp2014/ML_With_TensorFlow_On_GCP/06. End_To_End_ML_With_TensorFlow_On_GCP/Week_3/Lab_5_Training_On_Cloud_AI_Platform/Practice$`</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deactivate conda environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>`$ conda deactivate`</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activate newly created virtual environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>`$ source Venv/bin/activate`</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <p style='color:red'>Then we will open this notebook from inside our virtual environment.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook <a href=\"https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/courses/machine_learning/deepdive/06_structured/5_train.ipynb\">Link</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Necessary import of python package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/mujahid7292/Data/GoogleDriveSandCorp2014/ML_With_TensorFlow_On_GCP/06. End_To_End_ML_With_TensorFlow_On_GCP/Week_3/Lab_5_Training_On_Cloud_AI_Platform/Practice/Venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/media/mujahid7292/Data/GoogleDriveSandCorp2014/ML_With_TensorFlow_On_GCP/06. End_To_End_ML_With_TensorFlow_On_GCP/Week_3/Lab_5_Training_On_Cloud_AI_Platform/Practice/Venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/media/mujahid7292/Data/GoogleDriveSandCorp2014/ML_With_TensorFlow_On_GCP/06. End_To_End_ML_With_TensorFlow_On_GCP/Week_3/Lab_5_Training_On_Cloud_AI_Platform/Practice/Venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/media/mujahid7292/Data/GoogleDriveSandCorp2014/ML_With_TensorFlow_On_GCP/06. End_To_End_ML_With_TensorFlow_On_GCP/Week_3/Lab_5_Training_On_Cloud_AI_Platform/Practice/Venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:522: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/media/mujahid7292/Data/GoogleDriveSandCorp2014/ML_With_TensorFlow_On_GCP/06. End_To_End_ML_With_TensorFlow_On_GCP/Week_3/Lab_5_Training_On_Cloud_AI_Platform/Practice/Venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/media/mujahid7292/Data/GoogleDriveSandCorp2014/ML_With_TensorFlow_On_GCP/06. End_To_End_ML_With_TensorFlow_On_GCP/Week_3/Lab_5_Training_On_Cloud_AI_Platform/Practice/Venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version: 1.8.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import argparse\n",
    "import json\n",
    "import tensorflow as tf\n",
    "print('Tensorflow Version: {}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change these to try this notebook out\n",
    "ACCOUNT = 'student-04-972b7cf5493d@qwiklabs.net'\n",
    "SAC = 'jupyter-notebook-sac-e'\n",
    "SAC_KEY_DESTINATION = '/media/mujahid7292/Data/Gcloud_Tem_SAC'\n",
    "BUCKET = 'bucket-qwiklabs-gcp-04-e6818aa80ab4'\n",
    "PROJECT = 'qwiklabs-gcp-04-e6818aa80ab4'\n",
    "REGION = 'us-central1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bash Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['ACCOUNT'] = ACCOUNT\n",
    "os.environ['SAC'] = SAC\n",
    "os.environ['SAC_KEY_DESTINATION'] = SAC_KEY_DESTINATION\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['REGION'] = REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Google Application Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]='{}/{}.json'.format(SAC_KEY_DESTINATION,SAC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Whether Google Application Credential Was Set Successfully Outside Virtual Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GOOGLE_APPLICATION_CREDENTIALS=/media/mujahid7292/Data/Gcloud_Tem_SAC/jupyter-notebook-sac-e.json\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "set | grep GOOGLE_APPLICATION_CREDENTIALS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Default Project And Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated property [core/account].\n",
      "Updated property [core/project].\n",
      "Updated property [compute/region].\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud config set account $ACCOUNT\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create GCS bucket & copy training & evaluation data to bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if ! gsutil ls | grep -q gs://${BUCKET}/babyweight/preproc; then\n",
    "  #gsutil mb -l us-central1 -p ml-practice-260405 gs://bucket-ml-practice-260405\n",
    "  gsutil mb -l ${REGION} -p ${PROJECT} gs://${BUCKET}\n",
    "  # copy canonical set of preprocessed files if you didn't do previous notebook\n",
    "  #gsutil -m cp -R gs://cloud-training-demos/babyweight gs://bucket-qwiklabs-gcp-04-e6818aa80ab4\n",
    "  gsutil -m cp -R gs://cloud-training-demos/babyweight gs://${BUCKET}\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process is terminated.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gsutil ls gs://${BUCKET}/babyweight/preproc/*-00000*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the TensorFlow code working on a subset of the data, we can package the TensorFlow code up as a Python module and train it on Cloud AI Platform.\n",
    "<p>\n",
    "<h2> Train on Cloud AI Platform</h2>\n",
    "<p>\n",
    "Training on Cloud AI Platform requires:\n",
    "<ol>\n",
    "<li> Making the code a Python package\n",
    "<li> Using gcloud to submit the training code to Cloud AI Platform\n",
    "</ol>\n",
    "\n",
    "Ensure that the AI Platform API is enabled by going to this [link](https://console.developers.google.com/apis/library/ml.googleapis.com)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Task 1\n",
    "\n",
    "The following code writes babyweight/trainer/task.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf babyweight\n",
    "mkdir babyweight\n",
    "mkdir babyweight/trainer\n",
    "touch babyweight/trainer/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing babyweight/trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile babyweight/trainer/task.py\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "from . import model\n",
    "import tensorflow as tf\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Create an argument parser object\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    # Now add our arguments one by one\n",
    "    parser.add_argument(\n",
    "        '--data_path',\n",
    "        help = \"GCS or Local path to training & evaluation data.\",\n",
    "        required = True\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        '--output_dir',\n",
    "        help = \"GCS or Local location to write checkpoints and export models\",\n",
    "        required = True\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        '--batch_size',\n",
    "        help = \"Number of examples to compute gradient over\",\n",
    "        type = int,\n",
    "        default = 512\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        '--job-dir',\n",
    "        help = \"This model ignore this field, but it is required by gcloud\",\n",
    "        default = \"junk\"\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        '--nn_size',\n",
    "        help = \"Hidden layer size for DNN\",\n",
    "        nargs='+',\n",
    "        type = int,\n",
    "        default=[128,32,4]\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        '--nembeds',\n",
    "        help = \"Embedding size of a cross of n key. Real valued parameters\",\n",
    "        type = int,\n",
    "        default = 3\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        '--train_examples',\n",
    "        help = \"Number of examples (In thousands) to run the training job over. If this is more than actual # of examples available, it cycles through them. So specifying 1000 here when you have only 100k examples makes this 10 epochs.\",\n",
    "        type = int,\n",
    "        default = 5000\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        '--pattern',\n",
    "        help = \"Specify a pattern that has to be in a input files. For example 00001-of will process only one shard.\",\n",
    "        default = \"of\"\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        '--eval_steps',\n",
    "        help = \"Positive number of steps for which to evaluate model. Default to None, which means to evaluate untill input_fn raises an end-of-input exception\",\n",
    "        type = int,\n",
    "        default = None\n",
    "    )\n",
    "    \n",
    "    ## Parse all arguments\n",
    "    args = parser.parse_args()\n",
    "    arguments = args.__dict__\n",
    "    \n",
    "    ## Unused args provided by service\n",
    "    arguments.pop('job-dir', None)\n",
    "    arguments.pop('job_dir', None)\n",
    "    \n",
    "    ## Assign the arguments to the model variable\n",
    "    output_dir = arguments.pop('output_dir')\n",
    "    model.DATA_PATH = arguments.pop('data_path')\n",
    "    model.BATCH_SIZE = arguments.pop('batch_size')\n",
    "    model.TRAIN_STEPS = (arguments.pop('train_examples') * 1000) / model.BATCH_SIZE\n",
    "    model.EVAL_STEPS = arguments.pop('eval_steps')\n",
    "    print(\"We will train for {} steps using batch size {}\".format(model.TRAIN_STEPS, model.BATCH_SIZE))\n",
    "    model.PATTERN = arguments.pop('pattern')\n",
    "    model.NEMBEDS = arguments.pop('nembeds')\n",
    "    model.NN_SIZE = arguments.pop('nn_size')\n",
    "    print(\"We will use DNN size of {}\".format(model.NN_SIZE))\n",
    "    \n",
    "    \n",
    "    ## Append trail id to path if we are doing Hyperparameter Tunning\n",
    "    ## This code can be removed if you are not doing Hyperparameter Tunning\n",
    "    output_dir = os.path.join(\n",
    "        output_dir,\n",
    "        json.loads(\n",
    "            os.environ.get('TF_CONFIG', '{}')\n",
    "        ).get('task', {}).get('trail', '')\n",
    "    )\n",
    "    \n",
    "    ## Run the training job\n",
    "    model.train_and_evaluate(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Task 2\n",
    "\n",
    "The following code writes babyweight/trainer/model.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing babyweight/trainer/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile babyweight/trainer/model.py\n",
    "import shutil\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "DATA_PATH = None # Set from task.py file\n",
    "PATTERN = 'of' # gets all files\n",
    "\n",
    "# Determine CSV, Labels & key column\n",
    "CSV_COLUMNS = 'weight_pounds,is_male,mother_age,plurality,gestation_weeks,key'.split(',')\n",
    "LABEL_COLUMN = 'weight_pounds'\n",
    "KEY_COLUMN = 'key'\n",
    "\n",
    "# Set defaults value for each CSV columns\n",
    "DEFAULTS = [[0.0], ['null'], [0.0], ['null'], [0.0], ['nokey']]\n",
    "\n",
    "# Define some hyper parameter\n",
    "TRAIN_STEPS = 10000\n",
    "EVAL_STEPS = None\n",
    "BATCH_SIZE = 512\n",
    "NEMBEDS = 3\n",
    "NN_SIZE = [64, 16, 4]\n",
    "\n",
    "# Create an input function reading a file using the Dataset API\n",
    "# Then provide the results to the Estimator API\n",
    "def read_dataset(prefix, mode, batch_size):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def _input_fn():\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        def decode_csv(value_column):\n",
    "            \"\"\"\n",
    "            \"\"\"\n",
    "            columns = tf.decode_csv(records=value_column, record_defaults=DEFAULTS)\n",
    "            features = dict(zip(CSV_COLUMNS, columns))\n",
    "            label = features.pop(LABEL_COLUMN)\n",
    "            return features, label\n",
    "        \n",
    "        # Use prefix to create file path\n",
    "        file_path = '{}/babyweight/preproc/{}/{}'.format(DATA_PATH, prefix, PATTERN)\n",
    "        \n",
    "        # Create list of files, that matches pattern.\n",
    "        file_list = tf.gfile.Glob(file_path)\n",
    "        \n",
    "        # Create dataset from file list\n",
    "        dataset = (tf.data.TextLineDataset(filenames=file_list) # Read text file\n",
    "                   .map(decode_csv)) # Transform each element by applying decode_csv\n",
    "        \n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            num_epochs = None # Run Indefinietly\n",
    "            dataset = dataset.shuffle(buffer_size=10*batch_size)\n",
    "        else:\n",
    "            num_epochs = 1 # end-of-input after this\n",
    "        \n",
    "        dataset = dataset.repeat(num_epochs).batch(batch_size)\n",
    "        return dataset.make_one_shot_iterator().get_next()\n",
    "    return _input_fn\n",
    "\n",
    "# Define features column\n",
    "def get_wide_deep():\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Define column type\n",
    "    is_male,mother_age,plurality,gestation_weeks= \\\n",
    "    [\\\n",
    "     tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "        key='is_male',\n",
    "        vocabulary_list=['True', 'False', 'Unknown']\n",
    "    ),\n",
    "     tf.feature_column.numeric_column('mother_age'),\n",
    "     tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "         key='plurality',\n",
    "         vocabulary_list=['Single(1)', 'Twins(2)', 'Triplets(3)',\n",
    "                         'Quadruplets(4)', 'Quintuplets(5)','Multiple(2+)']\n",
    "     ),\n",
    "     tf.feature_column.numeric_column('gestation_weeks')\n",
    "    ]\n",
    "    \n",
    "    # Discretize the age & gestation weeks column. This will convert those two\n",
    "    # column from being deep to wide column.\n",
    "    age_buckets = tf.feature_column.bucketized_column(\n",
    "        source_column=mother_age,\n",
    "        boundaries=np.arange(start=15, stop=45, step=1).tolist()\n",
    "    )\n",
    "    gestation_buckets = tf.feature_column.bucketized_column(\n",
    "        source_column=gestation_weeks,\n",
    "        boundaries=np.arange(start=17, stop=47, step=1).tolist()\n",
    "    )\n",
    "    \n",
    "    # Sparse columns are wide, have a linear relationship with the output\n",
    "    wide = [\n",
    "        is_male,\n",
    "        plurality,\n",
    "        age_buckets,\n",
    "        gestation_buckets\n",
    "    ]\n",
    "    \n",
    "    # Feature cross all the wide column and embed into lower dimension.\n",
    "    crossed = tf.feature_column.crossed_column(\n",
    "        keys=wide,\n",
    "        hash_bucket_size=20000\n",
    "    )\n",
    "    embed = tf.feature_column.embedding_column(\n",
    "        categorical_column=crossed,\n",
    "        dimension=NEMBEDS\n",
    "    )\n",
    "    \n",
    "    # Continous columns are deep and have a complex relationship with the output.\n",
    "    deep = [\n",
    "        mother_age,\n",
    "        gestation_weeks,\n",
    "        embed\n",
    "    ]\n",
    "    \n",
    "    return wide, deep\n",
    "\n",
    "# Create serving input function to serve prediction later\n",
    "def serving_input_fn():\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    feature_placeholders = {\n",
    "        'is_male': tf.placeholder(dtype=tf.string, shape=[None]),\n",
    "        'mother_age': tf.placeholder(dtype=tf.float32, shape=[None]),\n",
    "        'plurality': tf.placeholder(dtype=tf.string, shape=[None]),\n",
    "        'gestation_weeks': tf.placeholder(dtype=tf.float32, shape=[None]),\n",
    "        KEY_COLUMN: tf.placeholder_with_default(input=tf.constant(['nokey']), shape=[None])\n",
    "    }\n",
    "    \n",
    "    features = {\n",
    "        key: tf.expand_dims(input=tensor, axis=-1)\n",
    "        for key, tensor in feature_placeholders.items()\n",
    "    }\n",
    "    \n",
    "    return tf.estimator.export.ServingInputReceiver(\n",
    "        features=features,\n",
    "        receiver_tensors=feature_placeholders\n",
    "    )\n",
    "\n",
    "# Create metric for hyper parameter tunning\n",
    "def my_rmse(labels, predictions):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    pred_values = predictions['predictions']\n",
    "    return {\n",
    "        'rmse': tf.metrics.root_mean_squared_error(\n",
    "            labels=labels,\n",
    "            predictions=pred_values\n",
    "        )\n",
    "    }\n",
    "\n",
    "# Create estimator to train and evaluate\n",
    "def train_and_evaluate(output_dir):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Ensure Filewriter Cache is clear for Tensorboard events file.\n",
    "    tf.summary.FileWriterCache.clear()\n",
    "    wide, deep = get_wide_deep()\n",
    "    EVAL_INTERVAl = 300 # Seconds\n",
    "    \n",
    "    run_config = tf.estimator.RunConfig(\n",
    "        save_checkpoints_secs=EVAL_INTERVAl,\n",
    "        keep_checkpoint_max=3\n",
    "    )\n",
    "    \n",
    "    estimator = tf.estimator.DNNLinearCombinedRegressor(\n",
    "        model_dir=output_dir,\n",
    "        linear_feature_columns=wide,\n",
    "        dnn_feature_columns=deep,\n",
    "        dnn_hidden_units=NN_SIZE,\n",
    "        config=run_config\n",
    "    )\n",
    "    \n",
    "    # Attach custom metric to the estimator.\n",
    "    estimator = tf.contrib.estimator.add_metrics(\n",
    "        estimator=estimator, \n",
    "        metric_fn=my_rmse\n",
    "    )\n",
    "    \n",
    "    # For batch prediction, you need a key associated with each instances\n",
    "    estimator = tf.contrib.estimator.forward_features(\n",
    "        estimator=estimator,\n",
    "        keys=KEY_COLUMN\n",
    "    )\n",
    "    \n",
    "    train_spec = tf.estimator.TrainSpec(\n",
    "        input_fn = read_dataset('train',tf.estimator.ModeKeys.TRAIN, BATCH_SIZE),\n",
    "        max_steps = TRAIN_STEPS\n",
    "    )\n",
    "    \n",
    "    exporter = tf.estimator.LatestExporter(\n",
    "        name='exporter',\n",
    "        serving_input_receiver_fn=serving_input_fn,\n",
    "        exports_to_keep=None\n",
    "    )\n",
    "    \n",
    "    eval_spec = tf.estimator.EvalSpec(\n",
    "        input_fn=read_dataset('eval', tf.estimator.ModeKeys.EVAL, 2**15), # No need to batch in eval\n",
    "        steps = EVAL_STEPS,\n",
    "        start_delay_secs=60, # Start evaluating after N seconds\n",
    "        throttle_secs=EVAL_INTERVAl, # Evaluate every N seconds\n",
    "        exporters=exporter\n",
    "    )\n",
    "    \n",
    "    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Task 3\n",
    "\n",
    "After moving the code to a package, make sure it works standalone. (Note the --pattern and --train_examples lines so that I am not trying to boil the ocean on my laptop). Even then, this takes about <b>3 minutes</b> in which you won't see any output ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval\n",
      "train\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ls data/babyweight/preproc/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf babyweight_trained\n",
    "export PYTHONPATH=${PYTHONPATH}:${PWD}/babyweight\n",
    "python -m trainer.task \\\n",
    "    --data_path=data \\\n",
    "    --output_dir=babyweight_trained \\\n",
    "    --job-dir=./tmp \\\n",
    "    --pattern='00000-of-'\\\n",
    "    --train_examples=1 \\\n",
    "    --eval_steps=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Task 4\n",
    "\n",
    "The JSON below represents an input into your prediction model. Write the input.json file below with the next cell, then run the prediction locally to assess whether it produces predictions correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile inputs.json\n",
    "{\"key\": \"b1\", \"is_male\": \"True\", \"mother_age\": 26.0, \"plurality\": \"Single(1)\", \"gestation_weeks\": 39}\n",
    "{\"key\": \"g1\", \"is_male\": \"False\", \"mother_age\": 26.0, \"plurality\": \"Single(1)\", \"gestation_weeks\": 39}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "sudo find \"/usr/lib/google-cloud-sdk/lib/googlecloudsdk/command_lib/ml_engine\" -name '*.pyc' -delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "MODEL_LOCATION=$(ls -d $(pwd)/babyweight_trained/export/exporter/* | tail -1)\n",
    "echo $MODEL_LOCATION1\n",
    "gcloud ai-platform local predict --model-dir=$MODEL_LOCATION --json-instances=inputs.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Task 5\n",
    "\n",
    "Once the code works in standalone mode, you can run it on Cloud AI Platform. Because this is on the entire dataset, it will take a while. The training run took about <b> two hours </b> for me. You can monitor the job from the GCP console in the Cloud AI Platform section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process is interrupted.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "OUTDIR=gs://${BUCKET}/babyweight/trained_model\n",
    "JOBNAME=babyweight_$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "gcloud ai-platform jobs submit training $JOBNAME \\\n",
    "  --region=$REGION \\\n",
    "  --module-name=trainer.task \\\n",
    "  --package-path=$(pwd)/babyweight/trainer \\\n",
    "  --job-dir=$OUTDIR \\\n",
    "  --staging-bucket=gs://$BUCKET \\\n",
    "  --scale-tier=STANDARD_1 \\\n",
    "  --runtime-version=$TFVERSION \\\n",
    "  -- \\\n",
    "  --data_path=${BUCKET} \\\n",
    "  --output_dir=${OUTDIR} \\\n",
    "  --train_examples=500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I ran it, I used train_examples=2000000. When training finished, I filtered in the Stackdriver log on the word \"dict\" and saw that the last line was:\n",
    "<pre>\n",
    "Saving dict for global step 5714290: average_loss = 1.06473, global_step = 5714290, loss = 34882.4, rmse = 1.03186\n",
    "</pre>\n",
    "The final RMSE was 1.03 pounds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Hyperparameter tuning </h2>\n",
    "<p>\n",
    "All of these are command-line parameters to my program.  To do hyperparameter tuning, create hyperparam.xml and pass it as --configFile.\n",
    "This step will take <b>up to 2 hours</b> -- you can increase maxParallelTrials or reduce maxTrials to get it done faster.  Since maxParallelTrials is the number of initial seeds to start searching from, you don't want it to be too large; otherwise, all you have is a random search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile hyperparam.yaml\n",
    "trainingInput:\n",
    "  scaleTier: STANDARD_1\n",
    "  hyperparameters:\n",
    "    hyperparameterMetricTag: rmse\n",
    "    goal: MINIMIZE\n",
    "    maxTrials: 20\n",
    "    maxParallelTrials: 5\n",
    "    enableTrialEarlyStopping: True\n",
    "    params:\n",
    "    - parameterName: batch_size\n",
    "      type: INTEGER\n",
    "      minValue: 8\n",
    "      maxValue: 512\n",
    "      scaleType: UNIT_LOG_SCALE\n",
    "    - parameterName: nembeds\n",
    "      type: INTEGER\n",
    "      minValue: 3\n",
    "      maxValue: 30\n",
    "      scaleType: UNIT_LINEAR_SCALE\n",
    "    - parameterName: nnsize\n",
    "      type: INTEGER\n",
    "      minValue: 64\n",
    "      maxValue: 512\n",
    "      scaleType: UNIT_LOG_SCALE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "OUTDIR=gs://${BUCKET}/babyweight/hyperparam\n",
    "JOBNAME=babyweight_$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "gcloud ai-platform jobs submit training $JOBNAME \\\n",
    "  --region=$REGION \\\n",
    "  --module-name=trainer.task \\\n",
    "  --package-path=$(pwd)/babyweight/trainer \\\n",
    "  --job-dir=$OUTDIR \\\n",
    "  --staging-bucket=gs://$BUCKET \\\n",
    "  --scale-tier=STANDARD_1 \\\n",
    "  --config=hyperparam.yaml \\\n",
    "  --runtime-version=$TFVERSION \\\n",
    "  -- \\\n",
    "  --bucket=${BUCKET} \\\n",
    "  --output_dir=${OUTDIR} \\\n",
    "  --eval_steps=10 \\\n",
    "  --train_examples=20000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Repeat training </h2>\n",
    "<p>\n",
    "This time with tuned parameters (note last line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "OUTDIR=gs://${BUCKET}/babyweight/trained_model_tuned\n",
    "JOBNAME=babyweight_$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "gcloud ai-platform jobs submit training $JOBNAME \\\n",
    "  --region=$REGION \\\n",
    "  --module-name=trainer.task \\\n",
    "  --package-path=$(pwd)/babyweight/trainer \\\n",
    "  --job-dir=$OUTDIR \\\n",
    "  --staging-bucket=gs://$BUCKET \\\n",
    "  --scale-tier=STANDARD_1 \\\n",
    "  --runtime-version=$TFVERSION \\\n",
    "  -- \\\n",
    "  --bucket=${BUCKET} \\\n",
    "  --output_dir=${OUTDIR} \\\n",
    "  --train_examples=20000 --batch_size=35 --nembeds=16 --nnsize=281"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
