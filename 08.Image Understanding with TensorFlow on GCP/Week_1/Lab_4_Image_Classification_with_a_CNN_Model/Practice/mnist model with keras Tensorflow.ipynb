{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST image classification with Keras Tensorflow backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import json\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = \"ml-practice-260405\" # REPLACE WITH YOUR PROJECT ID\n",
    "BUCKET = \"bucket-ml-practice-260405\" # REPLACE WITH YOUR BUCKET NAME\n",
    "REGION = \"us-central1\" # REPLACE WITH YOUR BUCKET REGION e.g. us-central1\n",
    "MODEL_TYPE = \"cnn\"  # \"linear\", \"dnn\", \"dnn_dropout\", or \"cnn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not change \n",
    "os.environ[\"ACCOUNT\"] = \"sandcorp2014@gmail.com\"\n",
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"BUCKET\"] = BUCKET\n",
    "os.environ[\"REGION\"] = REGION\n",
    "os.environ[\"MODEL_TYPE\"] = MODEL_TYPE\n",
    "os.environ[\"TFVERSION\"] = \"1.13\"  # Tensorflow version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated property [core/account].\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud config set account $ACCOUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "Updated property [compute/region].\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir mnistmodel_keras_tf\n",
    "mkdir mnistmodel_keras_tf/trainer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mnistmodel_keras_tf/trainer/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mnistmodel_keras_tf/trainer/__init__.py\n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mnistmodel_keras_tf/trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mnistmodel_keras_tf/trainer/task.py\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "from .import model\n",
    "import tensorflow as tf\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Create a parser object\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    # Input argument in the parser object\n",
    "    parser.add_argument(\n",
    "        \"--train_batch_size\",\n",
    "        help=\"Batch size for training\",\n",
    "        type = int,\n",
    "        default=100\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        \"--learning_rate\",\n",
    "        help=\"Initial Learning Rate For Training\",\n",
    "        type=float,\n",
    "        default=0.01\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        \"--train_steps\",\n",
    "        help=\"Steps to run the training jobs for\",\n",
    "        type=int,\n",
    "        default=0\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        \"--output_dir\",\n",
    "        help=\"Local Or GCS location to write checkpoint and export model\",\n",
    "        required=True\n",
    "    )\n",
    "    \n",
    "    # Generate List of models to print in help message\n",
    "    model_names = [name.replace(\"_model\",\"\") for name in dir(model) if name.endswith(\"_model\")]\n",
    "    \n",
    "    parser.add_argument(\n",
    "        '--model',\n",
    "        help=\"Type of model. Supported types are {}\".format(model_names),\n",
    "        default='cnn'\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        \"--job-dir\",\n",
    "        help = \"this model ignores this field, but it is required by gcloud\",\n",
    "        default = \"junk\"\n",
    "    )\n",
    "    \n",
    "    # Optional Hyperparameters used by cnn\n",
    "    parser.add_argument(\n",
    "        '--ksize1',\n",
    "        help = \"Kernel size of the first layer of the cnn\",\n",
    "        type=int,\n",
    "        default=5\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        '--ksize2',\n",
    "        help = \"Kernel size of the second layer of the cnn\",\n",
    "        type=int,\n",
    "        default=5\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        '--nfil1',\n",
    "        help = \"Number of filters in the first layer of the cnn\",\n",
    "        type=int,\n",
    "        default=10\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        '--nfil2',\n",
    "        help = \"Number of filters in the second layer of the cnn\",\n",
    "        type=int,\n",
    "        default=20\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        '--dprob',\n",
    "        help ='Drop out probability for cnn',\n",
    "        type = float,\n",
    "        default=0.25\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        '--batch_norm',\n",
    "        help = \"If specified, do batch_norm for cnn\",\n",
    "        dest = \"batch_norm\",\n",
    "        action = \"store_true\"\n",
    "    )\n",
    "    parser.set_defaults(batch_norm = False)\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    hparams = args.__dict__\n",
    "\n",
    "    # unused args provided by service\n",
    "    hparams.pop(\"job_dir\", None)\n",
    "    hparams.pop(\"job-dir\", None)\n",
    "\n",
    "    output_dir = hparams.pop(\"output_dir\")\n",
    "    # Append trial_id to path so hptuning jobs don\"t overwrite eachother\n",
    "    output_dir = os.path.join(\n",
    "        output_dir,\n",
    "        json.loads(\n",
    "            os.environ.get(\"TF_CONFIG\", \"{}\")\n",
    "        ).get(\"task\", {}).get(\"trial\", \"\")\n",
    "    )\n",
    "    \n",
    "    # Calculate train_steps if not provided\n",
    "    if hparams[\"train_steps\"] < 1:\n",
    "        # 10,000 steps with batch_size of 512\n",
    "        hparams[\"train_steps\"] = (10000 * 512) // hparams[\"train_batch_size\"]\n",
    "        print(\"Training for {} steps.\".format(hparams[\"train_steps\"]))\n",
    "    \n",
    "    # Run the training job\n",
    "    model.train_and_evaluate(output_dir, hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mnistmodel_keras_tf/trainer/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mnistmodel_keras_tf/trainer/model.py\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)\n",
    "\n",
    "HEIGHT=28\n",
    "WIDTH=28\n",
    "NCLASSES=10\n",
    "\n",
    "# Build Keras model using Keras Sequential API\n",
    "def linear_model(hparams):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.InputLayer(input_shape=[HEIGHT, WIDTH, 1], name=\"image\"))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(\n",
    "        units=NCLASSES,\n",
    "        activation=tf.nn.softmax,\n",
    "        name=\"probabilities\"\n",
    "    ))\n",
    "    return model\n",
    "    \n",
    "def dnn_model(hparams):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.InputLayer(input_shape=[HEIGHT, WIDTH, 1], name=\"image\"))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(units=300, activation=tf.nn.relu))\n",
    "    model.add(tf.keras.layers.Dense(units=100, activation=tf.nn.relu))\n",
    "    model.add(tf.keras.layers.Dense(units=30, activation=tf.nn.relu))\n",
    "    model.add(tf.keras.layers.Dense(units=NCLASSES, activation=tf.nn.softmax, name='Probabilities'))\n",
    "    return model\n",
    "\n",
    "def dnn_dropout_model(hparams):\n",
    "    dprob = hparams.get(\"dprob\", 0.1)\n",
    "    \n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.InputLayer(input_shape=[HEIGHT, WIDTH, 1], name=\"image\"))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(units=300, activation=tf.nn.relu))\n",
    "    model.add(tf.keras.layers.Dense(units=100, activation=tf.nn.relu))\n",
    "    model.add(tf.keras.layers.Dense(units=30, activation=tf.nn.relu))\n",
    "    model.add(tf.keras.layers.Dropout(rate=dprob))\n",
    "    model.add(tf.keras.layers.Dense(\n",
    "        units=NCLASSES, \n",
    "        activation=tf.nn.softmax,\n",
    "        name=\"probabilities\"\n",
    "    ))\n",
    "    return model\n",
    "\n",
    "def cnn_model(hparams):\n",
    "    ksize1 = hparams.get('ksize1', 5)\n",
    "    ksize2 = hparams.get('ksize2', 5)\n",
    "    nfil1 = hparams.get('nfil1', 10)\n",
    "    nfil2 = hparams.get('nfil2', 20)\n",
    "    dprob = hparams.get('dprob', 0.25)\n",
    "    \n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.InputLayer(\n",
    "        input_shape = [HEIGHT, WIDTH, 1],\n",
    "        name = \"image\"\n",
    "    )) # shape = (?, 28, 28, 1)\n",
    "    model.add(tf.keras.layers.Conv2D(\n",
    "        filters=nfil1,\n",
    "        kernel_size=ksize1,\n",
    "        padding='same',\n",
    "        activation=tf.nn.relu\n",
    "    )) # shape = (?, 28, 28, nfil1)\n",
    "    model.add(tf.keras.layers.MaxPooling2D(\n",
    "        pool_size=2,\n",
    "        strides=2\n",
    "    )) # shape = (?, 14, 14, nfil1)\n",
    "    model.add(tf.keras.layers.Conv2D(\n",
    "        filters=nfil2,\n",
    "        kernel_size=ksize2,\n",
    "        padding='same',\n",
    "        activation=tf.nn.relu\n",
    "    )) # shape = (?, 14, 14, nfil2)\n",
    "    model.add(tf.keras.layers.MaxPooling2D(\n",
    "        pool_size=2,\n",
    "        strides=2\n",
    "    )) # shape = (?, 7, 7, nfil2)\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    \n",
    "    # Apply Batch Normalization\n",
    "    if hparams['batch_norm']:\n",
    "        model.add(tf.keras.layers.Dense(\n",
    "            units=300,\n",
    "            activation=tf.nn.relu\n",
    "        ))\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "        model.add(tf.keras.layers.Activation(\n",
    "            activation=tf.nn.relu\n",
    "        ))\n",
    "    else:\n",
    "        model.add(tf.keras.layers.Dense(\n",
    "            units=300,\n",
    "            activation=tf.nn.relu\n",
    "        ))\n",
    "        \n",
    "    # Apply Drop Out\n",
    "    model.add(tf.keras.layers.Dropout(rate=dprob))\n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(\n",
    "        units=NCLASSES,\n",
    "        activation=None\n",
    "    ))\n",
    "    \n",
    "    # Apply Batch Normalization Once More\n",
    "    if hparams['batch_norm']:\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "        \n",
    "    # SoftMax Layer\n",
    "    model.add(tf.keras.layers.Dense(\n",
    "        units=NCLASSES,\n",
    "        activation=tf.nn.softmax,\n",
    "        name='probabilities'\n",
    "    ))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create serving input function for inference\n",
    "def serving_input_fn():\n",
    "    # Input will be rank 3\n",
    "    feature_placeholders = {\n",
    "        \"image\": tf.compat.v1.placeholder(dtype = tf.float32, shape=[None, HEIGHT, WIDTH])\n",
    "    }\n",
    "    # But model function require rank 4\n",
    "    features = {\n",
    "        \"image\": tf.expand_dims(input=feature_placeholders[\"image\"], axis=-1)\n",
    "    }\n",
    "    return tf.estimator.export.ServingInputReceiver(\n",
    "        features=features,\n",
    "        receiver_tensors=feature_placeholders\n",
    "    )\n",
    "\n",
    "    \n",
    "# Create train and evaluate function\n",
    "def train_and_evaluate(ouput_dir, hparams):\n",
    "    # Ensure filewriter cache is clear for TensorBoard event file.\n",
    "    tf.compat.v1.summary.FileWriterCache.clear()\n",
    "    \n",
    "    EVAL_INTERVAL = 60\n",
    "    \n",
    "    # Get mnist Data\n",
    "    mnist = tf.keras.datasets.mnist\n",
    "    \n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    \n",
    "    # Scale our training and testing features between 0 and 1\n",
    "    x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "    \n",
    "    # Reshape image to add a dimension for channels (1 in this case)\n",
    "    x_train = x_train.reshape([-1, HEIGHT, WIDTH, 1])\n",
    "    x_test = x_test.reshape([-1, HEIGHT, WIDTH, 1])\n",
    "    \n",
    "    # Convert labels to categorical one hot encoding\n",
    "    y_train = tf.keras.utils.to_categorical(y=y_train, num_classes=NCLASSES)\n",
    "    y_test = tf.keras.utils.to_categorical(y=y_test, num_classes=NCLASSES)\n",
    "    \n",
    "    # Create training input function\n",
    "    train_input_fn = tf.compat.v1.estimator.inputs.numpy_input_fn(\n",
    "        x={\"image\": x_train},\n",
    "        y=y_train,\n",
    "        batch_size=100,\n",
    "        num_epochs=None,\n",
    "        shuffle=True,\n",
    "        queue_capacity=5000\n",
    "    )\n",
    "    \n",
    "    # Create evaluation input function\n",
    "    eval_input_fn = tf.compat.v1.estimator.inputs.numpy_input_fn(\n",
    "        x={\"image\": x_test},\n",
    "        y=y_test,\n",
    "        batch_size=100,\n",
    "        num_epochs=1,\n",
    "        shuffle=False,\n",
    "        queue_capacity=5000\n",
    "    )\n",
    "    \n",
    "    # Build Keras model\n",
    "    model = cnn_model(hparams=hparams)\n",
    "    \n",
    "    # Compile Keras model with optimizer, loss function and eval metric\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Convert Keras model to estimator\n",
    "    estimator = tf.keras.estimator.model_to_estimator(\n",
    "        keras_model=model,\n",
    "        model_dir=ouput_dir,\n",
    "        config=tf.estimator.RunConfig(save_checkpoints_secs=EVAL_INTERVAL)\n",
    "    )\n",
    "    \n",
    "    # Set estimator's train_spec to use train_input_fn() and train for so many steps\n",
    "    train_spec = tf.estimator.TrainSpec(\n",
    "        input_fn=train_input_fn,\n",
    "        max_steps=hparams['train_steps']\n",
    "    )\n",
    "    \n",
    "    # Create exporter that use serving_input_fn() to \n",
    "    # create saved_model for serving.\n",
    "    exporter = tf.estimator.LatestExporter(\n",
    "        name='exporter',\n",
    "        serving_input_receiver_fn=serving_input_fn\n",
    "    )\n",
    "    \n",
    "    # Set estimators eval_spec to use eval_input_fn() and \n",
    "    # export saved_model\n",
    "    eval_spec = tf.estimator.EvalSpec(\n",
    "        input_fn=eval_input_fn,\n",
    "        steps=None,\n",
    "        exporters=exporter,\n",
    "        throttle_secs=EVAL_INTERVAL\n",
    "    )\n",
    "    \n",
    "    # Run train_and_evaluate loop\n",
    "    tf.estimator.train_and_evaluate(\n",
    "        estimator=estimator,\n",
    "        train_spec=train_spec,\n",
    "        eval_spec=eval_spec\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run as a Python module\n",
    "\n",
    "Since we want to run our code on Cloud ML Engine, we've packaged it as a python module.\n",
    "\n",
    "The `model.py` and `task.py` containing the model code is in <a href=\"mnistmodel_keras_tf/trainer\">mnistmodel_keras_tf/trainer</a>\n",
    "\n",
    "**Let's first run it locally for a few steps to test the code.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: The `gcloud ml-engine` commands have been renamed and will soon be removed. Please use `gcloud ai-platform` instead.\n",
      "2020-08-05 10:05:14.310346: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory\n",
      "2020-08-05 10:05:14.310414: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory\n",
      "2020-08-05 10:05:14.310422: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0805 10:05:15.341052 140584698599232 module_wrapper.py:138] From /media/mujahid7292/Data/GoogleDriveSandCorp2014/ML_With_TensorFlow_On_GCP/08.Image Understanding with TensorFlow on GCP/Week_1/Lab_4_Image_Classification_with_a_CNN_Model/Practice/mnistmodel_keras_tf/trainer/model.py:162: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n",
      "W0805 10:05:15.341237 140584698599232 module_wrapper.py:138] From /media/mujahid7292/Data/GoogleDriveSandCorp2014/ML_With_TensorFlow_On_GCP/08.Image Understanding with TensorFlow on GCP/Week_1/Lab_4_Image_Classification_with_a_CNN_Model/Practice/mnistmodel_keras_tf/trainer/model.py:162: The name tf.estimator.inputs.numpy_input_fn is deprecated. Please use tf.compat.v1.estimator.inputs.numpy_input_fn instead.\n",
      "\n",
      "2020-08-05 10:05:15.346728: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2020-08-05 10:05:15.346747: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2020-08-05 10:05:15.346760: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (mujahid7292-HP-ENVY-Notebook): /proc/driver/nvidia/version does not exist\n",
      "2020-08-05 10:05:15.346890: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2020-08-05 10:05:15.369561: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2899885000 Hz\n",
      "2020-08-05 10:05:15.370164: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55d5b05aa850 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-08-05 10:05:15.370223: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "I0805 10:05:15.454445 140584698599232 run_config.py:535] TF_CONFIG environment variable: {'environment': 'cloud', 'cluster': {}, 'job': {'args': ['--output_dir=./mnist_keras_tf_trained', '--train_steps=100', '--learning_rate=0.01', '--job-dir', 'JOB_DIR'], 'job_name': 'trainer.task'}, 'task': {}}\n",
      "I0805 10:05:15.455140 140584698599232 keras.py:540] Using the Keras model provided.\n",
      "I0805 10:05:15.456790 140584698599232 estimator.py:216] Using config: {'_model_dir': './mnist_keras_tf_trained/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 60, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "I0805 10:05:15.457155 140584698599232 estimator_training.py:186] Not using Distribute Coordinator.\n",
      "I0805 10:05:15.457283 140584698599232 training.py:612] Running training and evaluation locally (non-distributed).\n",
      "I0805 10:05:15.457441 140584698599232 training.py:700] Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 60.\n",
      "W0805 10:05:15.462175 140584698599232 deprecation.py:506] From /home/mujahid7292/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "W0805 10:05:15.462455 140584698599232 deprecation.py:323] From /home/mujahid7292/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "W0805 10:05:15.469015 140584698599232 deprecation.py:323] From /home/mujahid7292/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_queue_runner.py:62: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "W0805 10:05:15.469611 140584698599232 deprecation.py:323] From /home/mujahid7292/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_functions.py:500: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "I0805 10:05:15.473054 140584698599232 estimator.py:1151] Calling model_fn.\n",
      "I0805 10:05:15.701227 140584698599232 estimator.py:1153] Done calling model_fn.\n",
      "I0805 10:05:15.701365 140584698599232 estimator.py:1372] Warm-starting with WarmStartSettings: WarmStartSettings(ckpt_to_initialize_from='./mnist_keras_tf_trained/keras/keras_model.ckpt', vars_to_warm_start='.*', var_name_to_vocab_info={}, var_name_to_prev_var_name={})\n",
      "I0805 10:05:15.701424 140584698599232 warm_starting_util.py:464] Warm-starting from: ./mnist_keras_tf_trained/keras/keras_model.ckpt\n",
      "I0805 10:05:15.701471 140584698599232 warm_starting_util.py:343] Warm-starting variables only in TRAINABLE_VARIABLES.\n",
      "I0805 10:05:15.719836 140584698599232 warm_starting_util.py:538] Warm-started 10 variables.\n",
      "I0805 10:05:15.720472 140584698599232 basic_session_run_hooks.py:546] Create CheckpointSaverHook.\n",
      "I0805 10:05:15.793051 140584698599232 monitored_session.py:246] Graph was finalized.\n",
      "I0805 10:05:15.794701 140584698599232 saver.py:1284] Restoring parameters from ./mnist_keras_tf_trained/model.ckpt-100\n",
      "W0805 10:05:15.835116 140584698599232 deprecation.py:323] From /home/mujahid7292/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py:1069: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file utilities to get mtimes.\n",
      "I0805 10:05:15.867284 140584698599232 session_manager.py:504] Running local_init_op.\n",
      "I0805 10:05:15.876339 140584698599232 session_manager.py:507] Done running local_init_op.\n",
      "W0805 10:05:15.908652 140584698599232 deprecation.py:323] From /home/mujahid7292/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/training/monitored_session.py:906: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "I0805 10:05:16.097710 140584698599232 basic_session_run_hooks.py:613] Saving checkpoints for 100 into ./mnist_keras_tf_trained/model.ckpt.\n",
      "I0805 10:05:16.303439 140584698599232 basic_session_run_hooks.py:262] loss = 0.21679088, step = 100\n",
      "I0805 10:05:16.303747 140584698599232 basic_session_run_hooks.py:613] Saving checkpoints for 101 into ./mnist_keras_tf_trained/model.ckpt.\n",
      "I0805 10:05:16.361241 140584698599232 estimator.py:1151] Calling model_fn.\n",
      "I0805 10:05:16.461537 140584698599232 estimator.py:1153] Done calling model_fn.\n",
      "I0805 10:05:16.473859 140584698599232 evaluation.py:255] Starting evaluation at 2020-08-05T10:05:16Z\n",
      "I0805 10:05:16.499250 140584698599232 monitored_session.py:246] Graph was finalized.\n",
      "I0805 10:05:16.500388 140584698599232 saver.py:1284] Restoring parameters from ./mnist_keras_tf_trained/model.ckpt-101\n",
      "I0805 10:05:16.531144 140584698599232 session_manager.py:504] Running local_init_op.\n",
      "I0805 10:05:16.539522 140584698599232 session_manager.py:507] Done running local_init_op.\n",
      "I0805 10:05:17.722722 140584698599232 evaluation.py:273] Inference Time : 1.24875s\n",
      "I0805 10:05:17.722929 140584698599232 evaluation.py:276] Finished evaluation at 2020-08-05-10:05:17\n",
      "I0805 10:05:17.723008 140584698599232 estimator.py:2053] Saving dict for global step 101: accuracy = 0.9302, global_step = 101, loss = 0.24256535\n",
      "I0805 10:05:17.746890 140584698599232 estimator.py:2113] Saving 'checkpoint_path' summary for global step 101: ./mnist_keras_tf_trained/model.ckpt-101\n",
      "I0805 10:05:17.751126 140584698599232 estimator.py:1151] Calling model_fn.\n",
      "I0805 10:05:17.825551 140584698599232 estimator.py:1153] Done calling model_fn.\n",
      "W0805 10:05:17.825705 140584698599232 deprecation.py:323] From /home/mujahid7292/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "I0805 10:05:17.825853 140584698599232 export_utils.py:170] Signatures INCLUDED in export for Classify: None\n",
      "I0805 10:05:17.825903 140584698599232 export_utils.py:170] Signatures INCLUDED in export for Regress: None\n",
      "I0805 10:05:17.825954 140584698599232 export_utils.py:170] Signatures INCLUDED in export for Predict: ['serving_default']\n",
      "I0805 10:05:17.825997 140584698599232 export_utils.py:170] Signatures INCLUDED in export for Train: None\n",
      "I0805 10:05:17.826037 140584698599232 export_utils.py:170] Signatures INCLUDED in export for Eval: None\n",
      "I0805 10:05:17.827848 140584698599232 saver.py:1284] Restoring parameters from ./mnist_keras_tf_trained/model.ckpt-101\n",
      "I0805 10:05:17.841552 140584698599232 builder_impl.py:666] Assets added to graph.\n",
      "I0805 10:05:17.841665 140584698599232 builder_impl.py:461] No assets to write.\n",
      "I0805 10:05:17.876332 140584698599232 builder_impl.py:426] SavedModel written to: ./mnist_keras_tf_trained/export/exporter/temp-b'1596600317'/saved_model.pb\n",
      "I0805 10:05:17.921420 140584698599232 estimator.py:375] Loss for final step: 0.21679088.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "rm -rf mnistmodel_keras_tf.tar.gz mnist_keras_tf_trained\n",
    "JOB_DIR=./tmp\n",
    "gcloud ml-engine local train \\\n",
    "    --module-name=trainer.task \\\n",
    "    --package-path=./mnistmodel_keras_tf/trainer \\\n",
    "    --job-dir=JOB_DIR \\\n",
    "    -- \\\n",
    "    --output_dir=./mnist_keras_tf_trained \\\n",
    "    --train_steps=100 \\\n",
    "    --learning_rate=0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><p style='color:red'>Loss for final step: 0.4481694</p></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run as python module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-05 10:05:50.472670: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory\n",
      "2020-08-05 10:05:50.472744: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory\n",
      "2020-08-05 10:05:50.472752: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0805 10:05:51.539287 140438488995648 module_wrapper.py:138] From /media/mujahid7292/Data/GoogleDriveSandCorp2014/ML_With_TensorFlow_On_GCP/08.Image Understanding with TensorFlow on GCP/Week_1/Lab_4_Image_Classification_with_a_CNN_Model/Practice/mnistmodel_keras_tf/trainer/model.py:162: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n",
      "W0805 10:05:51.539456 140438488995648 module_wrapper.py:138] From /media/mujahid7292/Data/GoogleDriveSandCorp2014/ML_With_TensorFlow_On_GCP/08.Image Understanding with TensorFlow on GCP/Week_1/Lab_4_Image_Classification_with_a_CNN_Model/Practice/mnistmodel_keras_tf/trainer/model.py:162: The name tf.estimator.inputs.numpy_input_fn is deprecated. Please use tf.compat.v1.estimator.inputs.numpy_input_fn instead.\n",
      "\n",
      "2020-08-05 10:05:51.544794: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2020-08-05 10:05:51.544816: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2020-08-05 10:05:51.544829: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (mujahid7292-HP-ENVY-Notebook): /proc/driver/nvidia/version does not exist\n",
      "2020-08-05 10:05:51.544950: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2020-08-05 10:05:51.569684: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2899885000 Hz\n",
      "2020-08-05 10:05:51.570563: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56015393ff40 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-08-05 10:05:51.570633: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "I0805 10:05:51.662855 140438488995648 keras.py:540] Using the Keras model provided.\n",
      "W0805 10:05:51.667331 140438488995648 deprecation.py:506] From /home/mujahid7292/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "I0805 10:05:52.054701 140438488995648 estimator.py:216] Using config: {'_model_dir': './mnist_keras_tf_trained/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 60, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "I0805 10:05:52.055137 140438488995648 estimator_training.py:186] Not using Distribute Coordinator.\n",
      "I0805 10:05:52.055260 140438488995648 training.py:612] Running training and evaluation locally (non-distributed).\n",
      "I0805 10:05:52.055406 140438488995648 training.py:700] Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 60.\n",
      "W0805 10:05:52.058222 140438488995648 deprecation.py:323] From /home/mujahid7292/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "W0805 10:05:52.064241 140438488995648 deprecation.py:323] From /home/mujahid7292/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_queue_runner.py:62: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "W0805 10:05:52.064797 140438488995648 deprecation.py:323] From /home/mujahid7292/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_functions.py:500: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "I0805 10:05:52.068331 140438488995648 estimator.py:1151] Calling model_fn.\n",
      "I0805 10:05:52.289578 140438488995648 estimator.py:1153] Done calling model_fn.\n",
      "I0805 10:05:52.289719 140438488995648 estimator.py:1372] Warm-starting with WarmStartSettings: WarmStartSettings(ckpt_to_initialize_from='./mnist_keras_tf_trained/keras/keras_model.ckpt', vars_to_warm_start='.*', var_name_to_vocab_info={}, var_name_to_prev_var_name={})\n",
      "I0805 10:05:52.289771 140438488995648 warm_starting_util.py:464] Warm-starting from: ./mnist_keras_tf_trained/keras/keras_model.ckpt\n",
      "I0805 10:05:52.289806 140438488995648 warm_starting_util.py:343] Warm-starting variables only in TRAINABLE_VARIABLES.\n",
      "I0805 10:05:52.307807 140438488995648 warm_starting_util.py:538] Warm-started 10 variables.\n",
      "I0805 10:05:52.308468 140438488995648 basic_session_run_hooks.py:546] Create CheckpointSaverHook.\n",
      "I0805 10:05:52.380612 140438488995648 monitored_session.py:246] Graph was finalized.\n",
      "I0805 10:05:52.467769 140438488995648 session_manager.py:504] Running local_init_op.\n",
      "I0805 10:05:52.476819 140438488995648 session_manager.py:507] Done running local_init_op.\n",
      "W0805 10:05:52.510233 140438488995648 deprecation.py:323] From /home/mujahid7292/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/training/monitored_session.py:906: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "I0805 10:05:52.692047 140438488995648 basic_session_run_hooks.py:613] Saving checkpoints for 0 into ./mnist_keras_tf_trained/model.ckpt.\n",
      "I0805 10:05:52.892674 140438488995648 basic_session_run_hooks.py:262] loss = 2.3036497, step = 0\n",
      "I0805 10:05:56.347291 140438488995648 basic_session_run_hooks.py:613] Saving checkpoints for 100 into ./mnist_keras_tf_trained/model.ckpt.\n",
      "I0805 10:05:56.405221 140438488995648 estimator.py:1151] Calling model_fn.\n",
      "I0805 10:05:56.560436 140438488995648 estimator.py:1153] Done calling model_fn.\n",
      "I0805 10:05:56.572964 140438488995648 evaluation.py:255] Starting evaluation at 2020-08-05T10:05:56Z\n",
      "I0805 10:05:56.598702 140438488995648 monitored_session.py:246] Graph was finalized.\n",
      "I0805 10:05:56.600089 140438488995648 saver.py:1284] Restoring parameters from ./mnist_keras_tf_trained/model.ckpt-100\n",
      "I0805 10:05:56.631478 140438488995648 session_manager.py:504] Running local_init_op.\n",
      "I0805 10:05:56.640224 140438488995648 session_manager.py:507] Done running local_init_op.\n",
      "I0805 10:05:57.841941 140438488995648 evaluation.py:273] Inference Time : 1.26885s\n",
      "I0805 10:05:57.842955 140438488995648 evaluation.py:276] Finished evaluation at 2020-08-05-10:05:57\n",
      "I0805 10:05:57.843114 140438488995648 estimator.py:2053] Saving dict for global step 100: accuracy = 0.9327, global_step = 100, loss = 0.21967228\n",
      "I0805 10:05:57.873578 140438488995648 estimator.py:2113] Saving 'checkpoint_path' summary for global step 100: ./mnist_keras_tf_trained/model.ckpt-100\n",
      "I0805 10:05:57.878238 140438488995648 estimator.py:1151] Calling model_fn.\n",
      "I0805 10:05:57.977466 140438488995648 estimator.py:1153] Done calling model_fn.\n",
      "W0805 10:05:57.977612 140438488995648 deprecation.py:323] From /home/mujahid7292/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "I0805 10:05:57.977754 140438488995648 export_utils.py:170] Signatures INCLUDED in export for Classify: None\n",
      "I0805 10:05:57.977799 140438488995648 export_utils.py:170] Signatures INCLUDED in export for Regress: None\n",
      "I0805 10:05:57.977834 140438488995648 export_utils.py:170] Signatures INCLUDED in export for Predict: ['serving_default']\n",
      "I0805 10:05:57.977864 140438488995648 export_utils.py:170] Signatures INCLUDED in export for Train: None\n",
      "I0805 10:05:57.977890 140438488995648 export_utils.py:170] Signatures INCLUDED in export for Eval: None\n",
      "I0805 10:05:57.979559 140438488995648 saver.py:1284] Restoring parameters from ./mnist_keras_tf_trained/model.ckpt-100\n",
      "I0805 10:05:57.994156 140438488995648 builder_impl.py:666] Assets added to graph.\n",
      "I0805 10:05:57.994261 140438488995648 builder_impl.py:461] No assets to write.\n",
      "I0805 10:05:58.031377 140438488995648 builder_impl.py:426] SavedModel written to: ./mnist_keras_tf_trained/export/exporter/temp-b'1596600357'/saved_model.pb\n",
      "I0805 10:05:58.082662 140438488995648 estimator.py:375] Loss for final step: 0.27253845.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "rm -rf mnistmodel_keras_tf.tar.gz mnist_keras_tf_trained\n",
    "python3 -m mnistmodel_keras_tf.trainer.task \\\n",
    "    --learning_rate=0.01 \\\n",
    "    --train_steps=100 \\\n",
    "    --output_dir=./mnist_keras_tf_trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><p style='color:red'>Loss for final step: 0.42073974</p></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, let's do it on Cloud ML Engine so we can train on GPU (`--scale-tier=BASIC_GPU`)**\n",
    "\n",
    "Note the GPU speed up depends on the model type. You'll notice the more complex CNN model trains significantly faster on GPU, however the speed up on the simpler models is not as pronounced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://bucket-ml-practice-260405/mnist/trained_dnn us-central1 mnist_dnn_200805_040956\n",
      "jobId: mnist_dnn_200805_040956\n",
      "state: QUEUED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CommandException: 1 files/objects could not be removed.\n",
      "WARNING: The `gcloud ml-engine` commands have been renamed and will soon be removed. Please use `gcloud ai-platform` instead.\n",
      "Job [mnist_dnn_200805_040956] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe mnist_dnn_200805_040956\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs mnist_dnn_200805_040956\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "OUTDIR=gs://${BUCKET}/mnist/trained_dnn\n",
    "JOBNAME=mnist_cnn_$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "    --region=$REGION \\\n",
    "    --module-name=trainer.task \\\n",
    "    --package-path=./mnistmodel_keras_tf/trainer \\\n",
    "    --job-dir=$OUTDIR \\\n",
    "    --staging-bucket=gs://$BUCKET \\\n",
    "    --scale-tier=BASIC_GPU \\\n",
    "    --runtime-version=$TFVERSION \\\n",
    "    -- \\\n",
    "    --output_dir=$OUTDIR \\\n",
    "    --train_steps=100 \\\n",
    "    --learning_rate=0.01 \\\n",
    "    --train_batch_size=512 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><p style='color:red'>Loss for final step: 0.5132059</p></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying and predicting with model\n",
    "\n",
    "Deploy the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting and deploying mnist_dnn_dropout cnn from  ... this will take a few minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CommandException: One or more URLs matched no objects.\n",
      "WARNING: The `gcloud ml-engine` commands have been renamed and will soon be removed. Please use `gcloud ai-platform` instead.\n",
      "WARNING: Using endpoint [https://ml.googleapis.com/]\n",
      "Created ml engine model [projects/ml-practice-260405/models/mnist_dnn_dropout].\n",
      "ERROR: (gcloud.ml-engine.versions.create) argument --origin: expected one argument\n",
      "Usage: gcloud ml-engine versions create VERSION --model=MODEL [optional flags]\n",
      "  optional flags may be  --async | --config | --description | --framework |\n",
      "                         --help | --labels | --origin | --python-version |\n",
      "                         --runtime-version | --staging-bucket\n",
      "\n",
      "For detailed information on this command and its flags, run:\n",
      "  gcloud ml-engine versions create --help\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'MODEL_NAME=\"mnist_dnn_dropout\"\\nMODEL_VERSION=${MODEL_TYPE}\\nMODEL_LOCATION=$(gsutil ls gs://${BUCKET}/mnist/trained_${MODEL_TYPE}/export/exporter | tail -1)\\necho \"Deleting and deploying $MODEL_NAME $MODEL_VERSION from $MODEL_LOCATION ... this will take a few minutes\"\\n#gcloud ml-engine versions delete ${MODEL_VERSION} --model ${MODEL_NAME}\\n#gcloud ml-engine models delete ${MODEL_NAME}\\ngcloud ml-engine models create ${MODEL_NAME} --regions $REGION\\ngcloud ml-engine versions create ${MODEL_VERSION} --model ${MODEL_NAME} --origin ${MODEL_LOCATION} --runtime-version=$TFVERSION\\n'' returned non-zero exit status 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-1e6101c45617>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bash'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'MODEL_NAME=\"mnist_dnn_dropout\"\\nMODEL_VERSION=${MODEL_TYPE}\\nMODEL_LOCATION=$(gsutil ls gs://${BUCKET}/mnist/trained_${MODEL_TYPE}/export/exporter | tail -1)\\necho \"Deleting and deploying $MODEL_NAME $MODEL_VERSION from $MODEL_LOCATION ... this will take a few minutes\"\\n#gcloud ml-engine versions delete ${MODEL_VERSION} --model ${MODEL_NAME}\\n#gcloud ml-engine models delete ${MODEL_NAME}\\ngcloud ml-engine models create ${MODEL_NAME} --regions $REGION\\ngcloud ml-engine versions create ${MODEL_VERSION} --model ${MODEL_NAME} --origin ${MODEL_LOCATION} --runtime-version=$TFVERSION\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2357\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2358\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2359\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2360\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mnamed_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshebang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# write a basic docstring:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m</home/mujahid7292/anaconda3/lib/python3.7/site-packages/decorator.py:decorator-gen-110>\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_error\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_close\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'MODEL_NAME=\"mnist_dnn_dropout\"\\nMODEL_VERSION=${MODEL_TYPE}\\nMODEL_LOCATION=$(gsutil ls gs://${BUCKET}/mnist/trained_${MODEL_TYPE}/export/exporter | tail -1)\\necho \"Deleting and deploying $MODEL_NAME $MODEL_VERSION from $MODEL_LOCATION ... this will take a few minutes\"\\n#gcloud ml-engine versions delete ${MODEL_VERSION} --model ${MODEL_NAME}\\n#gcloud ml-engine models delete ${MODEL_NAME}\\ngcloud ml-engine models create ${MODEL_NAME} --regions $REGION\\ngcloud ml-engine versions create ${MODEL_VERSION} --model ${MODEL_NAME} --origin ${MODEL_LOCATION} --runtime-version=$TFVERSION\\n'' returned non-zero exit status 2."
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "MODEL_NAME=\"mnist_cnn_dropout\"\n",
    "MODEL_VERSION=${MODEL_TYPE}\n",
    "MODEL_LOCATION=$(gsutil ls gs://${BUCKET}/mnist/trained_${MODEL_TYPE}/export/exporter | tail -1)\n",
    "echo \"Deleting and deploying $MODEL_NAME $MODEL_VERSION from $MODEL_LOCATION ... this will take a few minutes\"\n",
    "#gcloud ml-engine versions delete ${MODEL_VERSION} --model ${MODEL_NAME}\n",
    "#gcloud ml-engine models delete ${MODEL_NAME}\n",
    "gcloud ml-engine models create ${MODEL_NAME} --regions $REGION\n",
    "gcloud ml-engine versions create ${MODEL_VERSION} --model ${MODEL_NAME} --origin ${MODEL_LOCATION} --runtime-version=$TFVERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To predict with the model, let's take one of the example images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, codecs\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "HEIGHT = 28\n",
    "WIDTH = 28\n",
    "\n",
    "# Get mnist data\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(_, _), (x_test, _) = mnist.load_data()\n",
    "\n",
    "# Scale our features between 0 and 1\n",
    "x_test = x_test / 255.0 \n",
    "\n",
    "IMGNO = 5 # CHANGE THIS to get different images\n",
    "jsondata = {\"image\": x_test[IMGNO].reshape(HEIGHT, WIDTH).tolist()}\n",
    "json.dump(jsondata, codecs.open(\"test.json\", 'w', encoding = \"utf-8\"))\n",
    "plt.imshow(x_test[IMGNO].reshape(HEIGHT, WIDTH));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Send it to the prediction service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gcloud ai-platform predict \\\n",
    "    --model=mnist_dnn_dropout \\\n",
    "    --version=${MODEL_TYPE} \\\n",
    "    --json-instances=./test.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Trained Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf mnistmodel_keras_tf.tar.gz mnist_keras_tf_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
