{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST image classification with Keras only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import json\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = \"ml-practice-260405\" # REPLACE WITH YOUR PROJECT ID\n",
    "BUCKET = \"bucket-ml-practice-260405\" # REPLACE WITH YOUR BUCKET NAME\n",
    "REGION = \"us-central1\" # REPLACE WITH YOUR BUCKET REGION e.g. us-central1\n",
    "MODEL_TYPE = \"dnn\"  # \"linear\", \"dnn\", \"dnn_dropout\", or \"cnn\"\n",
    "SAC = 'jupyter-notebook-sac-f'\n",
    "SAC_KEY_DESTINATION = '/media/mujahid7292/Data/Gcloud_Tem_SAC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not change \n",
    "os.environ[\"ACCOUNT\"] = \"sandcorp2014@gmail.com\"\n",
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"BUCKET\"] = BUCKET\n",
    "os.environ[\"REGION\"] = REGION\n",
    "os.environ['SAC'] = SAC\n",
    "os.environ['SAC_KEY_DESTINATION'] = SAC_KEY_DESTINATION\n",
    "os.environ[\"MODEL_TYPE\"] = MODEL_TYPE\n",
    "os.environ[\"TFVERSION\"] = \"2.1.0\"  # Tensorflow version\n",
    "os.environ[\"IMAGE_URI\"] = os.path.join(\"gcr.io\", PROJECT, \"mnistmodel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gcloud config set account $ACCOUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activate the service account with above key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gcloud auth activate-service-account \\\n",
    "--key-file=${SAC_KEY_DESTINATION}/${SAC}.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Google Application Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]='{}/{}.json'.format(SAC_KEY_DESTINATION,SAC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Whether Google Application Credential Was Set Successfully Outside Virtual Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "set | grep GOOGLE_APPLICATION_CREDENTIALS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Default Project And Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gcloud config set account $ACCOUNT\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Give GCS Access Permision To This Service Account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gcloud projects add-iam-policy-binding $PROJECT \\\n",
    "    --member serviceAccount:$SAC@$PROJECT.iam.gserviceaccount.com \\\n",
    "    --role roles/storage.admin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir mnistmodel_keras_only\n",
    "mkdir mnistmodel_keras_only/trainer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mnistmodel_keras_only/trainer/__init__.py\n",
    "# Empty file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mnistmodel_keras_only/trainer/task.py\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from . import model\n",
    "\n",
    "def _parse_arguments(argv):\n",
    "    \"\"\"\n",
    "    Parse command line arguments\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    parser.add_argument(\n",
    "        '--model_type',\n",
    "        help=\"Which model type to use.\",\n",
    "        type=str,\n",
    "        default='dnn'\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        '--epochs',\n",
    "        help='The number of epochs to train.',\n",
    "        type=int,\n",
    "        default=10\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        '--steps_per_epoch',\n",
    "        help='The number of steps per epoch to train.',\n",
    "        type=int,\n",
    "        default=100\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        '--job-dir',\n",
    "        help=\"Directory where to save the model.\",\n",
    "        type=str,\n",
    "        default='mnistmodel_keras_only/'\n",
    "    )\n",
    "    \n",
    "    return parser.parse_known_args(argv)\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Parse command line arguments and kicks off model training.\n",
    "    \"\"\"\n",
    "    args = _parse_arguments(sys.argv[1:])[0]\n",
    "    \n",
    "    model_layers = model.get_layers(args.model_type)\n",
    "    \n",
    "    image_model = model.build_model(model_layers, args.job_dir)\n",
    "    \n",
    "    model_history = model.train_and_evaluate(\n",
    "        image_model, args.epochs, args.steps_per_epoch, args.job_dir\n",
    "    )\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's group non-model functions into a util file to keep the model file simple. We'll copy over the `scale` and `load_dataset` functions from the previous lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mnistmodel_keras_only/trainer/util.py\n",
    "import tensorflow as tf\n",
    "\n",
    "def scale(image, label):\n",
    "    \"\"\"\n",
    "    Scale image from 0 to 255 int range to a 0.0 to 1.0 float range\n",
    "    \"\"\"\n",
    "    image = tf.cast(x=image, dtype=tf.float32)\n",
    "    image /= 255\n",
    "    image = tf.expand_dims(input=image, axis=-1)\n",
    "    return image, label\n",
    "\n",
    "def load_dataset(data, training=True, buffer_size=5000, batch_size=100, nclasses=10):\n",
    "    \"\"\"\n",
    "    Loads MNIST dataset into a tf.data.Dataset\n",
    "    \"\"\"\n",
    "    (x_train, y_train), (x_test, y_test) = data\n",
    "    \n",
    "    x = x_train if training else x_test\n",
    "    y = y_train if training else y_test\n",
    "    \n",
    "    # One-hot encode the class\n",
    "    y = tf.keras.utils.to_categorical(y = y, num_classes=nclasses)\n",
    "    \n",
    "    # Convert our data into tf.data\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x,y))\n",
    "    dataset = dataset.map(scale).batch(batch_size)\n",
    "    \n",
    "    # During training shuffle our dataset\n",
    "    if training:\n",
    "        dataset = dataset.shuffle(buffer_size).repeat()\n",
    "        \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's code the models! The [tf.keras API](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras) accepts an array of [layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers) into a [model object](https://www.tensorflow.org/api_docs/python/tf/keras/Model), so we can create a dictionary of layers based on the different model types we want to use. The below file has two functions: `get_layers` and `create_and_train_model`. We will build the structure of our model in `get_layers`. Last but not least, we'll copy over the training code from the previous lab into `train_and_evaluate`.\n",
    "\n",
    "These models progressively build on each other. Look at the imported `tensorflow.keras.layers` modules and the default values for the variables defined in `get_layers` for guidance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mnistmodel_keras_only/trainer/model.py\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.layers import (Conv2D, Dense, Dropout, Flatten,\n",
    "                                     MaxPooling2D, Softmax)\n",
    "\n",
    "from . import util\n",
    "\n",
    "# Image Variables\n",
    "WIDTH = 28\n",
    "HEIGHT = 28\n",
    "\n",
    "def get_layers(model_type, nclasses = 10, hidden_layer_1_neurons=400,\n",
    "              hidden_layer_2_neurons=100):\n",
    "    \"\"\"\n",
    "    Construct layers for keras model based on a dict of model types.\n",
    "    \"\"\"\n",
    "    model_layers = {\n",
    "        'linear':[\n",
    "            Flatten(),\n",
    "            Dense(nclasses),\n",
    "            Softmax()\n",
    "        ],\n",
    "        'dnn':[\n",
    "            Flatten(),\n",
    "            Dense(hidden_layer_1_neurons, activation='relu'),\n",
    "            Dense(hidden_layer_2_neurons, activation='relu'),\n",
    "            Dense(nclasses),\n",
    "            Softmax()\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    return model_layers[model_type]\n",
    "\n",
    "def build_model(layers, output_dir):\n",
    "    \"\"\"\n",
    "    Compiles keras model for image classification.\n",
    "    \"\"\"\n",
    "    model = Sequential(layers)\n",
    "    model.compile(optimizer='adam',\n",
    "                 loss='categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def train_and_evaluate(model, num_epochs, steps_per_epoch, output_dir):\n",
    "    \"\"\"\n",
    "    Compiles keras model and loads data into it for training.\n",
    "    \"\"\"\n",
    "    # Load MNIST dataset\n",
    "    mnist = tf.keras.datasets.mnist.load_data()\n",
    "    \n",
    "    # Spilt dataset into train and validation.\n",
    "    train_data = util.load_dataset(mnist)\n",
    "    validation_data = util.load_dataset(mnist, training=False)\n",
    "    \n",
    "    # Create TensorBoard callback\n",
    "    callbacks = []\n",
    "    if output_dir:\n",
    "        tensorboard_callback = TensorBoard(log_dir=output_dir)\n",
    "        callbacks = [tensorboard_callback]\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        train_data,\n",
    "        validation_data=validation_data,\n",
    "        epochs=num_epochs,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        verbose=2,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    \n",
    "    # Now save the trained model\n",
    "    if output_dir:\n",
    "        export_path = os.path.join(output_dir, 'keras_export')\n",
    "        model.save(export_path, save_format='tf')\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run as a Python module\n",
    "\n",
    "Since we want to run our code on Cloud ML Engine, we've packaged it as a python module.\n",
    "\n",
    "The `model.py` and `task.py` containing the model code is in <a href=\"mnistmodel_keras_only/trainer\">mnistmodel_keras_only/trainer</a>\n",
    "\n",
    "**Let's first run it locally for a few steps to test the code.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "MODEL_TYPE='dnn'\n",
    "JOB_DIR='mnistmodel_keras_only'\n",
    "rm -rf mnistmodel_keras_only.tar.gz mnist_keras_only_trained\n",
    "python3 -m mnistmodel_keras_only.trainer.task \\\n",
    "    --job-dir=${JOB_DIR}\\\n",
    "    --epochs=5 \\\n",
    "    --steps_per_epoch=50 \\\n",
    "    --model_type=${MODEL_TYPE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "MODEL_TYPE='dnn'\n",
    "JOB_DIR='mnistmodel_keras_only'\n",
    "rm -rf mnistmodel_keras_only.tar.gz mnist_keras_only_trained\n",
    "JOB_DIR=./tmp\n",
    "gcloud ml-engine local train \\\n",
    "    --module-name=trainer.task \\\n",
    "    --package-path=./mnistmodel_keras_only/trainer \\\n",
    "    -- \\\n",
    "    --job-dir=${JOB_DIR}\\\n",
    "    --epochs=10 \\\n",
    "    --steps_per_epoch=50 \\\n",
    "    --model_type=${MODEL_TYPE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, let's do it on Cloud ML Engine so we can train on GPU (`--scale-tier=BASIC_GPU`)**\n",
    "\n",
    "Note the GPU speed up depends on the model type. You'll notice the more complex CNN model trains significantly faster on GPU, however the speed up on the simpler models is not as pronounced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "OUTDIR=gs://${BUCKET}/mnist/trained_dnn\n",
    "JOBNAME=mnist_dnn_$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "    --region=$REGION \\\n",
    "    --module-name=trainer.task \\\n",
    "    --package-path=./mnistmodel_keras_only/trainer \\\n",
    "    --job-dir=$OUTDIR \\\n",
    "    --staging-bucket=gs://$BUCKET \\\n",
    "    --scale-tier=BASIC_GPU \\\n",
    "    --runtime-version=$TFVERSION \\\n",
    "    -- \\\n",
    "    --epochs=50 \\\n",
    "    --steps_per_epoch=50 \\\n",
    "    --model_type=$MODEL_TYPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Training\n",
    "\n",
    "Now that we know that our models are working as expected, let's run it on the [Google Cloud AI Platform](https://cloud.google.com/ml-engine/docs/). We can run it as a python module locally first using the command line.\n",
    "\n",
    "The below cell transfers some of our variables to the command line as well as create a job directory including a timestamp.\n",
    "\n",
    "You can change the model_type to try out different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.now().strftime('%y%m%d_%H%M%S')\n",
    "model_type='dnn'\n",
    "\n",
    "os.environ['MODEL_TYPE']=model_type\n",
    "os.environ['JOB_DIR']=\"gs://{}/mnist_{}_{}\".format(BUCKET,model_type,current_time)\n",
    "os.environ['JOB_NAME']=\"mnist_{}_{}\".format(model_type, current_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below runs the local version of the code. The epochs and steps_per_epoch flag can be changed to run for longer or shorther, as defined in our `mnistmodel_keras_only/trainer/task.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "python3 -m mnistmodel_keras_only.trainer.task \\\n",
    "    --job-dir=$JOB_DIR \\\n",
    "    --epochs=5 \\\n",
    "    --steps_per_epoch=50 \\\n",
    "    --model_type=$MODEL_TYPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on the cloud\n",
    "\n",
    "Since we're using an unreleased version of TensorFlow on AI Platform, we can instead use a [Deep Learning Container](https://cloud.google.com/ai-platform/deep-learning-containers/docs/overview) in order to take advantage of libraries and applications not normally packaged with AI Platform. Below is a simple [Dockerlife](https://docs.docker.com/engine/reference/builder/) which copies our code to be used in a TF2 environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mnistmodel_keras_only/Dockerfile\n",
    "FROM gcr.io/deeplearning-platform-release/tf2-cpu\n",
    "COPY mnistmodel_keras_only/trainer mnistmodel/trainer\n",
    "ENTRYPOINT [\"python3\", \"-m\", \"mnistmodel.trainer.task\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below command builds the image and ships it off to Google Cloud so it can be used for AI Platform. When built, it will show up [here](http://console.cloud.google.com/gcr) with the name `mnistmodel`. ([Click here](https://console.cloud.google.com/cloud-build) to enable Cloud Build)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><p style='color:red'>This below `docker buid` and `docker push` command will not run in this laptop. It will only run in `AI Platmorm` notebook. So stop here.</p></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker build -f mnistmodel_keras_only/Dockerfile -t $IMAGE_URI ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker push $IMAGE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can kickoff the [AI Platform training job](https://cloud.google.com/sdk/gcloud/reference/ai-platform/jobs/submit/training). We can pass in our docker image using the `master-image-uri` flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo $JOB_DIR $REGION $JOB_NAME\n",
    "gcloud ai-platform jobs submit training $JOB_NAME \\\n",
    "    --staging-bucket=gs://$BUCKET \\\n",
    "    --region=$REGION \\\n",
    "    --master-image-uri=$IMAGE_URI \\\n",
    "    --scale-tier=BASIC_GPU \\\n",
    "    --job-dir=$JOB_DIR \\\n",
    "    -- \\\n",
    "    --model_type=$MODEL_TYPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can't wait to see the results? Run the code below and copy the output into the [Google Cloud Shell](https://console.cloud.google.com/home/dashboard?cloudshell=true) to follow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying and predicting with model\n",
    "\n",
    "Once you have a model you're proud of, let's deploy it! All we need to do is give AI Platform the location of the model. Below uses the keras export path of the previous job, but `${JOB_DIR}keras_export/` can always be changed to a different path.\n",
    "\n",
    "Uncomment the delete commands below if you are getting an \"already exists error\" and want to deploy a new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "MODEL_NAME=\"mnist\"\n",
    "MODEL_VERSION=${MODEL_TYPE}\n",
    "MODEL_LOCATION=${JOB_DIR}keras_export/\n",
    "echo \"Deleting and deploying $MODEL_NAME $MODEL_VERSION from $MODEL_LOCATION ... this will take a few minutes\"\n",
    "#yes | gcloud ai-platform versions delete ${MODEL_VERSION} --model ${MODEL_NAME}\n",
    "#yes | gcloud ai-platform models delete ${MODEL_NAME}\n",
    "gcloud ai-platform models create ${MODEL_NAME} --regions $REGION\n",
    "gcloud ai-platform versions create ${MODEL_VERSION} \\\n",
    "    --model ${MODEL_NAME} \\\n",
    "    --origin ${MODEL_LOCATION} \\\n",
    "    --framework tensorflow \\\n",
    "    --runtime-version=2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
