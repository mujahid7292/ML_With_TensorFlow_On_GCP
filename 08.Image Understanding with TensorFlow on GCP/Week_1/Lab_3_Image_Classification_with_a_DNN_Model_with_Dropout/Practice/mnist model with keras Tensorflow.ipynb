{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST image classification with Keras Tensorflow backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import json\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = \"ml-practice-260405\" # REPLACE WITH YOUR PROJECT ID\n",
    "BUCKET = \"ml-practice-260405\" # REPLACE WITH YOUR BUCKET NAME\n",
    "REGION = \"us-central1\" # REPLACE WITH YOUR BUCKET REGION e.g. us-central1\n",
    "MODEL_TYPE = \"dnn_dropout\"  # \"linear\", \"dnn\", \"dnn_dropout\", or \"cnn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not change \n",
    "os.environ[\"ACCOUNT\"] = \"sandcorp2014@gmail.com\"\n",
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"BUCKET\"] = BUCKET\n",
    "os.environ[\"REGION\"] = REGION\n",
    "os.environ[\"MODEL_TYPE\"] = MODEL_TYPE\n",
    "os.environ[\"TFVERSION\"] = \"1.13\"  # Tensorflow version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gcloud config set account $ACCOUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir mnistmodel_keras_tf\n",
    "mkdir mnistmodel_keras_tf/trainer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mnistmodel_keras_tf/trainer/__init__.py\n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mnistmodel_keras_tf/trainer/task.py\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "from .import model\n",
    "import tensorflow as tf\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Create a parser object\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    # Input argument in the parser object\n",
    "    parser.add_argument(\n",
    "        \"--train_batch_size\",\n",
    "        help=\"Batch size for training\",\n",
    "        type = int,\n",
    "        default=100\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        \"--learning_rate\",\n",
    "        help=\"Initial Learning Rate For Training\",\n",
    "        type=float,\n",
    "        default=0.01\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        \"--train_steps\",\n",
    "        help=\"Steps to run the training jobs for\",\n",
    "        type=int,\n",
    "        default=0\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        \"--output_dir\",\n",
    "        help=\"Local Or GCS location to write checkpoint and export model\",\n",
    "        required=True\n",
    "    )\n",
    "    \n",
    "    # Generate List of models to print in help message\n",
    "    model_names = [name.replace(\"_model\",\"\") for name in dir(model) if name.endswith(\"_model\")]\n",
    "    \n",
    "    parser.add_argument(\n",
    "        '--model',\n",
    "        help=\"Type of model. Supported types are {}\".format(model_names),\n",
    "        default='linear'\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        \"--job-dir\",\n",
    "        help = \"this model ignores this field, but it is required by gcloud\",\n",
    "        default = \"junk\"\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        '--dprob',\n",
    "        help ='Drop out probability for cnn',\n",
    "        type = float,\n",
    "        default=0.25\n",
    "    )\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    hparams = args.__dict__\n",
    "\n",
    "    # unused args provided by service\n",
    "    hparams.pop(\"job_dir\", None)\n",
    "    hparams.pop(\"job-dir\", None)\n",
    "\n",
    "    output_dir = hparams.pop(\"output_dir\")\n",
    "    # Append trial_id to path so hptuning jobs don\"t overwrite eachother\n",
    "    output_dir = os.path.join(\n",
    "        output_dir,\n",
    "        json.loads(\n",
    "            os.environ.get(\"TF_CONFIG\", \"{}\")\n",
    "        ).get(\"task\", {}).get(\"trial\", \"\")\n",
    "    )\n",
    "    \n",
    "    # Calculate train_steps if not provided\n",
    "    if hparams[\"train_steps\"] < 1:\n",
    "        # 10,000 steps with batch_size of 512\n",
    "        hparams[\"train_steps\"] = (10000 * 512) // hparams[\"train_batch_size\"]\n",
    "        print(\"Training for {} steps.\".format(hparams[\"train_steps\"]))\n",
    "    \n",
    "    # Run the training job\n",
    "    model.train_and_evaluate(output_dir, hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mnistmodel_keras_tf/trainer/model.py\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)\n",
    "\n",
    "HEIGHT=28\n",
    "WIDTH=28\n",
    "NCLASSES=10\n",
    "\n",
    "# Build Keras model using Keras Sequential API\n",
    "def linear_model(hparams):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.InputLayer(input_shape=[HEIGHT, WIDTH, 1], name=\"image\"))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(\n",
    "        units=NCLASSES,\n",
    "        activation=tf.nn.softmax,\n",
    "        name=\"probabilities\"\n",
    "    ))\n",
    "    return model\n",
    "    \n",
    "def dnn_model(hparams):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.InputLayer(input_shape=[HEIGHT, WIDTH, 1], name=\"image\"))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(units=300, activation=tf.nn.relu))\n",
    "    model.add(tf.keras.layers.Dense(units=100, activation=tf.nn.relu))\n",
    "    model.add(tf.keras.layers.Dense(units=30, activation=tf.nn.relu))\n",
    "    model.add(tf.keras.layers.Dense(units=NCLASSES, activation=tf.nn.softmax, name='Probabilities'))\n",
    "    return model\n",
    "\n",
    "def dnn_dropout_model(hparams):\n",
    "    dprob = hparams.get(\"dprob\", 0.1)\n",
    "    \n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.InputLayer(input_shape=[HEIGHT, WIDTH, 1], name=\"image\"))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(units=300, activation=tf.nn.relu))\n",
    "    model.add(tf.keras.layers.Dense(units=100, activation=tf.nn.relu))\n",
    "    model.add(tf.keras.layers.Dense(units=30, activation=tf.nn.relu))\n",
    "    model.add(tf.keras.layers.Dropout(rate=dprob))\n",
    "    model.add(tf.keras.layers.Dense(\n",
    "        units=NCLASSES, \n",
    "        activation=tf.nn.softmax,\n",
    "        name=\"probabilities\"\n",
    "    ))\n",
    "    return model\n",
    "\n",
    "# Create serving input function for inference\n",
    "def serving_input_fn():\n",
    "    # Input will be rank 3\n",
    "    feature_placeholders = {\n",
    "        \"image\": tf.compat.v1.placeholder(dtype = tf.float32, shape=[None, HEIGHT, WIDTH])\n",
    "    }\n",
    "    # But model function require rank 4\n",
    "    features = {\n",
    "        \"image\": tf.expand_dims(input=feature_placeholders[\"image\"], axis=-1)\n",
    "    }\n",
    "    return tf.estimator.export.ServingInputReceiver(\n",
    "        features=features,\n",
    "        receiver_tensors=feature_placeholders\n",
    "    )\n",
    "\n",
    "    \n",
    "# Create train and evaluate function\n",
    "def train_and_evaluate(ouput_dir, hparams):\n",
    "    # Ensure filewriter cache is clear for TensorBoard event file.\n",
    "    tf.compat.v1.summary.FileWriterCache.clear()\n",
    "    \n",
    "    EVAL_INTERVAL = 60\n",
    "    \n",
    "    # Get mnist Data\n",
    "    mnist = tf.keras.datasets.mnist\n",
    "    \n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    \n",
    "    # Scale our training and testing features between 0 and 1\n",
    "    x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "    \n",
    "    # Reshape image to add a dimension for channels (1 in this case)\n",
    "    x_train = x_train.reshape([-1, HEIGHT, WIDTH, 1])\n",
    "    x_test = x_test.reshape([-1, HEIGHT, WIDTH, 1])\n",
    "    \n",
    "    # Convert labels to categorical one hot encoding\n",
    "    y_train = tf.keras.utils.to_categorical(y=y_train, num_classes=NCLASSES)\n",
    "    y_test = tf.keras.utils.to_categorical(y=y_test, num_classes=NCLASSES)\n",
    "    \n",
    "    # Create training input function\n",
    "    train_input_fn = tf.compat.v1.estimator.inputs.numpy_input_fn(\n",
    "        x={\"image\": x_train},\n",
    "        y=y_train,\n",
    "        batch_size=100,\n",
    "        num_epochs=None,\n",
    "        shuffle=True,\n",
    "        queue_capacity=5000\n",
    "    )\n",
    "    \n",
    "    # Create evaluation input function\n",
    "    eval_input_fn = tf.compat.v1.estimator.inputs.numpy_input_fn(\n",
    "        x={\"image\": x_test},\n",
    "        y=y_test,\n",
    "        batch_size=100,\n",
    "        num_epochs=1,\n",
    "        shuffle=False,\n",
    "        queue_capacity=5000\n",
    "    )\n",
    "    \n",
    "    # Build Keras model\n",
    "    model = dnn_dropout_model(hparams=hparams)\n",
    "    \n",
    "    # Compile Keras model with optimizer, loss function and eval metric\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Convert Keras model to estimator\n",
    "    estimator = tf.keras.estimator.model_to_estimator(\n",
    "        keras_model=model,\n",
    "        model_dir=ouput_dir,\n",
    "        config=tf.estimator.RunConfig(save_checkpoints_secs=EVAL_INTERVAL)\n",
    "    )\n",
    "    \n",
    "    # Set estimator's train_spec to use train_input_fn() and train for so many steps\n",
    "    train_spec = tf.estimator.TrainSpec(\n",
    "        input_fn=train_input_fn,\n",
    "        max_steps=hparams['train_steps']\n",
    "    )\n",
    "    \n",
    "    # Create exporter that use serving_input_fn() to create saved_model for serving\n",
    "    exporter = tf.estimator.LatestExporter(\n",
    "        name='exporter',\n",
    "        serving_input_receiver_fn=serving_input_fn\n",
    "    )\n",
    "    \n",
    "    # Set estimators eval_spec to use eval_input_fn() and export saved_model\n",
    "    eval_spec = tf.estimator.EvalSpec(\n",
    "        input_fn=eval_input_fn,\n",
    "        steps=None,\n",
    "        exporters=exporter,\n",
    "        throttle_secs=EVAL_INTERVAL\n",
    "    )\n",
    "    \n",
    "    # Run train_and_evaluate loop\n",
    "    tf.estimator.train_and_evaluate(\n",
    "        estimator=estimator,\n",
    "        train_spec=train_spec,\n",
    "        eval_spec=eval_spec\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run as a Python module\n",
    "\n",
    "Since we want to run our code on Cloud ML Engine, we've packaged it as a python module.\n",
    "\n",
    "The `model.py` and `task.py` containing the model code is in <a href=\"mnistmodel_keras_tf/trainer\">mnistmodel_keras_tf/trainer</a>\n",
    "\n",
    "**Let's first run it locally for a few steps to test the code.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf mnistmodel_keras_tf.tar.gz mnist_keras_tf_trained\n",
    "JOB_DIR=./tmp\n",
    "gcloud ml-engine local train \\\n",
    "    --module-name=trainer.task \\\n",
    "    --package-path=./mnistmodel_keras_tf/trainer \\\n",
    "    --job-dir=JOB_DIR \\\n",
    "    -- \\\n",
    "    --output_dir=./mnist_keras_tf_trained \\\n",
    "    --train_steps=100 \\\n",
    "    --learning_rate=0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><p style='color:red'>Loss for final step: 0.4481694</p></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run as python module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf mnistmodel_keras_tf.tar.gz mnist_keras_tf_trained\n",
    "python3 -m mnistmodel_keras_tf.trainer.task \\\n",
    "    --learning_rate=0.01 \\\n",
    "    --train_steps=100 \\\n",
    "    --output_dir=./mnist_keras_tf_trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><p style='color:red'>Loss for final step: 0.42073974</p></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, let's do it on Cloud ML Engine so we can train on GPU (`--scale-tier=BASIC_GPU`)**\n",
    "\n",
    "Note the GPU speed up depends on the model type. You'll notice the more complex CNN model trains significantly faster on GPU, however the speed up on the simpler models is not as pronounced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "OUTDIR=gs://${BUCKET}/mnist/trained_dnn\n",
    "JOBNAME=mnist_dnn_$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "    --region=$REGION \\\n",
    "    --module-name=trainer.task \\\n",
    "    --package-path=./mnistmodel_keras_tf/trainer \\\n",
    "    --job-dir=$OUTDIR \\\n",
    "    --staging-bucket=gs://$BUCKET \\\n",
    "    --scale-tier=BASIC_GPU \\\n",
    "    --runtime-version=$TFVERSION \\\n",
    "    -- \\\n",
    "    --output_dir=$OUTDIR \\\n",
    "    --train_steps=100 \\\n",
    "    --learning_rate=0.01 \\\n",
    "    --train_batch_size=512 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><p style='color:red'>Loss for final step: 0.5132059</p></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying and predicting with model\n",
    "\n",
    "Deploy the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "MODEL_NAME=\"mnist_dnn_dropout\"\n",
    "MODEL_VERSION=${MODEL_TYPE}\n",
    "MODEL_LOCATION=$(gsutil ls gs://${BUCKET}/mnist/trained_${MODEL_TYPE}/export/exporter | tail -1)\n",
    "echo \"Deleting and deploying $MODEL_NAME $MODEL_VERSION from $MODEL_LOCATION ... this will take a few minutes\"\n",
    "#gcloud ml-engine versions delete ${MODEL_VERSION} --model ${MODEL_NAME}\n",
    "#gcloud ml-engine models delete ${MODEL_NAME}\n",
    "gcloud ml-engine models create ${MODEL_NAME} --regions $REGION\n",
    "gcloud ml-engine versions create ${MODEL_VERSION} --model ${MODEL_NAME} --origin ${MODEL_LOCATION} --runtime-version=$TFVERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To predict with the model, let's take one of the example images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, codecs\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "HEIGHT = 28\n",
    "WIDTH = 28\n",
    "\n",
    "# Get mnist data\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(_, _), (x_test, _) = mnist.load_data()\n",
    "\n",
    "# Scale our features between 0 and 1\n",
    "x_test = x_test / 255.0 \n",
    "\n",
    "IMGNO = 5 # CHANGE THIS to get different images\n",
    "jsondata = {\"image\": x_test[IMGNO].reshape(HEIGHT, WIDTH).tolist()}\n",
    "json.dump(jsondata, codecs.open(\"test.json\", 'w', encoding = \"utf-8\"))\n",
    "plt.imshow(x_test[IMGNO].reshape(HEIGHT, WIDTH));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Send it to the prediction service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gcloud ai-platform predict \\\n",
    "    --model=mnist_dnn_dropout \\\n",
    "    --version=${MODEL_TYPE} \\\n",
    "    --json-instances=./test.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Trained Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf mnistmodel_keras_tf.tar.gz mnist_keras_tf_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
