{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST image classification with Keras Tensorflow backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import json\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = \"ml-practice-260405\" # REPLACE WITH YOUR PROJECT ID\n",
    "BUCKET = \"bucket-ml-practice-260405\" # REPLACE WITH YOUR BUCKET NAME\n",
    "REGION = \"us-central1\" # REPLACE WITH YOUR BUCKET REGION e.g. us-central1\n",
    "MODEL_TYPE = \"dnn_dropout\"  # \"linear\", \"dnn\", \"dnn_dropout\", or \"cnn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not change \n",
    "os.environ[\"ACCOUNT\"] = \"sandcorp2014@gmail.com\"\n",
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"BUCKET\"] = BUCKET\n",
    "os.environ[\"REGION\"] = REGION\n",
    "os.environ[\"MODEL_TYPE\"] = MODEL_TYPE\n",
    "os.environ[\"TFVERSION\"] = \"1.13\"  # Tensorflow version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gcloud config set account $ACCOUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir mnistmodel_keras_tf\n",
    "mkdir mnistmodel_keras_tf/trainer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mnistmodel_keras_tf/trainer/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mnistmodel_keras_tf/trainer/__init__.py\n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mnistmodel_keras_tf/trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mnistmodel_keras_tf/trainer/task.py\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "from .import model\n",
    "import tensorflow as tf\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Create a parser object\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    # Input argument in the parser object\n",
    "    parser.add_argument(\n",
    "        \"--train_batch_size\",\n",
    "        help=\"Batch size for training\",\n",
    "        type = int,\n",
    "        default=100\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        \"--learning_rate\",\n",
    "        help=\"Initial Learning Rate For Training\",\n",
    "        type=float,\n",
    "        default=0.01\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        \"--train_steps\",\n",
    "        help=\"Steps to run the training jobs for\",\n",
    "        type=int,\n",
    "        default=0\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        \"--output_dir\",\n",
    "        help=\"Local Or GCS location to write checkpoint and export model\",\n",
    "        required=True\n",
    "    )\n",
    "    \n",
    "    # Generate List of models to print in help message\n",
    "    model_names = [name.replace(\"_model\",\"\") for name in dir(model) if name.endswith(\"_model\")]\n",
    "    \n",
    "    parser.add_argument(\n",
    "        '--model',\n",
    "        help=\"Type of model. Supported types are {}\".format(model_names),\n",
    "        default='linear'\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        \"--job-dir\",\n",
    "        help = \"this model ignores this field, but it is required by gcloud\",\n",
    "        default = \"junk\"\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        '--dprob',\n",
    "        help ='Drop out probability for cnn',\n",
    "        type = float,\n",
    "        default=0.25\n",
    "    )\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    hparams = args.__dict__\n",
    "\n",
    "    # unused args provided by service\n",
    "    hparams.pop(\"job_dir\", None)\n",
    "    hparams.pop(\"job-dir\", None)\n",
    "\n",
    "    output_dir = hparams.pop(\"output_dir\")\n",
    "    # Append trial_id to path so hptuning jobs don\"t overwrite eachother\n",
    "    output_dir = os.path.join(\n",
    "        output_dir,\n",
    "        json.loads(\n",
    "            os.environ.get(\"TF_CONFIG\", \"{}\")\n",
    "        ).get(\"task\", {}).get(\"trial\", \"\")\n",
    "    )\n",
    "    \n",
    "    # Calculate train_steps if not provided\n",
    "    if hparams[\"train_steps\"] < 1:\n",
    "        # 10,000 steps with batch_size of 512\n",
    "        hparams[\"train_steps\"] = (10000 * 512) // hparams[\"train_batch_size\"]\n",
    "        print(\"Training for {} steps.\".format(hparams[\"train_steps\"]))\n",
    "    \n",
    "    # Run the training job\n",
    "    model.train_and_evaluate(output_dir, hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mnistmodel_keras_tf/trainer/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mnistmodel_keras_tf/trainer/model.py\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)\n",
    "\n",
    "HEIGHT=28\n",
    "WIDTH=28\n",
    "NCLASSES=10\n",
    "\n",
    "# Build Keras model using Keras Sequential API\n",
    "def linear_model(hparams):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.InputLayer(input_shape=[HEIGHT, WIDTH, 1], name=\"image\"))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(\n",
    "        units=NCLASSES,\n",
    "        activation=tf.nn.softmax,\n",
    "        name=\"probabilities\"\n",
    "    ))\n",
    "    return model\n",
    "    \n",
    "def dnn_model(hparams):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.InputLayer(input_shape=[HEIGHT, WIDTH, 1], name=\"image\"))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(units=300, activation=tf.nn.relu))\n",
    "    model.add(tf.keras.layers.Dense(units=100, activation=tf.nn.relu))\n",
    "    model.add(tf.keras.layers.Dense(units=30, activation=tf.nn.relu))\n",
    "    model.add(tf.keras.layers.Dense(units=NCLASSES, activation=tf.nn.softmax, name='Probabilities'))\n",
    "    return model\n",
    "\n",
    "def dnn_dropout_model(hparams):\n",
    "    dprob = hparams.get(\"dprob\", 0.1)\n",
    "    \n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.InputLayer(input_shape=[HEIGHT, WIDTH, 1], name=\"image\"))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(units=300, activation=tf.nn.relu))\n",
    "    model.add(tf.keras.layers.Dense(units=100, activation=tf.nn.relu))\n",
    "    model.add(tf.keras.layers.Dense(units=30, activation=tf.nn.relu))\n",
    "    model.add(tf.keras.layers.Dropout(rate=dprob))\n",
    "    model.add(tf.keras.layers.Dense(\n",
    "        units=NCLASSES, \n",
    "        activation=tf.nn.softmax,\n",
    "        name=\"probabilities\"\n",
    "    ))\n",
    "    return model\n",
    "\n",
    "# Create serving input function for inference\n",
    "def serving_input_fn():\n",
    "    # Input will be rank 3\n",
    "    feature_placeholders = {\n",
    "        \"image\": tf.compat.v1.placeholder(dtype = tf.float32, shape=[None, HEIGHT, WIDTH])\n",
    "    }\n",
    "    # But model function require rank 4\n",
    "    features = {\n",
    "        \"image\": tf.expand_dims(input=feature_placeholders[\"image\"], axis=-1)\n",
    "    }\n",
    "    return tf.estimator.export.ServingInputReceiver(\n",
    "        features=features,\n",
    "        receiver_tensors=feature_placeholders\n",
    "    )\n",
    "\n",
    "    \n",
    "# Create train and evaluate function\n",
    "def train_and_evaluate(ouput_dir, hparams):\n",
    "    # Ensure filewriter cache is clear for TensorBoard event file.\n",
    "    tf.compat.v1.summary.FileWriterCache.clear()\n",
    "    \n",
    "    EVAL_INTERVAL = 60\n",
    "    \n",
    "    # Get mnist Data\n",
    "    mnist = tf.keras.datasets.mnist\n",
    "    \n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    \n",
    "    # Scale our training and testing features between 0 and 1\n",
    "    x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "    \n",
    "    # Reshape image to add a dimension for channels (1 in this case)\n",
    "    x_train = x_train.reshape([-1, HEIGHT, WIDTH, 1])\n",
    "    x_test = x_test.reshape([-1, HEIGHT, WIDTH, 1])\n",
    "    \n",
    "    # Convert labels to categorical one hot encoding\n",
    "    y_train = tf.keras.utils.to_categorical(y=y_train, num_classes=NCLASSES)\n",
    "    y_test = tf.keras.utils.to_categorical(y=y_test, num_classes=NCLASSES)\n",
    "    \n",
    "    # Create training input function\n",
    "    train_input_fn = tf.compat.v1.estimator.inputs.numpy_input_fn(\n",
    "        x={\"image\": x_train},\n",
    "        y=y_train,\n",
    "        batch_size=100,\n",
    "        num_epochs=None,\n",
    "        shuffle=True,\n",
    "        queue_capacity=5000\n",
    "    )\n",
    "    \n",
    "    # Create evaluation input function\n",
    "    eval_input_fn = tf.compat.v1.estimator.inputs.numpy_input_fn(\n",
    "        x={\"image\": x_test},\n",
    "        y=y_test,\n",
    "        batch_size=100,\n",
    "        num_epochs=1,\n",
    "        shuffle=False,\n",
    "        queue_capacity=5000\n",
    "    )\n",
    "    \n",
    "    # Build Keras model\n",
    "    model = dnn_dropout_model(hparams=hparams)\n",
    "    \n",
    "    # Compile Keras model with optimizer, loss function and eval metric\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Convert Keras model to estimator\n",
    "    estimator = tf.keras.estimator.model_to_estimator(\n",
    "        keras_model=model,\n",
    "        model_dir=ouput_dir,\n",
    "        config=tf.estimator.RunConfig(save_checkpoints_secs=EVAL_INTERVAL)\n",
    "    )\n",
    "    \n",
    "    # Set estimator's train_spec to use train_input_fn() and train for so many steps\n",
    "    train_spec = tf.estimator.TrainSpec(\n",
    "        input_fn=train_input_fn,\n",
    "        max_steps=hparams['train_steps']\n",
    "    )\n",
    "    \n",
    "    # Create exporter that use serving_input_fn() to create saved_model for serving\n",
    "    exporter = tf.estimator.LatestExporter(\n",
    "        name='exporter',\n",
    "        serving_input_receiver_fn=serving_input_fn\n",
    "    )\n",
    "    \n",
    "    # Set estimators eval_spec to use eval_input_fn() and export saved_model\n",
    "    eval_spec = tf.estimator.EvalSpec(\n",
    "        input_fn=eval_input_fn,\n",
    "        steps=None,\n",
    "        exporters=exporter,\n",
    "        throttle_secs=EVAL_INTERVAL\n",
    "    )\n",
    "    \n",
    "    # Run train_and_evaluate loop\n",
    "    tf.estimator.train_and_evaluate(\n",
    "        estimator=estimator,\n",
    "        train_spec=train_spec,\n",
    "        eval_spec=eval_spec\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run as a Python module\n",
    "\n",
    "Since we want to run our code on Cloud ML Engine, we've packaged it as a python module.\n",
    "\n",
    "The `model.py` and `task.py` containing the model code is in <a href=\"mnistmodel_keras_tf/trainer\">mnistmodel_keras_tf/trainer</a>\n",
    "\n",
    "**Let's first run it locally for a few steps to test the code.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: The `gcloud ml-engine` commands have been renamed and will soon be removed. Please use `gcloud ai-platform` instead.\n",
      "2020-06-29 12:29:37.123031: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory\n",
      "2020-06-29 12:29:37.123099: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory\n",
      "2020-06-29 12:29:37.123105: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0629 12:29:38.138999 140464631359296 module_wrapper.py:138] From /media/mujahid7292/Data/GoogleDriveSandCorp2014/ML_With_TensorFlow_On_GCP/08.Image Understanding with TensorFlow on GCP/Week_1/Lab_3_Image_Classification_with_a_DNN_Model_with_Dropout/Practice/mnistmodel_keras_tf/trainer/model.py:91: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n",
      "W0629 12:29:38.139168 140464631359296 module_wrapper.py:138] From /media/mujahid7292/Data/GoogleDriveSandCorp2014/ML_With_TensorFlow_On_GCP/08.Image Understanding with TensorFlow on GCP/Week_1/Lab_3_Image_Classification_with_a_DNN_Model_with_Dropout/Practice/mnistmodel_keras_tf/trainer/model.py:91: The name tf.estimator.inputs.numpy_input_fn is deprecated. Please use tf.compat.v1.estimator.inputs.numpy_input_fn instead.\n",
      "\n",
      "2020-06-29 12:29:38.155983: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2020-06-29 12:29:38.156006: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2020-06-29 12:29:38.156020: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (mujahid7292-HP-ENVY-Notebook): /proc/driver/nvidia/version does not exist\n",
      "2020-06-29 12:29:38.156863: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2020-06-29 12:29:38.187889: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2899885000 Hz\n",
      "2020-06-29 12:29:38.188674: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5604fbdfd110 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-06-29 12:29:38.188688: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "I0629 12:29:38.266682 140464631359296 run_config.py:535] TF_CONFIG environment variable: {'environment': 'cloud', 'cluster': {}, 'job': {'args': ['--output_dir=./mnist_keras_tf_trained', '--train_steps=100', '--learning_rate=0.01', '--job-dir', 'JOB_DIR'], 'job_name': 'trainer.task'}, 'task': {}}\n",
      "I0629 12:29:38.267860 140464631359296 keras.py:540] Using the Keras model provided.\n",
      "W0629 12:29:38.273225 140464631359296 deprecation.py:506] From /home/mujahid7292/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "I0629 12:29:38.646685 140464631359296 estimator.py:216] Using config: {'_model_dir': './mnist_keras_tf_trained/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 60, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "I0629 12:29:38.647112 140464631359296 estimator_training.py:186] Not using Distribute Coordinator.\n",
      "I0629 12:29:38.647238 140464631359296 training.py:612] Running training and evaluation locally (non-distributed).\n",
      "I0629 12:29:38.647387 140464631359296 training.py:700] Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 60.\n",
      "W0629 12:29:38.650172 140464631359296 deprecation.py:323] From /home/mujahid7292/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "W0629 12:29:38.656070 140464631359296 deprecation.py:323] From /home/mujahid7292/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_queue_runner.py:62: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "W0629 12:29:38.656688 140464631359296 deprecation.py:323] From /home/mujahid7292/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_functions.py:500: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "I0629 12:29:38.660133 140464631359296 estimator.py:1151] Calling model_fn.\n",
      "I0629 12:29:38.849886 140464631359296 estimator.py:1153] Done calling model_fn.\n",
      "I0629 12:29:38.850028 140464631359296 estimator.py:1372] Warm-starting with WarmStartSettings: WarmStartSettings(ckpt_to_initialize_from='./mnist_keras_tf_trained/keras/keras_model.ckpt', vars_to_warm_start='.*', var_name_to_vocab_info={}, var_name_to_prev_var_name={})\n",
      "I0629 12:29:38.850086 140464631359296 warm_starting_util.py:464] Warm-starting from: ./mnist_keras_tf_trained/keras/keras_model.ckpt\n",
      "I0629 12:29:38.850120 140464631359296 warm_starting_util.py:343] Warm-starting variables only in TRAINABLE_VARIABLES.\n",
      "I0629 12:29:38.866819 140464631359296 warm_starting_util.py:538] Warm-started 8 variables.\n",
      "I0629 12:29:38.867453 140464631359296 basic_session_run_hooks.py:546] Create CheckpointSaverHook.\n",
      "I0629 12:29:38.935898 140464631359296 monitored_session.py:246] Graph was finalized.\n",
      "I0629 12:29:39.011633 140464631359296 session_manager.py:504] Running local_init_op.\n",
      "I0629 12:29:39.020007 140464631359296 session_manager.py:507] Done running local_init_op.\n",
      "W0629 12:29:39.049010 140464631359296 deprecation.py:323] From /home/mujahid7292/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/training/monitored_session.py:906: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "I0629 12:29:39.211392 140464631359296 basic_session_run_hooks.py:613] Saving checkpoints for 0 into ./mnist_keras_tf_trained/model.ckpt.\n",
      "I0629 12:29:39.410727 140464631359296 basic_session_run_hooks.py:262] loss = 2.343531, step = 0\n",
      "I0629 12:29:39.754634 140464631359296 basic_session_run_hooks.py:613] Saving checkpoints for 100 into ./mnist_keras_tf_trained/model.ckpt.\n",
      "I0629 12:29:39.807422 140464631359296 estimator.py:1151] Calling model_fn.\n",
      "I0629 12:29:39.890529 140464631359296 estimator.py:1153] Done calling model_fn.\n",
      "I0629 12:29:39.902692 140464631359296 evaluation.py:255] Starting evaluation at 2020-06-29T12:29:39Z\n",
      "I0629 12:29:39.928022 140464631359296 monitored_session.py:246] Graph was finalized.\n",
      "I0629 12:29:39.929073 140464631359296 saver.py:1284] Restoring parameters from ./mnist_keras_tf_trained/model.ckpt-100\n",
      "I0629 12:29:39.956805 140464631359296 session_manager.py:504] Running local_init_op.\n",
      "I0629 12:29:39.964992 140464631359296 session_manager.py:507] Done running local_init_op.\n",
      "I0629 12:29:40.206301 140464631359296 evaluation.py:273] Inference Time : 0.30350s\n",
      "I0629 12:29:40.206758 140464631359296 evaluation.py:276] Finished evaluation at 2020-06-29-12:29:40\n",
      "I0629 12:29:40.206846 140464631359296 estimator.py:2053] Saving dict for global step 100: accuracy = 0.9043, global_step = 100, loss = 0.31798634\n",
      "I0629 12:29:40.283159 140464631359296 estimator.py:2113] Saving 'checkpoint_path' summary for global step 100: ./mnist_keras_tf_trained/model.ckpt-100\n",
      "I0629 12:29:40.288296 140464631359296 estimator.py:1151] Calling model_fn.\n",
      "I0629 12:29:40.344677 140464631359296 estimator.py:1153] Done calling model_fn.\n",
      "W0629 12:29:40.344830 140464631359296 deprecation.py:323] From /home/mujahid7292/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "I0629 12:29:40.344974 140464631359296 export_utils.py:170] Signatures INCLUDED in export for Classify: None\n",
      "I0629 12:29:40.345018 140464631359296 export_utils.py:170] Signatures INCLUDED in export for Regress: None\n",
      "I0629 12:29:40.345054 140464631359296 export_utils.py:170] Signatures INCLUDED in export for Predict: ['serving_default']\n",
      "I0629 12:29:40.345085 140464631359296 export_utils.py:170] Signatures INCLUDED in export for Train: None\n",
      "I0629 12:29:40.345112 140464631359296 export_utils.py:170] Signatures INCLUDED in export for Eval: None\n",
      "I0629 12:29:40.346864 140464631359296 saver.py:1284] Restoring parameters from ./mnist_keras_tf_trained/model.ckpt-100\n",
      "I0629 12:29:40.359120 140464631359296 builder_impl.py:666] Assets added to graph.\n",
      "I0629 12:29:40.359213 140464631359296 builder_impl.py:461] No assets to write.\n",
      "I0629 12:29:40.393123 140464631359296 builder_impl.py:426] SavedModel written to: ./mnist_keras_tf_trained/export/exporter/temp-b'1593412180'/saved_model.pb\n",
      "I0629 12:29:40.432491 140464631359296 estimator.py:375] Loss for final step: 0.4481694.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "rm -rf mnistmodel_keras_tf.tar.gz mnist_keras_tf_trained\n",
    "JOB_DIR=./tmp\n",
    "gcloud ml-engine local train \\\n",
    "    --module-name=trainer.task \\\n",
    "    --package-path=./mnistmodel_keras_tf/trainer \\\n",
    "    --job-dir=JOB_DIR \\\n",
    "    -- \\\n",
    "    --output_dir=./mnist_keras_tf_trained \\\n",
    "    --train_steps=100 \\\n",
    "    --learning_rate=0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><p style='color:red'>Loss for final step: 0.4481694</p></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run as python module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-29 12:30:08.304680: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory\n",
      "2020-06-29 12:30:08.304762: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory\n",
      "2020-06-29 12:30:08.304774: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0629 12:30:09.333090 139747199412032 module_wrapper.py:138] From /media/mujahid7292/Data/GoogleDriveSandCorp2014/ML_With_TensorFlow_On_GCP/08.Image Understanding with TensorFlow on GCP/Week_1/Lab_3_Image_Classification_with_a_DNN_Model_with_Dropout/Practice/mnistmodel_keras_tf/trainer/model.py:91: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n",
      "W0629 12:30:09.333287 139747199412032 module_wrapper.py:138] From /media/mujahid7292/Data/GoogleDriveSandCorp2014/ML_With_TensorFlow_On_GCP/08.Image Understanding with TensorFlow on GCP/Week_1/Lab_3_Image_Classification_with_a_DNN_Model_with_Dropout/Practice/mnistmodel_keras_tf/trainer/model.py:91: The name tf.estimator.inputs.numpy_input_fn is deprecated. Please use tf.compat.v1.estimator.inputs.numpy_input_fn instead.\n",
      "\n",
      "2020-06-29 12:30:09.342374: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2020-06-29 12:30:09.342397: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2020-06-29 12:30:09.342412: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (mujahid7292-HP-ENVY-Notebook): /proc/driver/nvidia/version does not exist\n",
      "2020-06-29 12:30:09.342554: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2020-06-29 12:30:09.363896: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2899885000 Hz\n",
      "2020-06-29 12:30:09.364117: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5653f0d8c8e0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-06-29 12:30:09.364133: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "I0629 12:30:09.421783 139747199412032 keras.py:540] Using the Keras model provided.\n",
      "W0629 12:30:09.426141 139747199412032 deprecation.py:506] From /home/mujahid7292/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "I0629 12:30:09.749313 139747199412032 estimator.py:216] Using config: {'_model_dir': './mnist_keras_tf_trained/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 60, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "I0629 12:30:09.749732 139747199412032 estimator_training.py:186] Not using Distribute Coordinator.\n",
      "I0629 12:30:09.749846 139747199412032 training.py:612] Running training and evaluation locally (non-distributed).\n",
      "I0629 12:30:09.749995 139747199412032 training.py:700] Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 60.\n",
      "W0629 12:30:09.752801 139747199412032 deprecation.py:323] From /home/mujahid7292/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "W0629 12:30:09.758654 139747199412032 deprecation.py:323] From /home/mujahid7292/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_queue_runner.py:62: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "W0629 12:30:09.759279 139747199412032 deprecation.py:323] From /home/mujahid7292/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_functions.py:500: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "I0629 12:30:09.762753 139747199412032 estimator.py:1151] Calling model_fn.\n",
      "I0629 12:30:09.949399 139747199412032 estimator.py:1153] Done calling model_fn.\n",
      "I0629 12:30:09.949539 139747199412032 estimator.py:1372] Warm-starting with WarmStartSettings: WarmStartSettings(ckpt_to_initialize_from='./mnist_keras_tf_trained/keras/keras_model.ckpt', vars_to_warm_start='.*', var_name_to_vocab_info={}, var_name_to_prev_var_name={})\n",
      "I0629 12:30:09.949595 139747199412032 warm_starting_util.py:464] Warm-starting from: ./mnist_keras_tf_trained/keras/keras_model.ckpt\n",
      "I0629 12:30:09.949631 139747199412032 warm_starting_util.py:343] Warm-starting variables only in TRAINABLE_VARIABLES.\n",
      "I0629 12:30:09.964393 139747199412032 warm_starting_util.py:538] Warm-started 8 variables.\n",
      "I0629 12:30:09.965044 139747199412032 basic_session_run_hooks.py:546] Create CheckpointSaverHook.\n",
      "I0629 12:30:10.030163 139747199412032 monitored_session.py:246] Graph was finalized.\n",
      "I0629 12:30:10.113760 139747199412032 session_manager.py:504] Running local_init_op.\n",
      "I0629 12:30:10.123106 139747199412032 session_manager.py:507] Done running local_init_op.\n",
      "W0629 12:30:10.154840 139747199412032 deprecation.py:323] From /home/mujahid7292/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/training/monitored_session.py:906: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "I0629 12:30:10.346888 139747199412032 basic_session_run_hooks.py:613] Saving checkpoints for 0 into ./mnist_keras_tf_trained/model.ckpt.\n",
      "I0629 12:30:10.556627 139747199412032 basic_session_run_hooks.py:262] loss = 2.3032224, step = 0\n",
      "I0629 12:30:10.904966 139747199412032 basic_session_run_hooks.py:613] Saving checkpoints for 100 into ./mnist_keras_tf_trained/model.ckpt.\n",
      "I0629 12:30:10.956557 139747199412032 estimator.py:1151] Calling model_fn.\n",
      "I0629 12:30:11.042365 139747199412032 estimator.py:1153] Done calling model_fn.\n",
      "I0629 12:30:11.055598 139747199412032 evaluation.py:255] Starting evaluation at 2020-06-29T12:30:11Z\n",
      "I0629 12:30:11.081452 139747199412032 monitored_session.py:246] Graph was finalized.\n",
      "I0629 12:30:11.082529 139747199412032 saver.py:1284] Restoring parameters from ./mnist_keras_tf_trained/model.ckpt-100\n",
      "I0629 12:30:11.110556 139747199412032 session_manager.py:504] Running local_init_op.\n",
      "I0629 12:30:11.118911 139747199412032 session_manager.py:507] Done running local_init_op.\n",
      "I0629 12:30:11.351166 139747199412032 evaluation.py:273] Inference Time : 0.29545s\n",
      "I0629 12:30:11.351451 139747199412032 evaluation.py:276] Finished evaluation at 2020-06-29-12:30:11\n",
      "I0629 12:30:11.351533 139747199412032 estimator.py:2053] Saving dict for global step 100: accuracy = 0.9136, global_step = 100, loss = 0.30879351\n",
      "I0629 12:30:11.427993 139747199412032 estimator.py:2113] Saving 'checkpoint_path' summary for global step 100: ./mnist_keras_tf_trained/model.ckpt-100\n",
      "I0629 12:30:11.433327 139747199412032 estimator.py:1151] Calling model_fn.\n",
      "I0629 12:30:11.489836 139747199412032 estimator.py:1153] Done calling model_fn.\n",
      "W0629 12:30:11.489988 139747199412032 deprecation.py:323] From /home/mujahid7292/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "I0629 12:30:11.490134 139747199412032 export_utils.py:170] Signatures INCLUDED in export for Classify: None\n",
      "I0629 12:30:11.490179 139747199412032 export_utils.py:170] Signatures INCLUDED in export for Regress: None\n",
      "I0629 12:30:11.490215 139747199412032 export_utils.py:170] Signatures INCLUDED in export for Predict: ['serving_default']\n",
      "I0629 12:30:11.490245 139747199412032 export_utils.py:170] Signatures INCLUDED in export for Train: None\n",
      "I0629 12:30:11.490273 139747199412032 export_utils.py:170] Signatures INCLUDED in export for Eval: None\n",
      "I0629 12:30:11.492048 139747199412032 saver.py:1284] Restoring parameters from ./mnist_keras_tf_trained/model.ckpt-100\n",
      "I0629 12:30:11.504672 139747199412032 builder_impl.py:666] Assets added to graph.\n",
      "I0629 12:30:11.504769 139747199412032 builder_impl.py:461] No assets to write.\n",
      "I0629 12:30:11.538360 139747199412032 builder_impl.py:426] SavedModel written to: ./mnist_keras_tf_trained/export/exporter/temp-b'1593412211'/saved_model.pb\n",
      "I0629 12:30:11.576978 139747199412032 estimator.py:375] Loss for final step: 0.42073974.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "rm -rf mnistmodel_keras_tf.tar.gz mnist_keras_tf_trained\n",
    "python3 -m mnistmodel_keras_tf.trainer.task \\\n",
    "    --learning_rate=0.01 \\\n",
    "    --train_steps=100 \\\n",
    "    --output_dir=./mnist_keras_tf_trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><p style='color:red'>Loss for final step: 0.42073974</p></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, let's do it on Cloud ML Engine so we can train on GPU (`--scale-tier=BASIC_GPU`)**\n",
    "\n",
    "Note the GPU speed up depends on the model type. You'll notice the more complex CNN model trains significantly faster on GPU, however the speed up on the simpler models is not as pronounced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "OUTDIR=gs://${BUCKET}/mnist/trained_dnn\n",
    "JOBNAME=mnist_dnn_$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "    --region=$REGION \\\n",
    "    --module-name=trainer.task \\\n",
    "    --package-path=./mnistmodel_keras_tf/trainer \\\n",
    "    --job-dir=$OUTDIR \\\n",
    "    --staging-bucket=gs://$BUCKET \\\n",
    "    --scale-tier=BASIC_GPU \\\n",
    "    --runtime-version=$TFVERSION \\\n",
    "    -- \\\n",
    "    --output_dir=$OUTDIR \\\n",
    "    --train_steps=100 \\\n",
    "    --learning_rate=0.01 \\\n",
    "    --train_batch_size=512 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><p style='color:red'>Loss for final step: 0.19776809</p></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying and predicting with model\n",
    "\n",
    "Deploy the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "MODEL_NAME=\"mnist\"\n",
    "MODEL_VERSION=${MODEL_TYPE}\n",
    "MODEL_LOCATION=$(gsutil ls gs://${BUCKET}/mnist/trained_${MODEL_TYPE}/export/exporter | tail -1)\n",
    "echo \"Deleting and deploying $MODEL_NAME $MODEL_VERSION from $MODEL_LOCATION ... this will take a few minutes\"\n",
    "#gcloud ml-engine versions delete ${MODEL_VERSION} --model ${MODEL_NAME}\n",
    "#gcloud ml-engine models delete ${MODEL_NAME}\n",
    "gcloud ml-engine models create ${MODEL_NAME} --regions $REGION\n",
    "gcloud ml-engine versions create ${MODEL_VERSION} --model ${MODEL_NAME} --origin ${MODEL_LOCATION} --runtime-version=$TFVERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To predict with the model, let's take one of the example images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, codecs\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "HEIGHT = 28\n",
    "WIDTH = 28\n",
    "\n",
    "# Get mnist data\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(_, _), (x_test, _) = mnist.load_data()\n",
    "\n",
    "# Scale our features between 0 and 1\n",
    "x_test = x_test / 255.0 \n",
    "\n",
    "IMGNO = 5 # CHANGE THIS to get different images\n",
    "jsondata = {\"image\": x_test[IMGNO].reshape(HEIGHT, WIDTH).tolist()}\n",
    "json.dump(jsondata, codecs.open(\"test.json\", 'w', encoding = \"utf-8\"))\n",
    "plt.imshow(x_test[IMGNO].reshape(HEIGHT, WIDTH));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Send it to the prediction service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gcloud ai-platform predict \\\n",
    "    --model=mnist \\\n",
    "    --version=${MODEL_TYPE} \\\n",
    "    --json-instances=./test.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Trained Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf mnistmodel_keras_tf.tar.gz mnist_keras_tf_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
