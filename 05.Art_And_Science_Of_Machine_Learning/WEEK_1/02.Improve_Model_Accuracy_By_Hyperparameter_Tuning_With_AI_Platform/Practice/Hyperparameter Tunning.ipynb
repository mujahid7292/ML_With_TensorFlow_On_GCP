{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enable Virtual Environment For This Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activate Conda Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>`$ conda activate`</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Or Upgrade necessary software for virtual environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>`$ sudo apt-get install --upgrade python3-pip`</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>`$ sudo pip3 install --upgrade virtualenv`</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>`$ sudo pip3 install --upgrade setuptools`</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will go to the location of the directory, where we will create our virtual environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>`$ cd /media/mujahid7292/Data/GoogleDriveSandCorp2014/ML_With_TensorFlow_On_GCP/05.Art_And_Science_Of_Machine_Learning/WEEK_1/02.Improve_Model_Accuracy_By_Hyperparameter_Tuning_With_AI_Platform/Practice`</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deactivate conda environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>`$ conda deactivate`</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Virtual Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>`$ virtualenv Venv`</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activate newly created virtual environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>`$ source Venv/bin/activate`</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>`$ (Venv) which python`</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>`$ (Venv) pip list`</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>`$ (Venv) pip3 install jupyter`</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "numpy\n",
    "pandas\n",
    "tensorflow==1.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Using cached numpy-1.18.2-cp36-cp36m-manylinux1_x86_64.whl (20.2 MB)\n",
      "Collecting pandas\n",
      "  Using cached pandas-1.0.3-cp36-cp36m-manylinux1_x86_64.whl (10.0 MB)\n",
      "Collecting tensorflow==1.8.0\n",
      "  Using cached tensorflow-1.8.0-cp36-cp36m-manylinux1_x86_64.whl (49.1 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in ./Venv/lib/python3.6/site-packages (from pandas->-r requirements.txt (line 2)) (2.8.1)\n",
      "Collecting pytz>=2017.2\n",
      "  Using cached pytz-2019.3-py2.py3-none-any.whl (509 kB)\n",
      "Collecting astor>=0.6.0\n",
      "  Using cached astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
      "Processing /home/mujahid7292/.cache/pip/wheels/c3/af/84/3962a6af7b4ab336e951b7877dcfb758cf94548bb1771e0679/absl_py-0.9.0-py3-none-any.whl\n",
      "Processing /home/mujahid7292/.cache/pip/wheels/93/2a/eb/e58dbcbc963549ee4f065ff80a59f274cc7210b6eab962acdc/termcolor-1.1.0-py3-none-any.whl\n",
      "Collecting grpcio>=1.8.6\n",
      "  Using cached grpcio-1.27.2-cp36-cp36m-manylinux2010_x86_64.whl (2.7 MB)\n",
      "Collecting tensorboard<1.9.0,>=1.8.0\n",
      "  Using cached tensorboard-1.8.0-py3-none-any.whl (3.1 MB)\n",
      "Requirement already satisfied: wheel>=0.26 in ./Venv/lib/python3.6/site-packages (from tensorflow==1.8.0->-r requirements.txt (line 3)) (0.34.2)\n",
      "Collecting gast>=0.2.0\n",
      "  Using cached gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting protobuf>=3.4.0\n",
      "  Using cached protobuf-3.11.3-cp36-cp36m-manylinux1_x86_64.whl (1.3 MB)\n",
      "Requirement already satisfied: six>=1.10.0 in ./Venv/lib/python3.6/site-packages (from tensorflow==1.8.0->-r requirements.txt (line 3)) (1.14.0)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.2.1-py2.py3-none-any.whl (88 kB)\n",
      "Processing /home/mujahid7292/.cache/pip/wheels/90/1c/cb/a87fd097ff74648ecc468a703001f6c7c86d8a71d459e65c98/html5lib-0.9999999-py3-none-any.whl\n",
      "Collecting werkzeug>=0.11.10\n",
      "  Using cached Werkzeug-1.0.0-py2.py3-none-any.whl (298 kB)\n",
      "Collecting bleach==1.5.0\n",
      "  Using cached bleach-1.5.0-py2.py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: setuptools in ./Venv/lib/python3.6/site-packages (from protobuf>=3.4.0->tensorflow==1.8.0->-r requirements.txt (line 3)) (46.0.0)\n",
      "Installing collected packages: numpy, pytz, pandas, astor, absl-py, termcolor, grpcio, markdown, html5lib, werkzeug, protobuf, bleach, tensorboard, gast, tensorflow\n",
      "  Attempting uninstall: bleach\n",
      "    Found existing installation: bleach 3.1.3\n",
      "    Uninstalling bleach-3.1.3:\n",
      "      Successfully uninstalled bleach-3.1.3\n",
      "Successfully installed absl-py-0.9.0 astor-0.8.1 bleach-1.5.0 gast-0.3.3 grpcio-1.27.2 html5lib-0.9999999 markdown-3.2.1 numpy-1.18.2 pandas-1.0.3 protobuf-3.11.3 pytz-2019.3 tensorboard-1.8.0 tensorflow-1.8.0 termcolor-1.1.0 werkzeug-1.0.0\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package            Version  \n",
      "------------------ ---------\n",
      "absl-py            0.9.0    \n",
      "astor              0.8.1    \n",
      "attrs              19.3.0   \n",
      "backcall           0.1.0    \n",
      "bleach             1.5.0    \n",
      "decorator          4.4.2    \n",
      "defusedxml         0.6.0    \n",
      "entrypoints        0.3      \n",
      "gast               0.3.3    \n",
      "grpcio             1.27.2   \n",
      "html5lib           0.9999999\n",
      "importlib-metadata 1.5.0    \n",
      "ipykernel          5.2.0    \n",
      "ipython            7.13.0   \n",
      "ipython-genutils   0.2.0    \n",
      "ipywidgets         7.5.1    \n",
      "jedi               0.16.0   \n",
      "Jinja2             2.11.1   \n",
      "jsonschema         3.2.0    \n",
      "jupyter            1.0.0    \n",
      "jupyter-client     6.1.0    \n",
      "jupyter-console    6.1.0    \n",
      "jupyter-core       4.6.3    \n",
      "Markdown           3.2.1    \n",
      "MarkupSafe         1.1.1    \n",
      "mistune            0.8.4    \n",
      "nbconvert          5.6.1    \n",
      "nbformat           5.0.4    \n",
      "notebook           6.0.3    \n",
      "numpy              1.18.2   \n",
      "pandas             1.0.3    \n",
      "pandocfilters      1.4.2    \n",
      "parso              0.6.2    \n",
      "pexpect            4.8.0    \n",
      "pickleshare        0.7.5    \n",
      "pip                20.0.2   \n",
      "prometheus-client  0.7.1    \n",
      "prompt-toolkit     3.0.4    \n",
      "protobuf           3.11.3   \n",
      "ptyprocess         0.6.0    \n",
      "Pygments           2.6.1    \n",
      "pyrsistent         0.15.7   \n",
      "python-dateutil    2.8.1    \n",
      "pytz               2019.3   \n",
      "pyzmq              19.0.0   \n",
      "qtconsole          4.7.1    \n",
      "QtPy               1.9.0    \n",
      "Send2Trash         1.5.0    \n",
      "setuptools         46.0.0   \n",
      "six                1.14.0   \n",
      "tensorboard        1.8.0    \n",
      "tensorflow         1.8.0    \n",
      "termcolor          1.1.0    \n",
      "terminado          0.8.3    \n",
      "testpath           0.4.4    \n",
      "tornado            6.0.4    \n",
      "traitlets          4.3.3    \n",
      "wcwidth            0.1.9    \n",
      "webencodings       0.5.1    \n",
      "Werkzeug           1.0.0    \n",
      "wheel              0.34.2   \n",
      "widgetsnbextension 3.5.1    \n",
      "zipp               3.1.0    \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip3 list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/mujahid7292/Data/GoogleDriveSandCorp2014/ML_With_TensorFlow_On_GCP/05.Art_And_Science_Of_Machine_Learning/WEEK_1/02.Improve_Model_Accuracy_By_Hyperparameter_Tuning_With_AI_Platform/Practice/Venv/bin/python\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.6.9\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a>https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/courses/machine_learning/deepdive/05_artandscience/b_hyperparam.ipynb</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning with Cloud AI Platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning Objectives:**\n",
    "  *  Improve the accuracy of the model by hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "PROJECT = 'ml-practice-260405'\n",
    "BUCKET = 'buck-ml-practice-260405'\n",
    "REGION = 'us-central1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For bash\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['REGION'] = REGION\n",
    "os.environ['TFVERSION'] = '1.8' # Tensorflow version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "Updated property [compute/region].\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create command line programme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to submit to `Cloud AI Platform`, we need to create a distributed training programme. Let's convert our housing example to fit that paradigm, using the Estimator API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf house_prediction_module\n",
    "mkdir house_prediction_module\n",
    "mkdir house_prediction_module/trainer\n",
    "touch house_prediction_module/trainer/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting house_prediction_module/trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile house_prediction_module/trainer/task.py\n",
    "import argparse\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "from .import model\n",
    "\n",
    "if __name__ == '__main__' and \"get_ipython\" not in dir():\n",
    "    # Create a parser object\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    # Add training argument to the parser object\n",
    "    parser.add_argument(\n",
    "        '--learning_rate',\n",
    "        type=float,\n",
    "        default=0.01\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--batch_size',\n",
    "        type=int,\n",
    "        default=30\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--output_dir',\n",
    "        help=\"GCS location to write checkpoints and export models\",\n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--job-dir',\n",
    "        help=\"This model ignore this field, but it is required by gcloud\",\n",
    "        default='junk'\n",
    "    )\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    arguments = args.__dict__\n",
    "    \n",
    "    # Unused args provided by service\n",
    "    arguments.pop('job_dir',None)\n",
    "    arguments.pop('job-dir', None)\n",
    "    \n",
    "    # Append trail_id to path if we are doing hyperparameter tunning\n",
    "    # This code can be removed if you are not doing any hyperparameter tuning\n",
    "    arguments['output_dir'] = os.path.join(\n",
    "        arguments['output_dir'],\n",
    "        json.loads(\n",
    "            os.environ.get('TF_CONFIG', '{}')\n",
    "        ).get('task', {}).get('trail', '')\n",
    "    )\n",
    "    \n",
    "    # Now run the training\n",
    "    shutil.rmtree(arguments['output_dir'], ignore_errors=True) # Start fresh each time\n",
    "    \n",
    "    # Pass the command line arguments to our model's train_and_evaluate function\n",
    "    model.train_and_evaluate(arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/mujahid7292/Data/GoogleDriveSandCorp2014/ML_With_TensorFlow_On_GCP/05.Art_And_Science_Of_Machine_Learning/WEEK_1/02.Improve_Model_Accuracy_By_Hyperparameter_Tuning_With_AI_Platform/Practice/Venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/media/mujahid7292/Data/GoogleDriveSandCorp2014/ML_With_TensorFlow_On_GCP/05.Art_And_Science_Of_Machine_Learning/WEEK_1/02.Improve_Model_Accuracy_By_Hyperparameter_Tuning_With_AI_Platform/Practice/Venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/media/mujahid7292/Data/GoogleDriveSandCorp2014/ML_With_TensorFlow_On_GCP/05.Art_And_Science_Of_Machine_Learning/WEEK_1/02.Improve_Model_Accuracy_By_Hyperparameter_Tuning_With_AI_Platform/Practice/Venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/media/mujahid7292/Data/GoogleDriveSandCorp2014/ML_With_TensorFlow_On_GCP/05.Art_And_Science_Of_Machine_Learning/WEEK_1/02.Improve_Model_Accuracy_By_Hyperparameter_Tuning_With_AI_Platform/Practice/Venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:522: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/media/mujahid7292/Data/GoogleDriveSandCorp2014/ML_With_TensorFlow_On_GCP/05.Art_And_Science_Of_Machine_Learning/WEEK_1/02.Improve_Model_Accuracy_By_Hyperparameter_Tuning_With_AI_Platform/Practice/Venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/media/mujahid7292/Data/GoogleDriveSandCorp2014/ML_With_TensorFlow_On_GCP/05.Art_And_Science_Of_Machine_Learning/WEEK_1/02.Improve_Model_Accuracy_By_Hyperparameter_Tuning_With_AI_Platform/Practice/Venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing house_prediction_module/trainer/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile house_prediction_module/trainer/model.py\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "# Read the dataset from GCS\n",
    "df = pd.read_csv(\n",
    "    \"https://storage.googleapis.com/ml_universities/california_housing_train.csv\",\n",
    "    sep=','\n",
    ")\n",
    "\n",
    "# Create new feature\n",
    "df['num_rooms'] = df['total_rooms'] / df['households']\n",
    "\n",
    "# Now split the whole data into training and evaluation\n",
    "np.random.seed(seed=1) # Makes dataset split reproducible\n",
    "msk = np.random.randn(len(df)) < 0.8 # As we want to split 80 To 20\n",
    "\n",
    "# Now create training dataframe by keeping 80% of the total data\n",
    "df_train = df[msk]\n",
    "# Now create evaluation dataframe by keeping rest 20% of the total data\n",
    "df_eval = df[~msk]\n",
    "\n",
    "# Constant for our training\n",
    "SCALE = 100000 # 1 Lac\n",
    "\n",
    "# Now create our training input function\n",
    "def train_input_fn(df_train, batch_size):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    return tf.estimator.inputs.pandas_input_fn(\n",
    "        x=df_train[['num_rooms']],\n",
    "        y=df_train['median_house_value'] / SCALE, # Note the scalling\n",
    "        num_epochs=None,\n",
    "        batch_size=batch_size, # Note the batch size\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "# Now create our evaluation input function\n",
    "def eval_input_fn(df_eval, batch_size):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    return tf.estimator.inputs.pandas_input_fn(\n",
    "        x=df_eval[['num_rooms']],\n",
    "        y=df_eval['median_house_value'] / SCALE, # Note the scalling\n",
    "        num_epochs=1,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "# Define feature's column\n",
    "features = [tf.feature_column.numeric_column('num_rooms')]\n",
    "\n",
    "# Now create our train_and_evaluate() function\n",
    "def train_and_evaluate(args):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Compute appropriate number of steps\n",
    "    num_steps = (len(df_train) / args['batch_size']) / args['learning_rate'] # if learning rate = 0.01, 100 epochs\n",
    "    \n",
    "    # Create custom optimizer\n",
    "    myopt = tf.train.FtrlOptimizer(learning_rate=args['learning_rate']) # Note the learning rate\n",
    "    \n",
    "    # Create Linear Regressor Estimator Object\n",
    "    estimator = tf.estimator.LinearRegressor(\n",
    "        feature_columns=features,\n",
    "        model_dir=args['output_dir'],\n",
    "        optimizer=myopt\n",
    "    )\n",
    "    \n",
    "    # Add RMSE evaluation metric\n",
    "    def rmse(labels, predictions):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        pred_values = tf.cast(predictions['predictions'], tf.float64)\n",
    "        return {'rmse' : tf.metrics.root_mean_squared_error(labels*SCALE, pred_values*SCALE)}\n",
    "    \n",
    "    # Attach custom evaluation metric to the estimator object\n",
    "    estimator = tf.contrib.estimator.add_metrics(estimator, rmse)\n",
    "    \n",
    "    # Now create our training specefication\n",
    "    train_spec = tf.estimator.TrainSpec(\n",
    "        input_fn=train_input_fn(df_train,args['batch_size']),\n",
    "        max_steps=num_steps\n",
    "    )\n",
    "    \n",
    "    # Now create our evaluation specefication\n",
    "    eval_spec = tf.estimator.EvalSpec(\n",
    "        input_fn=eval_input_fn(df_eval, len(df_eval)),\n",
    "        steps=None\n",
    "    )\n",
    "    \n",
    "    # Now finish our function\n",
    "    tf.estimator.train_and_evaluate(estimator,train_spec,eval_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Unexpected tensorflow version 2020-03-23 15:55:08.970408: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory\n",
      "2020-03-23 15:55:08.970469: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory\n",
      "2020-03-23 15:55:08.970475: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2.1.0\n",
      ", using the default primary node name, aka \"chief\" for cluster settings\n",
      "<subprocess.Popen object at 0x7ff300bf3c50>\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'rm -rf house_trained\\nexport PYTHONPATH=${PYTHONPATH}:${PWD}/house_prediction_module\\ngcloud ai-platform local train \\\\\\n    --module-name=trainer.task \\\\\\n    --job-dir=house_trained \\\\\\n    --package-path=$(pwd)/trainer \\\\\\n    -- \\\\\\n    --batch_size=30 \\\\\\n    --learning_rate=0.02 \\\\\\n    --output_dir=house_trained\\n'' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-78b4036e523a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bash'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rm -rf house_trained\\nexport PYTHONPATH=${PYTHONPATH}:${PWD}/house_prediction_module\\ngcloud ai-platform local train \\\\\\n    --module-name=trainer.task \\\\\\n    --job-dir=house_trained \\\\\\n    --package-path=$(pwd)/trainer \\\\\\n    -- \\\\\\n    --batch_size=30 \\\\\\n    --learning_rate=0.02 \\\\\\n    --output_dir=house_trained\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/media/mujahid7292/Data/GoogleDriveSandCorp2014/ML_With_TensorFlow_On_GCP/05.Art_And_Science_Of_Machine_Learning/WEEK_1/02.Improve_Model_Accuracy_By_Hyperparameter_Tuning_With_AI_Platform/Practice/Venv/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2360\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2361\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2362\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2363\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/mujahid7292/Data/GoogleDriveSandCorp2014/ML_With_TensorFlow_On_GCP/05.Art_And_Science_Of_Machine_Learning/WEEK_1/02.Improve_Model_Accuracy_By_Hyperparameter_Tuning_With_AI_Platform/Practice/Venv/lib/python3.6/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mnamed_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshebang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# write a basic docstring:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-110>\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n",
      "\u001b[0;32m/media/mujahid7292/Data/GoogleDriveSandCorp2014/ML_With_TensorFlow_On_GCP/05.Art_And_Science_Of_Machine_Learning/WEEK_1/02.Improve_Model_Accuracy_By_Hyperparameter_Tuning_With_AI_Platform/Practice/Venv/lib/python3.6/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/mujahid7292/Data/GoogleDriveSandCorp2014/ML_With_TensorFlow_On_GCP/05.Art_And_Science_Of_Machine_Learning/WEEK_1/02.Improve_Model_Accuracy_By_Hyperparameter_Tuning_With_AI_Platform/Practice/Venv/lib/python3.6/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_error\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_close\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'rm -rf house_trained\\nexport PYTHONPATH=${PYTHONPATH}:${PWD}/house_prediction_module\\ngcloud ai-platform local train \\\\\\n    --module-name=trainer.task \\\\\\n    --job-dir=house_trained \\\\\\n    --package-path=$(pwd)/trainer \\\\\\n    -- \\\\\\n    --batch_size=30 \\\\\\n    --learning_rate=0.02 \\\\\\n    --output_dir=house_trained\\n'' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# First delete the `house_trained` directory\n",
    "rm -rf house_trained\n",
    "# Export the python path\n",
    "export PYTHONPATH=${PYTHONPATH}:${PWD}/house_prediction_module\n",
    "# Now run the gcloud local training\n",
    "gcloud ai-platform local train \\\n",
    "    --module-name=trainer.task \\\n",
    "    --job-dir=house_trained \\\n",
    "    --package-path=$(pwd)/trainer \\\n",
    "    -- \\\n",
    "    --batch_size=30 \\\n",
    "    --learning_rate=0.02 \\\n",
    "    --output_dir=house_trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create hyperparam.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hyperparam.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile hyperparam.yaml\n",
    "trainingInput:\n",
    "  hyperparameters:\n",
    "    goal: MINIMIZE\n",
    "    maxTrials: 5\n",
    "    maxParallelTrials: 1\n",
    "    hyperparameterMetricTag: rmse\n",
    "    params:\n",
    "    - parameterName: batch_size\n",
    "      type: INTEGER\n",
    "      minValue: 8\n",
    "      maxValue: 64\n",
    "      scaleType: UNIT_LINEAR_SCALE\n",
    "    - parameterName: learning_rate\n",
    "      type: DOUBLE\n",
    "      minValue: 0.01\n",
    "      maxValue: 0.1\n",
    "      scaleType: UNIT_LOG_SCALE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create GCS bucket if it does not exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating gs://buck-ml-practice-260405/...\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "if ! gsutil ls | grep -q gs://${BUCKET}/; then\n",
    "    gsutil mb -l ${REGION} gs://${BUCKET}\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CommandException: 1 files/objects could not be removed.\n",
      "ERROR: (gcloud.ai-platform.jobs.submit.training) INVALID_ARGUMENT: Field: runtime_version Error: The specified runtime version '1.8' with the Python version '' is not supported or is deprecated.  Please specify a different runtime version. See https://cloud.google.com/ml-engine/docs/runtime-version-list for a list of supported versions\n",
      "- '@type': type.googleapis.com/google.rpc.BadRequest\n",
      "  fieldViolations:\n",
      "  - description: The specified runtime version '1.8' with the Python version '' is\n",
      "      not supported or is deprecated.  Please specify a different runtime version.\n",
      "      See https://cloud.google.com/ml-engine/docs/runtime-version-list for a list\n",
      "      of supported versions\n",
      "    field: runtime_version\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'OUTDIR=gs://${BUCKET}/house_trained   # CHANGE bucket name appropriately\\ngsutil rm -rf $OUTDIR\\nexport PYTHONPATH=${PYTHONPATH}:${PWD}/house_prediction_module\\ngcloud ai-platform jobs submit training house_$(date -u +%y%m%d_%H%M%S) \\\\\\n   --config=hyperparam.yaml \\\\\\n   --module-name=trainer.task \\\\\\n   --package-path=$(pwd)/house_prediction_module/trainer \\\\\\n   --job-dir=$OUTDIR \\\\\\n   --runtime-version=$TFVERSION \\\\\\n   --\\\\\\n   --output_dir=$OUTDIR \\\\\\n'' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-947b8cff2d17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bash'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'OUTDIR=gs://${BUCKET}/house_trained   # CHANGE bucket name appropriately\\ngsutil rm -rf $OUTDIR\\nexport PYTHONPATH=${PYTHONPATH}:${PWD}/house_prediction_module\\ngcloud ai-platform jobs submit training house_$(date -u +%y%m%d_%H%M%S) \\\\\\n   --config=hyperparam.yaml \\\\\\n   --module-name=trainer.task \\\\\\n   --package-path=$(pwd)/house_prediction_module/trainer \\\\\\n   --job-dir=$OUTDIR \\\\\\n   --runtime-version=$TFVERSION \\\\\\n   --\\\\\\n   --output_dir=$OUTDIR \\\\\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/media/mujahid7292/Data/GoogleDriveSandCorp2014/ML_With_TensorFlow_On_GCP/05.Art_And_Science_Of_Machine_Learning/WEEK_1/02.Improve_Model_Accuracy_By_Hyperparameter_Tuning_With_AI_Platform/Practice/Venv/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2360\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2361\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2362\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2363\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/mujahid7292/Data/GoogleDriveSandCorp2014/ML_With_TensorFlow_On_GCP/05.Art_And_Science_Of_Machine_Learning/WEEK_1/02.Improve_Model_Accuracy_By_Hyperparameter_Tuning_With_AI_Platform/Practice/Venv/lib/python3.6/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mnamed_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshebang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# write a basic docstring:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-110>\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n",
      "\u001b[0;32m/media/mujahid7292/Data/GoogleDriveSandCorp2014/ML_With_TensorFlow_On_GCP/05.Art_And_Science_Of_Machine_Learning/WEEK_1/02.Improve_Model_Accuracy_By_Hyperparameter_Tuning_With_AI_Platform/Practice/Venv/lib/python3.6/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/mujahid7292/Data/GoogleDriveSandCorp2014/ML_With_TensorFlow_On_GCP/05.Art_And_Science_Of_Machine_Learning/WEEK_1/02.Improve_Model_Accuracy_By_Hyperparameter_Tuning_With_AI_Platform/Practice/Venv/lib/python3.6/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_error\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_close\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'OUTDIR=gs://${BUCKET}/house_trained   # CHANGE bucket name appropriately\\ngsutil rm -rf $OUTDIR\\nexport PYTHONPATH=${PYTHONPATH}:${PWD}/house_prediction_module\\ngcloud ai-platform jobs submit training house_$(date -u +%y%m%d_%H%M%S) \\\\\\n   --config=hyperparam.yaml \\\\\\n   --module-name=trainer.task \\\\\\n   --package-path=$(pwd)/house_prediction_module/trainer \\\\\\n   --job-dir=$OUTDIR \\\\\\n   --runtime-version=$TFVERSION \\\\\\n   --\\\\\\n   --output_dir=$OUTDIR \\\\\\n'' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "OUTDIR=gs://${BUCKET}/house_trained   # CHANGE bucket name appropriately\n",
    "gsutil rm -rf $OUTDIR\n",
    "export PYTHONPATH=${PYTHONPATH}:${PWD}/house_prediction_module\n",
    "gcloud ai-platform jobs submit training house_$(date -u +%y%m%d_%H%M%S) \\\n",
    "   --config=hyperparam.yaml \\\n",
    "   --module-name=trainer.task \\\n",
    "   --package-path=$(pwd)/house_prediction_module/trainer \\\n",
    "   --job-dir=$OUTDIR \\\n",
    "   --runtime-version=$TFVERSION \\\n",
    "   --\\\n",
    "   --output_dir=$OUTDIR \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
