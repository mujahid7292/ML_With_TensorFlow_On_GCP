{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going from one machine to many, might sound complicated, but as we will see, with the estimator API and ML engine managing the cluster automatically, you get distribution out of the box. The function that implements distributed training is called `tf.estimator.train_and_evaluate()`. The name of the function also highlights that evaluating and monitoring a large training job will be important. We will see that later. Let's focus for now on distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Training:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/Snap_1.jpg\" width=\"800\" height=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Data Parallelism: </b> Data Parallelism replicate your model on multiple workers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The traditional distribution model for training neural networks is called data parallelism. Your model is replicated on multiple workers. At each training steps, these load a batch of training data, hopefully a different one each, compute their gradients and send them to one or several central parameter servers, which hold all the weights and biases of the neural network model. The gradients are applied as they arrive. Change the weights and biases, and the updated model is then sent back to workers for the next step of training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot of work to do to make this happen. Workers must be started, then they receive their copy of the model, data flows between workers. Parameter servers must be established. The system must also handle exceptions and failures and if an incident occurs restart failed workers from where they left off. And check pointing also becomes a bit more complicated, when all this is going on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately for you, distribution will be as simple as writing a config file. All the boilerplate code is already written in the estimator API(`tf.estimator.train_and_evaluate()`). You will need to do four things.\n",
    "\n",
    "<li><b>Choose your estimator.</b></li>\n",
    "<li><b>Provide a run configuration.</b></li>\n",
    "<li><b>Provide training data through `train_spec`</b></li>\n",
    "<li><b>Provide evaluating data through `eval_spec`</b></li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once that is set up, call `tf.estimator.train_and_evaluate()`. And if you are running on ML engine, and have specified the cluster size, distributed training will kick in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/Snap_2.jpg\" width=\"800\" height=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have a look at above four things"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"./trained_model\"\n",
    "run_config = tf.estimator.RunConfig(\n",
    "    model_dir=OUTPUT_DIR, \n",
    "    save_summary_steps=100,\n",
    "    save_checkpoints_steps=2000 # After every 2000 steps our model will save weights and biases.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The run config first. This is where you specify the output directory for checkpoints. You can still set it directly, when instantiating the estimator, but it's cleaner to have it here, along with other checkpoint settings. Indeed, this is also where you set the frequency at which you want to see checkpoints and also the frequency of your training logs or summaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = tf.estimator.LinearRegressor(\n",
    "    feature_columns=feat_cols,\n",
    "    config=run_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_spec = tf.estimator.TrainSpec(input_fn=train_input_fn, max_steps=50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `tf.estimator.TrainSpec()`, is where you pass in your data input function for training data. Please use the data set API to set it up properly. Optionally, you can limit the training to a given number of steps. By default, training proceeds until the input data set is exhausted. Which might happen after multiple epochs if that's how you set it up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval Spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_spec = tf.estimator.EvalSpec(\n",
    "    input_fn=eval_input_fn,\n",
    "    steps=100, # We want to evaluate 100 batches of test data\n",
    "    throttle_secs=600, # 10 Minutes (We want to run our evaluation on every 10 minutes)\n",
    "    exporters=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `EvalSpec`, is where you plug in your test data set. Yes, if you want to see how well your model is doing, you have to measure that on a data set that it has not seen during training. Usually a subset of your data that you set aside for testing. The test data comes in through an `eval_input_fn` and again, please use the data set API to get it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You also get to specify on how many batches of test data you want to evaluate, and how frequently evaluations happen.  \n",
    "\n",
    "One implementation detail to bear in mind, in distributed training, evaluation happens on a dedicated server, which respond the model from the latest checkpoint and then runs eval. So, you cannot get evaluations more frequently than the check points frequency you entered in your `run_config`. You can, however, get them less frequently, by adding the throttling parameter in the EvalSpec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You notice that the EvalSpec also has a parameter for exporters. They control how a model is exported for deployment to production, and we will cover them in the next chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running The Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.estimator.train_and_evaluate(\n",
    "    estimator=estimator,\n",
    "    train_spec=train_spec,\n",
    "    eval_spec=eval_spec\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importance Of Data Shuffling In Distributed Training:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to say a couple of words about an important practical consideration, data shuffling. The stochastic gradient descent algorithm that neural networks use for training, only works on well-shuffled data. The data set API has a shuffle function that can help there, but some people might not use it if they think their data set is already well shuffled on disk. With distributed training, beware. Even with a well-shuffled data set on disk, if all your workers are loading straight from this data set, they will be seeing the same batch of data, at the same time, and produce the same gradients. The benefits of distributed training are wasted. Your multiple workers all do the exact same things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.list_files(\"train.csv-*\") \\\n",
    "                         .shuffle(100) \\\n",
    "                         .flat_mapla(tf.data.TextLineDataset) \\\n",
    "                         .map(decode_csv)\n",
    "\n",
    "dataset = dataset.shuffle(1000) \\\n",
    "                 .repeat(15) \\\n",
    "                 .batch(128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With data set that shuffle, the shuffling happens independently on each worker using a different random seed, so please use it. Even if your data comes already shuffled on disk. \n",
    "\n",
    "And if you want to be extra sure, you can also shuffle the list of filenames in your sharded data set. `tf.data.Dataset.list_files()`, returns a data set of filenames, so just call shuffle on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
