{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Text Classification using TensorFlow/Keras on AI Platform </h1>\n",
    "\n",
    "This notebook illustrates:\n",
    "<ol>\n",
    "<li> Creating datasets for AI Platform using BigQuery\n",
    "<li> Creating a text classification model using the Estimator API with a Keras model\n",
    "<li> Training on Cloud AI Platform\n",
    "<li> Rerun with pre-trained embedding\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the right version of Tensorflow is installed.\n",
    "!pip freeze | grep tensorflow==2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change these to try this notebook out\n",
    "ACCOUNT = 'sandcorp2014@gmail.com'\n",
    "SAC = 'jupyter-notebook-sac-f'\n",
    "SAC_KEY_DESTINATION = '/media/mujahid7292/Data/Gcloud_Tem_SAC'\n",
    "BUCKET = 'ml-practice-260405'\n",
    "PROJECT = 'ml-practice-260405'\n",
    "REGION = 'us-central1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['ACCOUNT'] = ACCOUNT\n",
    "os.environ['SAC'] = SAC\n",
    "os.environ['SAC_KEY_DESTINATION'] = SAC_KEY_DESTINATION# LogIn To Google Cloud\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['REGION'] = REGION\n",
    "os.environ['TFVERSION'] = '2.1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogIn To Google Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gcloud auth login $ACCOUNT --force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'COLAB_GPU' in os.environ:  # this is always set on Colab, the value is 0 or 1 depending on whether a GPU is attached\n",
    "  from google.colab import auth\n",
    "  auth.authenticate_user()\n",
    "  # download \"sidecar files\" since on Colab, this notebook will be on Drive\n",
    "  !rm -rf txtclsmodel\n",
    "  !git clone --depth 1 https://github.com/GoogleCloudPlatform/training-data-analyst\n",
    "  !mv  training-data-analyst/courses/machine_learning/deepdive/09_sequence/txtclsmodel/ .\n",
    "  !rm -rf training-data-analyst\n",
    "  # downgrade TensorFlow to the version this notebook has been tested with\n",
    "  !pip install --upgrade tensorflow==$TFVERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Google Application Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]='{}/{}.json'.format(SAC_KEY_DESTINATION,SAC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Whether Google Application Credential Was Set Successfully Outside Virtual Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "set | grep GOOGLE_APPLICATION_CREDENTIALS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will look at the titles of articles and figure out whether the article came from the New York Times, TechCrunch or GitHub. \n",
    "\n",
    "We will use [hacker news](https://news.ycombinator.com/) as our data source. It is an aggregator that displays tech related headlines from various  sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Dataset from BigQuery \n",
    "\n",
    "Hacker news headlines are available as a BigQuery public dataset. The [dataset](https://bigquery.cloud.google.com/table/bigquery-public-data:hacker_news.stories?tab=details) contains all headlines from the sites inception in October 2006 until October 2015. \n",
    "\n",
    "Here is a sample of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext google.cloud.bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery --project $PROJECT\n",
    "SELECT\n",
    "  url, title, score\n",
    "FROM\n",
    "  `bigquery-public-data.hacker_news.stories`\n",
    "WHERE\n",
    "  LENGTH(title) > 10\n",
    "  AND score > 10\n",
    "  AND LENGTH(url) > 0\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some regular expression parsing in BigQuery to get the source of the newspaper article from the URL. For example, if the url is http://mobile.nytimes.com/...., I want to be left with <i>nytimes</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery --project $PROJECT\n",
    "SELECT\n",
    "  ARRAY_REVERSE(SPLIT(REGEXP_EXTRACT(url, '.*://(.[^/]+)/'), '.'))[OFFSET(1)] AS source,\n",
    "  COUNT(title) AS num_articles\n",
    "FROM\n",
    "  `bigquery-public-data.hacker_news.stories`\n",
    "WHERE\n",
    "  REGEXP_CONTAINS(REGEXP_EXTRACT(url, '.*://(.[^/]+)/'), '.com$')\n",
    "  AND LENGTH(title) > 10\n",
    "GROUP BY\n",
    "  source\n",
    "ORDER BY num_articles DESC\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have good parsing of the URL to get the source, let's put together a dataset of source and titles. This will be our labeled dataset for AI Platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "bq = bigquery.Client(project=PROJECT)\n",
    "\n",
    "query=\"\"\"\n",
    "SELECT source, LOWER(REGEXP_REPLACE(title, '[^a-zA-Z0-9 $.-]', ' ')) AS title FROM\n",
    "  (SELECT\n",
    "    ARRAY_REVERSE(SPLIT(REGEXP_EXTRACT(url, '.*://(.[^/]+)/'), '.'))[OFFSET(1)] AS source,\n",
    "    title\n",
    "  FROM\n",
    "    `bigquery-public-data.hacker_news.stories`\n",
    "  WHERE\n",
    "    REGEXP_CONTAINS(REGEXP_EXTRACT(url, '.*://(.[^/]+)/'), '.com$')\n",
    "    AND LENGTH(title) > 10\n",
    "  )\n",
    "WHERE (source = 'github' OR source = 'nytimes' OR source = 'techcrunch')\n",
    "\"\"\"\n",
    "\n",
    "df = bq.query(query + \" LIMIT 5\").to_dataframe()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ML training, we will need to split our dataset into training and evaluation datasets (and perhaps an independent test dataset if we are going to do model or feature selection based on the evaluation dataset).  \n",
    "\n",
    "A simple, repeatable way to do this is to use the hash of a well-distributed column in our data (See https://www.oreilly.com/learning/repeatable-sampling-of-data-sets-in-bigquery-for-machine-learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf = bq.query(query + \" AND ABS(MOD(FARM_FINGERPRINT(title), 4)) > 0\").to_dataframe()\n",
    "evaldf  = bq.query(query + \" AND ABS(MOD(FARM_FINGERPRINT(title), 4)) = 0\").to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we can see that roughly 75% of the data is used for training, and 25% for evaluation. \n",
    "\n",
    "We can also see that within each dataset, the classes are roughly balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf['source'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaldf['source'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we will save our data, which is currently in-memory, to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "DATADIR='data/txtcls'\n",
    "shutil.rmtree(DATADIR, ignore_errors=True)\n",
    "os.makedirs(DATADIR)\n",
    "traindf.to_csv( os.path.join(DATADIR,'train.tsv'), header=False, index=False, encoding='utf-8', sep='\\t')\n",
    "evaldf.to_csv( os.path.join(DATADIR,'eval.tsv'), header=False, index=False, encoding='utf-8', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -3 data/txtcls/train.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l data/txtcls/*.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow/Keras Code\n",
    "\n",
    "Please explore the code in this <a href=\"txtclsmodel/trainer\">directory</a>: `model.py` contains the TensorFlow model and `task.py` parses command line arguments and launches off the training job.\n",
    "\n",
    "In particular look for the following:\n",
    "\n",
    "1. [tf.keras.preprocessing.text.Tokenizer.fit_on_texts()](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer#fit_on_texts) to generate a mapping from our word vocabulary to integers\n",
    "2. [tf.keras.preprocessing.text.Tokenizer.texts_to_sequences()](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer#texts_to_sequences) to encode our sentences into a sequence of their respective word-integers\n",
    "3. [tf.keras.preprocessing.sequence.pad_sequences()](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences) to pad all sequences to be the same length\n",
    "\n",
    "The embedding layer in the keras model takes care of one-hot encoding these integers and learning a dense emedding represetation from them. \n",
    "\n",
    "Finally we pass the embedded text representation through a CNN model pictured below\n",
    "\n",
    "<img src=images/txtcls_model.png  width=25%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Locally (optional step)\n",
    "Let's make sure the code compiles by running locally for a fraction of an epoch.\n",
    "This may not work if you don't have all the packages installed locally for gcloud (such as in Colab).\n",
    "This is an optional step; move on to training on the cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: six in /home/mujahid7292/anaconda3/lib/python3.7/site-packages (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.1; however, version 20.2.3 is available.\n",
      "You should consider upgrading via the '/home/mujahid7292/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.0\n"
     ]
    }
   ],
   "source": [
    "import six\n",
    "print(six.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>github</td>\n",
       "      <td>this guy just found out how to bypass adblocker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>github</td>\n",
       "      <td>show hn  dodo   command line task management f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>github</td>\n",
       "      <td>show hn  webservicemock   mock out external ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>github</td>\n",
       "      <td>magento category attributes dependency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>github</td>\n",
       "      <td>write actionscript in swift whaa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    label                                               text\n",
       "0  github    this guy just found out how to bypass adblocker\n",
       "1  github  show hn  dodo   command line task management f...\n",
       "2  github  show hn  webservicemock   mock out external ca...\n",
       "3  github             magento category attributes dependency\n",
       "4  github                  write actionscript in swift whaa "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "column_names = ('label', 'text')\n",
    "train_data_path = './data/txtcls/train.csv'\n",
    "df_train = pd.read_csv(\n",
    "    train_data_path, \n",
    "    names=column_names, \n",
    "    sep='\\t'\n",
    ")\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-storage in /home/mujahid7292/anaconda3/lib/python3.7/site-packages (1.31.2)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /home/mujahid7292/anaconda3/lib/python3.7/site-packages (from google-cloud-storage) (2.23.0)\n",
      "Requirement already satisfied: google-auth<2.0dev,>=1.11.0 in /home/mujahid7292/anaconda3/lib/python3.7/site-packages (from google-cloud-storage) (1.13.1)\n",
      "Requirement already satisfied: google-resumable-media<2.0dev,>=1.0.0 in /home/mujahid7292/anaconda3/lib/python3.7/site-packages (from google-cloud-storage) (1.1.0)\n",
      "Requirement already satisfied: google-cloud-core<2.0dev,>=1.4.1 in /home/mujahid7292/anaconda3/lib/python3.7/site-packages (from google-cloud-storage) (1.4.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/mujahid7292/anaconda3/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2019.11.28)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/mujahid7292/anaconda3/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/mujahid7292/anaconda3/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (1.25.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/mujahid7292/anaconda3/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.0.4)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/mujahid7292/anaconda3/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage) (3.1.1)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /home/mujahid7292/anaconda3/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage) (46.0.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/mujahid7292/anaconda3/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage) (0.2.8)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in /home/mujahid7292/anaconda3/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage) (4.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/mujahid7292/anaconda3/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage) (1.15.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0; python_version >= \"3.5\" in /home/mujahid7292/anaconda3/lib/python3.7/site-packages (from google-resumable-media<2.0dev,>=1.0.0->google-cloud-storage) (1.0.0)\n",
      "Requirement already satisfied: google-api-core<2.0.0dev,>=1.19.0 in /home/mujahid7292/anaconda3/lib/python3.7/site-packages (from google-cloud-core<2.0dev,>=1.4.1->google-cloud-storage) (1.22.4)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/mujahid7292/anaconda3/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.11.0->google-cloud-storage) (0.4.8)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /home/mujahid7292/anaconda3/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0; python_version >= \"3.5\"->google-resumable-media<2.0dev,>=1.0.0->google-cloud-storage) (1.12.3)\n",
      "Requirement already satisfied: pytz in /home/mujahid7292/anaconda3/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.19.0->google-cloud-core<2.0dev,>=1.4.1->google-cloud-storage) (2019.3)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /home/mujahid7292/anaconda3/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.19.0->google-cloud-core<2.0dev,>=1.4.1->google-cloud-storage) (3.13.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /home/mujahid7292/anaconda3/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.19.0->google-cloud-core<2.0dev,>=1.4.1->google-cloud-storage) (1.51.0)\n",
      "Requirement already satisfied: pycparser in /home/mujahid7292/anaconda3/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0; python_version >= \"3.5\"->google-resumable-media<2.0dev,>=1.0.0->google-cloud-storage) (2.19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.1; however, version 20.2.3 is available.\n",
      "You should consider upgrading via the '/home/mujahid7292/anaconda3/bin/python -m pip install --upgrade pip' command.\n",
      "2020-10-11 08:22:59.659537: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory\n",
      "2020-10-11 08:22:59.659617: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory\n",
      "2020-10-11 08:22:59.659628: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I1011 08:23:01.798858 139767572309824 run_config.py:535] TF_CONFIG environment variable: {'environment': 'cloud', 'cluster': {}, 'job': {'args': ['--output_dir=./txtcls_trained', '--train_data_path=./train.csv', '--eval_data_path=./eval.csv', '--num_epochs=5'], 'job_name': 'trainer.task'}, 'task': {}}\n",
      "2020-10-11 08:23:01.811564: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2020-10-11 08:23:01.811583: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2020-10-11 08:23:01.811597: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (mujahid7292-HP-ENVY-Notebook): /proc/driver/nvidia/version does not exist\n",
      "2020-10-11 08:23:01.812612: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2020-10-11 08:23:01.849429: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2899885000 Hz\n",
      "2020-10-11 08:23:01.850784: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55e709380830 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-10-11 08:23:01.850855: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "I1011 08:23:02.010946 139767572309824 keras.py:540] Using the Keras model provided.\n",
      "W1011 08:23:02.020279 139767572309824 deprecation.py:506] From /home/mujahid7292/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "I1011 08:23:02.818403 139767572309824 estimator.py:216] Using config: {'_model_dir': './txtcls_trained', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "W1011 08:23:03.710037 139767572309824 module_wrapper.py:138] From /media/mujahid7292/Data/GoogleDriveSandCorp2014/ML_With_TensorFlow_On_GCP/09.Sequence Models for Time Series and Natural Language Processing/Week_2/Lab_7_Text Classification using TensorFlow Or Keras on AI Platform/sequence/txtclsmodel/trainer/model.py:105: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n",
      "W1011 08:23:03.710174 139767572309824 module_wrapper.py:138] From /media/mujahid7292/Data/GoogleDriveSandCorp2014/ML_With_TensorFlow_On_GCP/09.Sequence Models for Time Series and Natural Language Processing/Week_2/Lab_7_Text Classification using TensorFlow Or Keras on AI Platform/sequence/txtclsmodel/trainer/model.py:105: The name tf.estimator.inputs.numpy_input_fn is deprecated. Please use tf.compat.v1.estimator.inputs.numpy_input_fn instead.\n",
      "\n",
      "I1011 08:23:04.063291 139767572309824 estimator_training.py:186] Not using Distribute Coordinator.\n",
      "I1011 08:23:04.063460 139767572309824 training.py:612] Running training and evaluation locally (non-distributed).\n",
      "I1011 08:23:04.063626 139767572309824 training.py:700] Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 500 or save_checkpoints_secs None.\n",
      "W1011 08:23:04.067373 139767572309824 deprecation.py:323] From /home/mujahid7292/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "W1011 08:23:04.073472 139767572309824 deprecation.py:323] From /home/mujahid7292/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_queue_runner.py:62: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "W1011 08:23:04.074034 139767572309824 deprecation.py:323] From /home/mujahid7292/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_functions.py:500: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "I1011 08:23:04.077568 139767572309824 estimator.py:1151] Calling model_fn.\n",
      "I1011 08:23:04.306660 139767572309824 estimator.py:1153] Done calling model_fn.\n",
      "I1011 08:23:04.306804 139767572309824 estimator.py:1372] Warm-starting with WarmStartSettings: WarmStartSettings(ckpt_to_initialize_from='./txtcls_trained/keras/keras_model.ckpt', vars_to_warm_start='.*', var_name_to_vocab_info={}, var_name_to_prev_var_name={})\n",
      "I1011 08:23:04.306861 139767572309824 warm_starting_util.py:464] Warm-starting from: ./txtcls_trained/keras/keras_model.ckpt\n",
      "I1011 08:23:04.306897 139767572309824 warm_starting_util.py:343] Warm-starting variables only in TRAINABLE_VARIABLES.\n",
      "I1011 08:23:04.323779 139767572309824 warm_starting_util.py:538] Warm-started 7 variables.\n",
      "I1011 08:23:04.324447 139767572309824 basic_session_run_hooks.py:546] Create CheckpointSaverHook.\n",
      "I1011 08:23:04.402120 139767572309824 monitored_session.py:246] Graph was finalized.\n",
      "I1011 08:23:04.531282 139767572309824 session_manager.py:504] Running local_init_op.\n",
      "I1011 08:23:04.540133 139767572309824 session_manager.py:507] Done running local_init_op.\n",
      "W1011 08:23:04.567277 139767572309824 deprecation.py:323] From /home/mujahid7292/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/training/monitored_session.py:906: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "I1011 08:23:04.773428 139767572309824 basic_session_run_hooks.py:613] Saving checkpoints for 0 into ./txtcls_trained/model.ckpt.\n",
      "I1011 08:23:05.337172 139767572309824 basic_session_run_hooks.py:262] loss = 1.0914071, step = 0\n",
      "I1011 08:23:10.567273 139767572309824 basic_session_run_hooks.py:700] global_step/sec: 19.1192\n",
      "I1011 08:23:10.567813 139767572309824 basic_session_run_hooks.py:260] loss = 0.6461099, step = 100 (5.231 sec)\n",
      "I1011 08:23:15.603116 139767572309824 basic_session_run_hooks.py:700] global_step/sec: 19.8576\n",
      "I1011 08:23:15.603536 139767572309824 basic_session_run_hooks.py:260] loss = 0.56629324, step = 200 (5.036 sec)\n",
      "I1011 08:23:20.264796 139767572309824 basic_session_run_hooks.py:700] global_step/sec: 21.4515\n",
      "I1011 08:23:20.265174 139767572309824 basic_session_run_hooks.py:260] loss = 0.37059176, step = 300 (4.662 sec)\n",
      "I1011 08:23:24.750925 139767572309824 basic_session_run_hooks.py:700] global_step/sec: 22.291\n",
      "I1011 08:23:24.751445 139767572309824 basic_session_run_hooks.py:260] loss = 0.43181074, step = 400 (4.486 sec)\n",
      "I1011 08:23:29.229831 139767572309824 basic_session_run_hooks.py:613] Saving checkpoints for 500 into ./txtcls_trained/model.ckpt.\n",
      "I1011 08:23:29.564715 139767572309824 estimator.py:1151] Calling model_fn.\n",
      "I1011 08:23:29.663277 139767572309824 estimator.py:1153] Done calling model_fn.\n",
      "I1011 08:23:29.677280 139767572309824 evaluation.py:255] Starting evaluation at 2020-10-11T08:23:29Z\n",
      "I1011 08:23:29.705077 139767572309824 monitored_session.py:246] Graph was finalized.\n",
      "I1011 08:23:29.706147 139767572309824 saver.py:1284] Restoring parameters from ./txtcls_trained/model.ckpt-500\n",
      "I1011 08:23:29.739605 139767572309824 session_manager.py:504] Running local_init_op.\n",
      "I1011 08:23:29.749230 139767572309824 session_manager.py:507] Done running local_init_op.\n",
      "I1011 08:23:31.063820 139767572309824 evaluation.py:273] Inference Time : 1.38643s\n",
      "I1011 08:23:31.064333 139767572309824 evaluation.py:276] Finished evaluation at 2020-10-11-08:23:31\n",
      "I1011 08:23:31.064448 139767572309824 estimator.py:2053] Saving dict for global step 500: acc = 0.7756749, global_step = 500, loss = 0.53388935\n",
      "I1011 08:23:31.094378 139767572309824 estimator.py:2113] Saving 'checkpoint_path' summary for global step 500: ./txtcls_trained/model.ckpt-500\n",
      "I1011 08:23:31.099215 139767572309824 estimator.py:1151] Calling model_fn.\n",
      "I1011 08:23:31.177589 139767572309824 estimator.py:1153] Done calling model_fn.\n",
      "W1011 08:23:31.177766 139767572309824 deprecation.py:323] From /home/mujahid7292/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "I1011 08:23:31.177927 139767572309824 export_utils.py:170] Signatures INCLUDED in export for Classify: None\n",
      "I1011 08:23:31.177981 139767572309824 export_utils.py:170] Signatures INCLUDED in export for Regress: None\n",
      "I1011 08:23:31.178036 139767572309824 export_utils.py:170] Signatures INCLUDED in export for Predict: ['serving_default']\n",
      "I1011 08:23:31.178082 139767572309824 export_utils.py:170] Signatures INCLUDED in export for Train: None\n",
      "I1011 08:23:31.178126 139767572309824 export_utils.py:170] Signatures INCLUDED in export for Eval: None\n",
      "I1011 08:23:31.180085 139767572309824 saver.py:1284] Restoring parameters from ./txtcls_trained/model.ckpt-500\n",
      "I1011 08:23:31.197826 139767572309824 builder_impl.py:666] Assets added to graph.\n",
      "I1011 08:23:31.197927 139767572309824 builder_impl.py:461] No assets to write.\n",
      "I1011 08:23:31.309977 139767572309824 builder_impl.py:426] SavedModel written to: ./txtcls_trained/export/exporter/temp-b'1602383011'/saved_model.pb\n",
      "I1011 08:23:31.354802 139767572309824 basic_session_run_hooks.py:700] global_step/sec: 15.1426\n",
      "I1011 08:23:31.355255 139767572309824 basic_session_run_hooks.py:260] loss = 0.30492103, step = 500 (6.604 sec)\n",
      "I1011 08:23:35.946590 139767572309824 basic_session_run_hooks.py:700] global_step/sec: 21.778\n",
      "I1011 08:23:35.947052 139767572309824 basic_session_run_hooks.py:260] loss = 0.38856035, step = 600 (4.592 sec)\n",
      "I1011 08:23:40.719136 139767572309824 basic_session_run_hooks.py:700] global_step/sec: 20.9532\n",
      "I1011 08:23:40.719617 139767572309824 basic_session_run_hooks.py:260] loss = 0.32903874, step = 700 (4.773 sec)\n",
      "I1011 08:23:45.289893 139767572309824 basic_session_run_hooks.py:700] global_step/sec: 21.8782\n",
      "I1011 08:23:45.290732 139767572309824 basic_session_run_hooks.py:260] loss = 0.36413366, step = 800 (4.571 sec)\n",
      "I1011 08:23:49.901858 139767572309824 basic_session_run_hooks.py:700] global_step/sec: 21.6827\n",
      "I1011 08:23:49.902307 139767572309824 basic_session_run_hooks.py:260] loss = 0.23644151, step = 900 (4.612 sec)\n",
      "I1011 08:23:54.558095 139767572309824 basic_session_run_hooks.py:613] Saving checkpoints for 1000 into ./txtcls_trained/model.ckpt.\n",
      "I1011 08:23:55.342947 139767572309824 estimator.py:1151] Calling model_fn.\n",
      "I1011 08:23:55.444145 139767572309824 estimator.py:1153] Done calling model_fn.\n",
      "I1011 08:23:55.457226 139767572309824 evaluation.py:255] Starting evaluation at 2020-10-11T08:23:55Z\n",
      "I1011 08:23:55.483381 139767572309824 monitored_session.py:246] Graph was finalized.\n",
      "I1011 08:23:55.484452 139767572309824 saver.py:1284] Restoring parameters from ./txtcls_trained/model.ckpt-1000\n",
      "I1011 08:23:55.515695 139767572309824 session_manager.py:504] Running local_init_op.\n",
      "I1011 08:23:55.524227 139767572309824 session_manager.py:507] Done running local_init_op.\n",
      "I1011 08:23:56.667843 139767572309824 evaluation.py:273] Inference Time : 1.21051s\n",
      "I1011 08:23:56.668319 139767572309824 evaluation.py:276] Finished evaluation at 2020-10-11-08:23:56\n",
      "I1011 08:23:56.668412 139767572309824 estimator.py:2053] Saving dict for global step 1000: acc = 0.8112807, global_step = 1000, loss = 0.47185326\n",
      "I1011 08:23:56.668734 139767572309824 estimator.py:2113] Saving 'checkpoint_path' summary for global step 1000: ./txtcls_trained/model.ckpt-1000\n",
      "I1011 08:23:56.673090 139767572309824 estimator.py:1151] Calling model_fn.\n",
      "I1011 08:23:56.751169 139767572309824 estimator.py:1153] Done calling model_fn.\n",
      "I1011 08:23:56.751394 139767572309824 export_utils.py:170] Signatures INCLUDED in export for Classify: None\n",
      "I1011 08:23:56.751463 139767572309824 export_utils.py:170] Signatures INCLUDED in export for Regress: None\n",
      "I1011 08:23:56.751503 139767572309824 export_utils.py:170] Signatures INCLUDED in export for Predict: ['serving_default']\n",
      "I1011 08:23:56.751537 139767572309824 export_utils.py:170] Signatures INCLUDED in export for Train: None\n",
      "I1011 08:23:56.751567 139767572309824 export_utils.py:170] Signatures INCLUDED in export for Eval: None\n",
      "I1011 08:23:56.753675 139767572309824 saver.py:1284] Restoring parameters from ./txtcls_trained/model.ckpt-1000\n",
      "I1011 08:23:56.771764 139767572309824 builder_impl.py:666] Assets added to graph.\n",
      "I1011 08:23:56.771871 139767572309824 builder_impl.py:461] No assets to write.\n",
      "I1011 08:23:57.021107 139767572309824 builder_impl.py:426] SavedModel written to: ./txtcls_trained/export/exporter/temp-b'1602383036'/saved_model.pb\n",
      "I1011 08:23:57.072436 139767572309824 basic_session_run_hooks.py:700] global_step/sec: 13.9459\n",
      "I1011 08:23:57.072834 139767572309824 basic_session_run_hooks.py:260] loss = 0.22276178, step = 1000 (7.171 sec)\n",
      "I1011 08:24:01.794159 139767572309824 basic_session_run_hooks.py:700] global_step/sec: 21.1788\n",
      "I1011 08:24:01.794642 139767572309824 basic_session_run_hooks.py:260] loss = 0.36921573, step = 1100 (4.722 sec)\n",
      "I1011 08:24:06.418858 139767572309824 basic_session_run_hooks.py:700] global_step/sec: 21.623\n",
      "I1011 08:24:06.419297 139767572309824 basic_session_run_hooks.py:260] loss = 0.32346237, step = 1200 (4.625 sec)\n",
      "I1011 08:24:11.077986 139767572309824 basic_session_run_hooks.py:700] global_step/sec: 21.4633\n",
      "I1011 08:24:11.078469 139767572309824 basic_session_run_hooks.py:260] loss = 0.34146702, step = 1300 (4.659 sec)\n",
      "I1011 08:24:15.700245 139767572309824 basic_session_run_hooks.py:700] global_step/sec: 21.6344\n",
      "I1011 08:24:15.700811 139767572309824 basic_session_run_hooks.py:260] loss = 0.27153295, step = 1400 (4.622 sec)\n",
      "I1011 08:24:20.189244 139767572309824 basic_session_run_hooks.py:613] Saving checkpoints for 1500 into ./txtcls_trained/model.ckpt.\n",
      "I1011 08:24:20.637940 139767572309824 estimator.py:1151] Calling model_fn.\n",
      "I1011 08:24:20.725368 139767572309824 estimator.py:1153] Done calling model_fn.\n",
      "I1011 08:24:20.739244 139767572309824 evaluation.py:255] Starting evaluation at 2020-10-11T08:24:20Z\n",
      "I1011 08:24:20.764974 139767572309824 monitored_session.py:246] Graph was finalized.\n",
      "I1011 08:24:20.766076 139767572309824 saver.py:1284] Restoring parameters from ./txtcls_trained/model.ckpt-1500\n",
      "I1011 08:24:20.797399 139767572309824 session_manager.py:504] Running local_init_op.\n",
      "I1011 08:24:20.805577 139767572309824 session_manager.py:507] Done running local_init_op.\n",
      "I1011 08:24:22.015321 139767572309824 evaluation.py:273] Inference Time : 1.27597s\n",
      "I1011 08:24:22.015826 139767572309824 evaluation.py:276] Finished evaluation at 2020-10-11-08:24:22\n",
      "I1011 08:24:22.015934 139767572309824 estimator.py:2053] Saving dict for global step 1500: acc = 0.81772804, global_step = 1500, loss = 0.46632555\n",
      "I1011 08:24:22.016323 139767572309824 estimator.py:2113] Saving 'checkpoint_path' summary for global step 1500: ./txtcls_trained/model.ckpt-1500\n",
      "I1011 08:24:22.020807 139767572309824 estimator.py:1151] Calling model_fn.\n",
      "I1011 08:24:22.100219 139767572309824 estimator.py:1153] Done calling model_fn.\n",
      "I1011 08:24:22.100461 139767572309824 export_utils.py:170] Signatures INCLUDED in export for Classify: None\n",
      "I1011 08:24:22.100533 139767572309824 export_utils.py:170] Signatures INCLUDED in export for Regress: None\n",
      "I1011 08:24:22.100591 139767572309824 export_utils.py:170] Signatures INCLUDED in export for Predict: ['serving_default']\n",
      "I1011 08:24:22.100639 139767572309824 export_utils.py:170] Signatures INCLUDED in export for Train: None\n",
      "I1011 08:24:22.100685 139767572309824 export_utils.py:170] Signatures INCLUDED in export for Eval: None\n",
      "I1011 08:24:22.102756 139767572309824 saver.py:1284] Restoring parameters from ./txtcls_trained/model.ckpt-1500\n",
      "I1011 08:24:22.120817 139767572309824 builder_impl.py:666] Assets added to graph.\n",
      "I1011 08:24:22.120922 139767572309824 builder_impl.py:461] No assets to write.\n",
      "I1011 08:24:22.229906 139767572309824 builder_impl.py:426] SavedModel written to: ./txtcls_trained/export/exporter/temp-b'1602383062'/saved_model.pb\n",
      "I1011 08:24:22.276670 139767572309824 basic_session_run_hooks.py:700] global_step/sec: 15.2058\n",
      "I1011 08:24:22.277105 139767572309824 basic_session_run_hooks.py:260] loss = 0.18223384, step = 1500 (6.576 sec)\n",
      "I1011 08:24:26.743764 139767572309824 basic_session_run_hooks.py:700] global_step/sec: 22.386\n",
      "I1011 08:24:26.744214 139767572309824 basic_session_run_hooks.py:260] loss = 0.20406252, step = 1600 (4.467 sec)\n",
      "I1011 08:24:31.340747 139767572309824 basic_session_run_hooks.py:700] global_step/sec: 21.7536\n",
      "I1011 08:24:31.341936 139767572309824 basic_session_run_hooks.py:260] loss = 0.17798312, step = 1700 (4.598 sec)\n",
      "I1011 08:24:35.803510 139767572309824 basic_session_run_hooks.py:700] global_step/sec: 22.4074\n",
      "I1011 08:24:35.803956 139767572309824 basic_session_run_hooks.py:260] loss = 0.21266958, step = 1800 (4.462 sec)\n",
      "I1011 08:24:40.274959 139767572309824 basic_session_run_hooks.py:700] global_step/sec: 22.3641\n",
      "I1011 08:24:40.275407 139767572309824 basic_session_run_hooks.py:260] loss = 0.17779702, step = 1900 (4.471 sec)\n",
      "I1011 08:24:44.669708 139767572309824 basic_session_run_hooks.py:613] Saving checkpoints for 2000 into ./txtcls_trained/model.ckpt.\n",
      "I1011 08:24:44.973163 139767572309824 estimator.py:1151] Calling model_fn.\n",
      "I1011 08:24:45.061295 139767572309824 estimator.py:1153] Done calling model_fn.\n",
      "I1011 08:24:45.073965 139767572309824 evaluation.py:255] Starting evaluation at 2020-10-11T08:24:45Z\n",
      "I1011 08:24:45.099275 139767572309824 monitored_session.py:246] Graph was finalized.\n",
      "I1011 08:24:45.100550 139767572309824 saver.py:1284] Restoring parameters from ./txtcls_trained/model.ckpt-2000\n",
      "I1011 08:24:45.131696 139767572309824 session_manager.py:504] Running local_init_op.\n",
      "I1011 08:24:45.139872 139767572309824 session_manager.py:507] Done running local_init_op.\n",
      "I1011 08:24:46.366780 139767572309824 evaluation.py:273] Inference Time : 1.29270s\n",
      "I1011 08:24:46.367475 139767572309824 evaluation.py:276] Finished evaluation at 2020-10-11-08:24:46\n",
      "I1011 08:24:46.367621 139767572309824 estimator.py:2053] Saving dict for global step 2000: acc = 0.8114887, global_step = 2000, loss = 0.52709347\n",
      "I1011 08:24:46.368023 139767572309824 estimator.py:2113] Saving 'checkpoint_path' summary for global step 2000: ./txtcls_trained/model.ckpt-2000\n",
      "I1011 08:24:46.374947 139767572309824 estimator.py:1151] Calling model_fn.\n",
      "I1011 08:24:46.478841 139767572309824 estimator.py:1153] Done calling model_fn.\n",
      "I1011 08:24:46.479078 139767572309824 export_utils.py:170] Signatures INCLUDED in export for Classify: None\n",
      "I1011 08:24:46.479151 139767572309824 export_utils.py:170] Signatures INCLUDED in export for Regress: None\n",
      "I1011 08:24:46.479193 139767572309824 export_utils.py:170] Signatures INCLUDED in export for Predict: ['serving_default']\n",
      "I1011 08:24:46.479228 139767572309824 export_utils.py:170] Signatures INCLUDED in export for Train: None\n",
      "I1011 08:24:46.479260 139767572309824 export_utils.py:170] Signatures INCLUDED in export for Eval: None\n",
      "I1011 08:24:46.481281 139767572309824 saver.py:1284] Restoring parameters from ./txtcls_trained/model.ckpt-2000\n",
      "I1011 08:24:46.499358 139767572309824 builder_impl.py:666] Assets added to graph.\n",
      "I1011 08:24:46.499463 139767572309824 builder_impl.py:461] No assets to write.\n",
      "I1011 08:24:46.608939 139767572309824 builder_impl.py:426] SavedModel written to: ./txtcls_trained/export/exporter/temp-b'1602383086'/saved_model.pb\n",
      "I1011 08:24:46.651014 139767572309824 basic_session_run_hooks.py:700] global_step/sec: 15.6837\n",
      "I1011 08:24:46.651481 139767572309824 basic_session_run_hooks.py:260] loss = 0.23573199, step = 2000 (6.376 sec)\n",
      "I1011 08:24:51.089164 139767572309824 basic_session_run_hooks.py:700] global_step/sec: 22.5319\n",
      "I1011 08:24:51.089603 139767572309824 basic_session_run_hooks.py:260] loss = 0.25941664, step = 2100 (4.438 sec)\n",
      "I1011 08:24:55.542163 139767572309824 basic_session_run_hooks.py:700] global_step/sec: 22.4568\n",
      "I1011 08:24:55.542627 139767572309824 basic_session_run_hooks.py:260] loss = 0.17471927, step = 2200 (4.453 sec)\n",
      "I1011 08:24:59.962487 139767572309824 basic_session_run_hooks.py:700] global_step/sec: 22.6228\n",
      "I1011 08:24:59.963075 139767572309824 basic_session_run_hooks.py:260] loss = 0.14779897, step = 2300 (4.420 sec)\n",
      "I1011 08:25:04.348537 139767572309824 basic_session_run_hooks.py:700] global_step/sec: 22.7996\n",
      "I1011 08:25:04.349146 139767572309824 basic_session_run_hooks.py:260] loss = 0.19319212, step = 2400 (4.386 sec)\n",
      "I1011 08:25:08.721907 139767572309824 basic_session_run_hooks.py:613] Saving checkpoints for 2500 into ./txtcls_trained/model.ckpt.\n",
      "W1011 08:25:08.995409 139767572309824 deprecation.py:323] From /home/mujahid7292/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "I1011 08:25:09.037014 139767572309824 estimator.py:1151] Calling model_fn.\n",
      "I1011 08:25:09.124167 139767572309824 estimator.py:1153] Done calling model_fn.\n",
      "I1011 08:25:09.136917 139767572309824 evaluation.py:255] Starting evaluation at 2020-10-11T08:25:09Z\n",
      "I1011 08:25:09.162359 139767572309824 monitored_session.py:246] Graph was finalized.\n",
      "I1011 08:25:09.163592 139767572309824 saver.py:1284] Restoring parameters from ./txtcls_trained/model.ckpt-2500\n",
      "I1011 08:25:09.194895 139767572309824 session_manager.py:504] Running local_init_op.\n",
      "I1011 08:25:09.202987 139767572309824 session_manager.py:507] Done running local_init_op.\n",
      "I1011 08:25:10.421926 139767572309824 evaluation.py:273] Inference Time : 1.28490s\n",
      "I1011 08:25:10.422423 139767572309824 evaluation.py:276] Finished evaluation at 2020-10-11-08:25:10\n",
      "I1011 08:25:10.422521 139767572309824 estimator.py:2053] Saving dict for global step 2500: acc = 0.8098249, global_step = 2500, loss = 0.5657222\n",
      "I1011 08:25:10.422873 139767572309824 estimator.py:2113] Saving 'checkpoint_path' summary for global step 2500: ./txtcls_trained/model.ckpt-2500\n",
      "I1011 08:25:10.427358 139767572309824 estimator.py:1151] Calling model_fn.\n",
      "I1011 08:25:10.508712 139767572309824 estimator.py:1153] Done calling model_fn.\n",
      "I1011 08:25:10.508942 139767572309824 export_utils.py:170] Signatures INCLUDED in export for Classify: None\n",
      "I1011 08:25:10.509015 139767572309824 export_utils.py:170] Signatures INCLUDED in export for Regress: None\n",
      "I1011 08:25:10.509065 139767572309824 export_utils.py:170] Signatures INCLUDED in export for Predict: ['serving_default']\n",
      "I1011 08:25:10.509100 139767572309824 export_utils.py:170] Signatures INCLUDED in export for Train: None\n",
      "I1011 08:25:10.509131 139767572309824 export_utils.py:170] Signatures INCLUDED in export for Eval: None\n",
      "I1011 08:25:10.511092 139767572309824 saver.py:1284] Restoring parameters from ./txtcls_trained/model.ckpt-2500\n",
      "I1011 08:25:10.529147 139767572309824 builder_impl.py:666] Assets added to graph.\n",
      "I1011 08:25:10.529247 139767572309824 builder_impl.py:461] No assets to write.\n",
      "I1011 08:25:10.635904 139767572309824 builder_impl.py:426] SavedModel written to: ./txtcls_trained/export/exporter/temp-b'1602383110'/saved_model.pb\n",
      "I1011 08:25:10.679416 139767572309824 basic_session_run_hooks.py:700] global_step/sec: 15.7956\n",
      "I1011 08:25:10.679868 139767572309824 basic_session_run_hooks.py:260] loss = 0.12112759, step = 2500 (6.331 sec)\n",
      "I1011 08:25:15.109715 139767572309824 basic_session_run_hooks.py:700] global_step/sec: 22.5719\n",
      "I1011 08:25:15.110187 139767572309824 basic_session_run_hooks.py:260] loss = 0.12765412, step = 2600 (4.430 sec)\n",
      "I1011 08:25:19.590426 139767572309824 basic_session_run_hooks.py:700] global_step/sec: 22.3179\n",
      "I1011 08:25:19.590874 139767572309824 basic_session_run_hooks.py:260] loss = 0.14168406, step = 2700 (4.481 sec)\n",
      "I1011 08:25:23.952040 139767572309824 basic_session_run_hooks.py:700] global_step/sec: 22.9273\n",
      "I1011 08:25:23.952488 139767572309824 basic_session_run_hooks.py:260] loss = 0.10883534, step = 2800 (4.362 sec)\n",
      "I1011 08:25:24.750205 139767572309824 basic_session_run_hooks.py:613] Saving checkpoints for 2819 into ./txtcls_trained/model.ckpt.\n",
      "I1011 08:25:25.055469 139767572309824 estimator.py:1151] Calling model_fn.\n",
      "I1011 08:25:25.142935 139767572309824 estimator.py:1153] Done calling model_fn.\n",
      "I1011 08:25:25.155682 139767572309824 evaluation.py:255] Starting evaluation at 2020-10-11T08:25:25Z\n",
      "I1011 08:25:25.181097 139767572309824 monitored_session.py:246] Graph was finalized.\n",
      "I1011 08:25:25.182285 139767572309824 saver.py:1284] Restoring parameters from ./txtcls_trained/model.ckpt-2819\n",
      "I1011 08:25:25.213624 139767572309824 session_manager.py:504] Running local_init_op.\n",
      "I1011 08:25:25.221568 139767572309824 session_manager.py:507] Done running local_init_op.\n",
      "I1011 08:25:26.447289 139767572309824 evaluation.py:273] Inference Time : 1.29150s\n",
      "I1011 08:25:26.447797 139767572309824 evaluation.py:276] Finished evaluation at 2020-10-11-08:25:26\n",
      "I1011 08:25:26.447908 139767572309824 estimator.py:2053] Saving dict for global step 2819: acc = 0.8073291, global_step = 2819, loss = 0.60722566\n",
      "I1011 08:25:26.448262 139767572309824 estimator.py:2113] Saving 'checkpoint_path' summary for global step 2819: ./txtcls_trained/model.ckpt-2819\n",
      "I1011 08:25:26.452688 139767572309824 estimator.py:1151] Calling model_fn.\n",
      "I1011 08:25:26.533944 139767572309824 estimator.py:1153] Done calling model_fn.\n",
      "I1011 08:25:26.534192 139767572309824 export_utils.py:170] Signatures INCLUDED in export for Classify: None\n",
      "I1011 08:25:26.534265 139767572309824 export_utils.py:170] Signatures INCLUDED in export for Regress: None\n",
      "I1011 08:25:26.534324 139767572309824 export_utils.py:170] Signatures INCLUDED in export for Predict: ['serving_default']\n",
      "I1011 08:25:26.534372 139767572309824 export_utils.py:170] Signatures INCLUDED in export for Train: None\n",
      "I1011 08:25:26.534420 139767572309824 export_utils.py:170] Signatures INCLUDED in export for Eval: None\n",
      "I1011 08:25:26.536508 139767572309824 saver.py:1284] Restoring parameters from ./txtcls_trained/model.ckpt-2819\n",
      "I1011 08:25:26.554868 139767572309824 builder_impl.py:666] Assets added to graph.\n",
      "I1011 08:25:26.554969 139767572309824 builder_impl.py:461] No assets to write.\n",
      "I1011 08:25:26.768084 139767572309824 builder_impl.py:426] SavedModel written to: ./txtcls_trained/export/exporter/temp-b'1602383126'/saved_model.pb\n",
      "I1011 08:25:26.908522 139767572309824 estimator.py:375] Loss for final step: 0.13962376.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip install google-cloud-storage\n",
    "rm -rf txtcls_trained\n",
    "gcloud ai-platform local train \\\n",
    "   --module-name=trainer.task \\\n",
    "   --package-path=./txtclsmodel/trainer \\\n",
    "   -- \\\n",
    "   --output_dir=./txtcls_trained \\\n",
    "   --train_data_path=./train.csv \\\n",
    "   --eval_data_path=./eval.csv \\\n",
    "   --num_epochs=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on the Cloud\n",
    "\n",
    "Let's first copy our training data to the cloud:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gsutil cp data/txtcls/*.tsv gs://${BUCKET}/txtcls/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "OUTDIR=gs://${BUCKET}/txtcls/trained_fromscratch\n",
    "JOBNAME=txtcls_$(date -u +%y%m%d_%H%M%S)\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "gcloud ai-platform jobs submit training $JOBNAME \\\n",
    " --region=$REGION \\\n",
    " --module-name=trainer.task \\\n",
    " --package-path=${PWD}/txtclsmodel/trainer \\\n",
    " --job-dir=$OUTDIR \\\n",
    " --scale-tier=BASIC_GPU \\\n",
    " --runtime-version 2.1 \\\n",
    " --python-version 3.7 \\\n",
    " -- \\\n",
    " --output_dir=$OUTDIR \\\n",
    " --train_data_path=gs://${BUCKET}/txtcls/train.tsv \\\n",
    " --eval_data_path=gs://${BUCKET}/txtcls/eval.tsv \\\n",
    " --num_epochs=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the job name appropriately. View the job in the console, and wait until the job is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud ai-platform jobs describe txtcls_190209_224828"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "What accuracy did you get? You should see around 80%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rerun with Pre-trained Embedding\n",
    "\n",
    "We will use the popular GloVe embedding which is trained on Wikipedia as well as various news sources like the New York Times.\n",
    "\n",
    "You can read more about Glove at the project homepage: https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "You can download the embedding files directly from the stanford.edu site, but we've rehosted it in a GCS bucket for faster download speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp gs://cloud-training-demos/courses/machine_learning/deepdive/09_sequence/text_classification/glove.6B.200d.txt gs://$BUCKET/txtcls/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the embedding is downloaded re-run your cloud training job with the added command line argument: \n",
    "\n",
    "` --embedding_path=gs://${BUCKET}/txtcls/glove.6B.200d.txt`\n",
    "\n",
    "While the final accuracy may not change significantly, you should notice the model is able to converge to it much more quickly because it no longer has to learn an embedding from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "- This implementation is based on code from: https://github.com/google/eng-edu/tree/master/ml/guides/text_classification.\n",
    "- See the full text classification tutorial at: https://developers.google.com/machine-learning/guides/text-classification/\n",
    "\n",
    "## Next step\n",
    "Client-side tokenizing in Python is hugely problematic. See <a href=\"text_classification_native.ipynb\">Text classification with native serving</a> for how to carry out the preprocessing in the serving function itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2020 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
