{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Text Classification using TensorFlow/Keras on AI Platform </h1>\n",
    "\n",
    "This notebook illustrates:\n",
    "<ol>\n",
    "<li> Creating datasets for AI Platform using BigQuery\n",
    "<li> Creating a text classification model using the Estimator API with a Keras model\n",
    "<li> Training on Cloud AI Platform\n",
    "<li> Rerun with pre-trained embedding\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the right version of Tensorflow is installed.\n",
    "!pip freeze | grep tensorflow==2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change these to try this notebook out\n",
    "ACCOUNT = 'sandcorp2014@gmail.com'\n",
    "SAC = 'jupyter-notebook-sac-f'\n",
    "SAC_KEY_DESTINATION = '/media/mujahid7292/Data/Gcloud_Tem_SAC'\n",
    "BUCKET = 'ml-practice-260405'\n",
    "PROJECT = 'ml-practice-260405'\n",
    "REGION = 'us-central1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['ACCOUNT'] = ACCOUNT\n",
    "os.environ['SAC'] = SAC\n",
    "os.environ['SAC_KEY_DESTINATION'] = SAC_KEY_DESTINATION# LogIn To Google Cloud\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['REGION'] = REGION\n",
    "os.environ['TFVERSION'] = '2.1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogIn To Google Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening in existing browser session.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your browser has been opened to visit:\n",
      "\n",
      "    https://accounts.google.com/o/oauth2/auth?code_challenge=_q3BTAY957bYnIfTsSQtvFWZVfNfPISdbJwMtk99qCU&prompt=select_account&code_challenge_method=S256&access_type=offline&redirect_uri=http%3A%2F%2Flocalhost%3A8085%2F&response_type=code&client_id=32555940559.apps.googleusercontent.com&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fappengine.admin+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcompute+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth\n",
      "\n",
      "\n",
      "[1011/094541.459934:ERROR:nacl_helper_linux.cc(308)] NaCl helper process running without a sandbox!\n",
      "Most likely you need to configure your SUID sandbox correctly\n",
      "\n",
      "You are now logged in as [sandcorp2014@gmail.com].\n",
      "Your current project is [ml-practice-260405].  You can change this setting by running:\n",
      "  $ gcloud config set project PROJECT_ID\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud auth login $ACCOUNT --force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'COLAB_GPU' in os.environ:  # this is always set on Colab, the value is 0 or 1 depending on whether a GPU is attached\n",
    "  from google.colab import auth\n",
    "  auth.authenticate_user()\n",
    "  # download \"sidecar files\" since on Colab, this notebook will be on Drive\n",
    "  !rm -rf txtclsmodel\n",
    "  !git clone --depth 1 https://github.com/GoogleCloudPlatform/training-data-analyst\n",
    "  !mv  training-data-analyst/courses/machine_learning/deepdive/09_sequence/txtclsmodel/ .\n",
    "  !rm -rf training-data-analyst\n",
    "  # downgrade TensorFlow to the version this notebook has been tested with\n",
    "  !pip install --upgrade tensorflow==$TFVERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Google Application Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]='{}/{}.json'.format(SAC_KEY_DESTINATION,SAC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Whether Google Application Credential Was Set Successfully Outside Virtual Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GOOGLE_APPLICATION_CREDENTIALS=/media/mujahid7292/Data/Gcloud_Tem_SAC/jupyter-notebook-sac-f.json\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "set | grep GOOGLE_APPLICATION_CREDENTIALS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will look at the titles of articles and figure out whether the article came from the New York Times, TechCrunch or GitHub. \n",
    "\n",
    "We will use [hacker news](https://news.ycombinator.com/) as our data source. It is an aggregator that displays tech related headlines from various  sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Dataset from BigQuery \n",
    "\n",
    "Hacker news headlines are available as a BigQuery public dataset. The [dataset](https://bigquery.cloud.google.com/table/bigquery-public-data:hacker_news.stories?tab=details) contains all headlines from the sites inception in October 2006 until October 2015. \n",
    "\n",
    "Here is a sample of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext google.cloud.bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>http://www.dumpert.nl/mediabase/6560049/3eb18e...</td>\n",
       "      <td>Calling the NSA: \"I accidentally deleted an e-...</td>\n",
       "      <td>258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>http://blog.liip.ch/archive/2013/10/28/hhvm-an...</td>\n",
       "      <td>Amazing performance with HHVM and PHP with a S...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>http://www.gamedev.net/page/resources/_/techni...</td>\n",
       "      <td>A Journey Through the CPU Pipeline</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>http://jfarcand.wordpress.com/2011/02/25/atmos...</td>\n",
       "      <td>Atmosphere Framework 0.7 released: GWT, Wicket...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>http://tech.gilt.com/post/90578399884/immutabl...</td>\n",
       "      <td>Immutable Infrastructure with Docker and EC2 [...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>http://thechangelog.com/post/501053444/episode...</td>\n",
       "      <td>Changelog 0.2.0 - node.js w/Felix Geisendorfer</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>http://openangelforum.com/2010/09/09/second-bo...</td>\n",
       "      <td>Second Open Angel Forum in Boston Oct 13th--fr...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>http://bredele.github.io/async</td>\n",
       "      <td>A collection of JavaScript asynchronous patterns</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>http://www.smashingmagazine.com/2007/08/25/20-...</td>\n",
       "      <td>20 Free and Fresh Icon Sets</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>http://www.cio.com/article/147801/Study_Finds_...</td>\n",
       "      <td>Study: Only 1 in 5 Workers is \"Engaged\" in The...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  http://www.dumpert.nl/mediabase/6560049/3eb18e...   \n",
       "1  http://blog.liip.ch/archive/2013/10/28/hhvm-an...   \n",
       "2  http://www.gamedev.net/page/resources/_/techni...   \n",
       "3  http://jfarcand.wordpress.com/2011/02/25/atmos...   \n",
       "4  http://tech.gilt.com/post/90578399884/immutabl...   \n",
       "5  http://thechangelog.com/post/501053444/episode...   \n",
       "6  http://openangelforum.com/2010/09/09/second-bo...   \n",
       "7                     http://bredele.github.io/async   \n",
       "8  http://www.smashingmagazine.com/2007/08/25/20-...   \n",
       "9  http://www.cio.com/article/147801/Study_Finds_...   \n",
       "\n",
       "                                               title  score  \n",
       "0  Calling the NSA: \"I accidentally deleted an e-...    258  \n",
       "1  Amazing performance with HHVM and PHP with a S...     11  \n",
       "2                 A Journey Through the CPU Pipeline     11  \n",
       "3  Atmosphere Framework 0.7 released: GWT, Wicket...     11  \n",
       "4  Immutable Infrastructure with Docker and EC2 [...     11  \n",
       "5     Changelog 0.2.0 - node.js w/Felix Geisendorfer     11  \n",
       "6  Second Open Angel Forum in Boston Oct 13th--fr...     11  \n",
       "7   A collection of JavaScript asynchronous patterns     11  \n",
       "8                        20 Free and Fresh Icon Sets     11  \n",
       "9  Study: Only 1 in 5 Workers is \"Engaged\" in The...     11  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery --project $PROJECT\n",
    "SELECT\n",
    "  url, title, score\n",
    "FROM\n",
    "  `bigquery-public-data.hacker_news.stories`\n",
    "WHERE\n",
    "  LENGTH(title) > 10\n",
    "  AND score > 10\n",
    "  AND LENGTH(url) > 0\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some regular expression parsing in BigQuery to get the source of the newspaper article from the URL. For example, if the url is http://mobile.nytimes.com/...., I want to be left with <i>nytimes</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>num_articles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>blogspot</td>\n",
       "      <td>41386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>github</td>\n",
       "      <td>36525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>techcrunch</td>\n",
       "      <td>30891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>youtube</td>\n",
       "      <td>30848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>nytimes</td>\n",
       "      <td>28787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>medium</td>\n",
       "      <td>18422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>google</td>\n",
       "      <td>18235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>wordpress</td>\n",
       "      <td>17667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>arstechnica</td>\n",
       "      <td>13749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>wired</td>\n",
       "      <td>12841</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        source  num_articles\n",
       "0     blogspot         41386\n",
       "1       github         36525\n",
       "2   techcrunch         30891\n",
       "3      youtube         30848\n",
       "4      nytimes         28787\n",
       "5       medium         18422\n",
       "6       google         18235\n",
       "7    wordpress         17667\n",
       "8  arstechnica         13749\n",
       "9        wired         12841"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery --project $PROJECT\n",
    "SELECT\n",
    "  ARRAY_REVERSE(SPLIT(REGEXP_EXTRACT(url, '.*://(.[^/]+)/'), '.'))[OFFSET(1)] AS source,\n",
    "  COUNT(title) AS num_articles\n",
    "FROM\n",
    "  `bigquery-public-data.hacker_news.stories`\n",
    "WHERE\n",
    "  REGEXP_CONTAINS(REGEXP_EXTRACT(url, '.*://(.[^/]+)/'), '.com$')\n",
    "  AND LENGTH(title) > 10\n",
    "GROUP BY\n",
    "  source\n",
    "ORDER BY num_articles DESC\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have good parsing of the URL to get the source, let's put together a dataset of source and titles. This will be our labeled dataset for AI Platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>github</td>\n",
       "      <td>erlang  eunit and travis without rebar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>github</td>\n",
       "      <td>node-pngdefry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>github</td>\n",
       "      <td>let s study for google</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>github</td>\n",
       "      <td>mcabber-festival   im   speech notifications</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>github</td>\n",
       "      <td>an open source pok mon game on ios with locati...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   source                                              title\n",
       "0  github             erlang  eunit and travis without rebar\n",
       "1  github                                      node-pngdefry\n",
       "2  github                             let s study for google\n",
       "3  github       mcabber-festival   im   speech notifications\n",
       "4  github  an open source pok mon game on ios with locati..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "bq = bigquery.Client(project=PROJECT)\n",
    "\n",
    "query=\"\"\"\n",
    "SELECT source, LOWER(REGEXP_REPLACE(title, '[^a-zA-Z0-9 $.-]', ' ')) AS title FROM\n",
    "  (SELECT\n",
    "    ARRAY_REVERSE(SPLIT(REGEXP_EXTRACT(url, '.*://(.[^/]+)/'), '.'))[OFFSET(1)] AS source,\n",
    "    title\n",
    "  FROM\n",
    "    `bigquery-public-data.hacker_news.stories`\n",
    "  WHERE\n",
    "    REGEXP_CONTAINS(REGEXP_EXTRACT(url, '.*://(.[^/]+)/'), '.com$')\n",
    "    AND LENGTH(title) > 10\n",
    "  )\n",
    "WHERE (source = 'github' OR source = 'nytimes' OR source = 'techcrunch')\n",
    "\"\"\"\n",
    "\n",
    "df = bq.query(query + \" LIMIT 5\").to_dataframe()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ML training, we will need to split our dataset into training and evaluation datasets (and perhaps an independent test dataset if we are going to do model or feature selection based on the evaluation dataset).  \n",
    "\n",
    "A simple, repeatable way to do this is to use the hash of a well-distributed column in our data (See https://www.oreilly.com/learning/repeatable-sampling-of-data-sets-in-bigquery-for-machine-learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf = bq.query(query + \" AND ABS(MOD(FARM_FINGERPRINT(title), 4)) > 0\").to_dataframe()\n",
    "evaldf  = bq.query(query + \" AND ABS(MOD(FARM_FINGERPRINT(title), 4)) = 0\").to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we can see that roughly 75% of the data is used for training, and 25% for evaluation. \n",
    "\n",
    "We can also see that within each dataset, the classes are roughly balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "github        27445\n",
       "techcrunch    23131\n",
       "nytimes       21586\n",
       "Name: source, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindf['source'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "github        9080\n",
       "techcrunch    7760\n",
       "nytimes       7201\n",
       "Name: source, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaldf['source'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we will save our data, which is currently in-memory, to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "DATADIR='data/txtcls'\n",
    "shutil.rmtree(DATADIR, ignore_errors=True)\n",
    "os.makedirs(DATADIR)\n",
    "traindf.to_csv( os.path.join(DATADIR,'train.tsv'), header=False, index=False, encoding='utf-8', sep='\\t')\n",
    "evaldf.to_csv( os.path.join(DATADIR,'eval.tsv'), header=False, index=False, encoding='utf-8', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "github\tthis guy just found out how to bypass adblocker\r\n",
      "github\tshow hn  dodo   command line task management for developers\r\n",
      "github\tshow hn  webservicemock   mock out external calls for local development\r\n"
     ]
    }
   ],
   "source": [
    "!head -3 data/txtcls/train.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  24041 data/txtcls/eval.tsv\r\n",
      "  72162 data/txtcls/train.tsv\r\n",
      "  96203 total\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l data/txtcls/*.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow/Keras Code\n",
    "\n",
    "Please explore the code in this <a href=\"txtclsmodel/trainer\">directory</a>: `model.py` contains the TensorFlow model and `task.py` parses command line arguments and launches off the training job.\n",
    "\n",
    "In particular look for the following:\n",
    "\n",
    "1. [tf.keras.preprocessing.text.Tokenizer.fit_on_texts()](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer#fit_on_texts) to generate a mapping from our word vocabulary to integers\n",
    "2. [tf.keras.preprocessing.text.Tokenizer.texts_to_sequences()](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer#texts_to_sequences) to encode our sentences into a sequence of their respective word-integers\n",
    "3. [tf.keras.preprocessing.sequence.pad_sequences()](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences) to pad all sequences to be the same length\n",
    "\n",
    "The embedding layer in the keras model takes care of one-hot encoding these integers and learning a dense emedding represetation from them. \n",
    "\n",
    "Finally we pass the embedded text representation through a CNN model pictured below\n",
    "\n",
    "<img src=txtcls_model.png  width=25%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting txtclsmodel/trainer/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile txtclsmodel/trainer/__init__.py\n",
    "# Empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting txtclsmodel/trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile txtclsmodel/trainer/task.py\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "from . import model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # parse command line arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        '--output_dir',\n",
    "        help='GCS location to write checkpoints and export models',\n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--train_data_path',\n",
    "        help='can be a local path or a GCS url (gs://...)',\n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--eval_data_path',\n",
    "        help='can be a local path or a GCS url (gs://...)',\n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--embedding_path',\n",
    "        help='OPTIONAL: can be a local path or a GCS url (gs://...). \\\n",
    "              Download from: https://nlp.stanford.edu/projects/glove/',\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--num_epochs',\n",
    "        help='number of times to go through the data, default=10',\n",
    "        default=10,\n",
    "        type=float\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--batch_size',\n",
    "        help='number of records to read during each training step, default=128',\n",
    "        default=128,\n",
    "        type=int\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--learning_rate',\n",
    "        help='learning rate for gradient descent, default=.001',\n",
    "        default=.001,\n",
    "        type=float\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--native',\n",
    "        action='store_true',\n",
    "        help='use native in-graph pre-processing functions',\n",
    "    )\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "    hparams = args.__dict__\n",
    "    output_dir = hparams.pop('output_dir')\n",
    "    \n",
    "    # initiate training\n",
    "    model.train_and_evaluate(output_dir, hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting txtclsmodel/trainer/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile txtclsmodel/trainer/model.py\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.preprocessing import text\n",
    "from tensorflow.python.keras import models\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.layers import Dropout\n",
    "from tensorflow.python.keras.layers import Embedding\n",
    "from tensorflow.python.keras.layers import Conv1D\n",
    "from tensorflow.python.keras.layers import MaxPool1D\n",
    "from tensorflow.python.keras.layers import GlobalAveragePooling1D\n",
    "\n",
    "from google.cloud import storage\n",
    "\n",
    "\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)\n",
    "\n",
    "CLASSES = {'github': 0, 'nytimes': 1, 'techcrunch': 2}  # label-to-int mapping\n",
    "TOP_K = 20000  # Limit on the number vocabulary size used for tokenization\n",
    "MAX_SEQUENCE_LENGTH = 50  # Sentences will be truncated/padded to this length\n",
    "\n",
    "\n",
    "def download_from_gcs(source, destination):\n",
    "    \"\"\"\n",
    "    Helper function to download data from Google Cloud Storage\n",
    "      # Arguments:\n",
    "          source: string, the GCS URL to download from (e.g. 'gs://bucket/file.csv')\n",
    "          destination: string, the filename to save as on local disk. MUST be filename\n",
    "          ONLY, doesn't support folders. (e.g. 'file.csv', NOT 'folder/file.csv')\n",
    "      # Returns: nothing, downloads file to local disk\n",
    "    \"\"\"\n",
    "    search = re.search('gs://(.*?)/(.*)', source)\n",
    "    bucket_name = search.group(1)\n",
    "    blob_name = search.group(2)\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    bucket.blob(blob_name).download_to_filename(destination)\n",
    "    \n",
    "\n",
    "def load_hacker_news_data(train_data_path, eval_data_path):\n",
    "    \"\"\"\n",
    "    Parses raw tsv containing hacker news headlines and \n",
    "    returns (sentence, integer label) pairs.\n",
    "      # Arguments:\n",
    "          train_data_path: string, path to tsv containing training data.\n",
    "            can be a local path or a GCS url (gs://...)\n",
    "          eval_data_path: string, path to tsv containing eval data.\n",
    "            can be a local path or a GCS url (gs://...)\n",
    "      # Returns:\n",
    "          ((train_sentences, train_labels), (test_sentences, test_labels)):  sentences\n",
    "            are lists of strings, labels are numpy integer arrays\n",
    "    \"\"\"\n",
    "    if train_data_path.startswith('gs://'):\n",
    "        download_from_gcs(train_data_path, destination='train.csv')\n",
    "        train_data_path='train.csv'\n",
    "    if eval_data_path.startswith('gs://'):\n",
    "        download_from_gcs(eval_data_path, destination='eval.csv')\n",
    "        eval_data_path='eval.csv'\n",
    "        \n",
    "    # Parse CSV using pandas\n",
    "    column_names=('label','text')\n",
    "    \n",
    "    df_train = pd.read_csv(\n",
    "        train_data_path,\n",
    "        names=column_names,\n",
    "        sep='\\t'\n",
    "    )\n",
    "    df_eval = pd.read_csv(\n",
    "        eval_data_path,\n",
    "        names=column_names,\n",
    "        sep='\\t'\n",
    "    )\n",
    "    return (\n",
    "        (list(df_train['text']), np.array(df_train['label'].map(CLASSES))),\n",
    "        (list(df_eval['text']), np.array(df_eval['label'].map(CLASSES)))\n",
    "    )\n",
    "\n",
    "\n",
    "def input_fn(texts, labels, tokenizer, batch_size, mode):\n",
    "    \"\"\"\n",
    "    Create tf.estimator compatible input function\n",
    "      # Arguments:\n",
    "          texts: [strings], list of sentences\n",
    "          labels: numpy int vector, integer labels for sentences\n",
    "          tokenizer: tf.python.keras.preprocessing.text.Tokenizer\n",
    "            used to convert sentences to integers\n",
    "          batch_size: int, number of records to use for each train batch\n",
    "          mode: tf.estimator.ModeKeys.TRAIN or tf.estimator.ModeKeys.EVAL\n",
    "      # Returns:\n",
    "          tf.estimator.inputs.numpy_input_fn, produces feature and label\n",
    "            tensors one batch at a time\n",
    "    \"\"\"\n",
    "    \n",
    "    # Transform text to sequence of integer.\n",
    "    x = tokenizer.texts_to_sequnces(texts)\n",
    "    \n",
    "    # Fix sequence length to max value. Sequences shorter \n",
    "    # than the length are padded in the beginning and sequences\n",
    "    # longer are truncated at the beginning.\n",
    "    x = sequence.pad_sequences(\n",
    "        x,\n",
    "        maxlen=MAX_SEQUENCE_LENGTH\n",
    "    )\n",
    "    \n",
    "    # default settings for training\n",
    "    num_epochs = None\n",
    "    shuffle = True\n",
    "\n",
    "    # override if this is eval\n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        num_epochs = 1\n",
    "        shuffle = False\n",
    "        \n",
    "    return tf.compat.v1.estimator.inputs.numpy_input_fn(\n",
    "        x,\n",
    "        y=labels,\n",
    "        batch_size=batch_size,\n",
    "        num_epochs=num_epochs,\n",
    "        shuffle=shuffle,\n",
    "        queue_capacity=50000\n",
    "    )\n",
    "\n",
    "\n",
    "def keras_estimator(model_dir, config, learning_rate, filters=64,\n",
    "                   dropout_rate=0.2, embedding_dim=200,\n",
    "                   kernel_size=3, pool_size=3, embedding_path=None,\n",
    "                   word_index=None):\n",
    "    \"\"\"\n",
    "    Builds a CNN model using keras and converts to tf.estimator.Estimator\n",
    "      # Arguments\n",
    "          model_dir: string, file path where training files will be written\n",
    "          config: tf.estimator.RunConfig, specifies properties of tf Estimator\n",
    "          filters: int, output dimension of the layers.\n",
    "          kernel_size: int, length of the convolution window.\n",
    "          embedding_dim: int, dimension of the embedding vectors.\n",
    "          dropout_rate: float, percentage of input to drop at Dropout layers.\n",
    "          pool_size: int, factor by which to downscale input at MaxPooling layer.\n",
    "          embedding_path: string , file location of pre-trained embedding (if used)\n",
    "            defaults to None which will cause the model to train embedding from scratch\n",
    "          word_index: dictionary, mapping of vocabulary to integers. used only if\n",
    "            pre-trained embedding is provided\n",
    "\n",
    "        # Returns\n",
    "            A tf.estimator.Estimator\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create model instances.\n",
    "    model = models.Sequential()\n",
    "    num_features = min(len(word_index) + 1, TOP_K)\n",
    "    \n",
    "    # Add embedding layer. If pre-trained embedding is used add \n",
    "    # weights to the embeddings layer and set trainable to input\n",
    "    # is_embedding_trainable flag.\n",
    "    if embedding_path != None:\n",
    "        embedding_matrix = get_embedding_matrix(\n",
    "            word_index, embedding_path, embedding_dim\n",
    "        )\n",
    "        \n",
    "        is_embedding_trainable = True  # set to False to freeze embedding weights\n",
    "        \n",
    "        model.add(Embedding(\n",
    "            input_dim=num_features,\n",
    "            output_dim=embedding_dim,\n",
    "            input_length=MAX_SEQUENCE_LENGTH,\n",
    "            weights=[embedding_matrix],\n",
    "            trainable=is_embedding_trainable\n",
    "        ))\n",
    "    else:\n",
    "        model.add(Embedding(\n",
    "            input_dim=num_features,\n",
    "            output_dim=embedding_dim,\n",
    "            input_length=MAX_SEQUENCE_LENGTH,\n",
    "        ))\n",
    "        \n",
    "    model.add(Dropout(\n",
    "        rate=dropout_rate\n",
    "    ))\n",
    "    \n",
    "    model.add(Conv1D(\n",
    "        filters=filters,\n",
    "        kernel_size=kernel_size,\n",
    "        activation='relu',\n",
    "        bias_initializer='random_uniform',\n",
    "        padding='same'\n",
    "    ))\n",
    "    model.add(MaxPool1D(\n",
    "        pool_size=pool_size\n",
    "    ))\n",
    "    \n",
    "    model.add(Conv1D(\n",
    "        filters=filters * 2,\n",
    "        kernel_size=kernel_size,\n",
    "        activation='relu',\n",
    "        bias_initializer='random_uniform',\n",
    "        padding='same'\n",
    "    ))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    \n",
    "    model.add(Dropout(\n",
    "        rate=dropout_rate\n",
    "    ))\n",
    "    \n",
    "    model.add(Dense(\n",
    "        len(CLASSES),\n",
    "        activation='softmax'\n",
    "    ))\n",
    "    \n",
    "    # Compile model with learning parameters.\n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "        learning_rate=learning_rate\n",
    "    )\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['acc']\n",
    "    )\n",
    "    \n",
    "    estimator = tf.keras.estimator.model_to_estimator(\n",
    "        keras_model=model,\n",
    "        model_dir=model_dir,\n",
    "        config=config\n",
    "    )\n",
    "    \n",
    "    return estimator\n",
    "\n",
    "\n",
    "def serving_input_fn():\n",
    "    \"\"\"\n",
    "    Defines the features to be passed to the model during inference\n",
    "      Expects already tokenized and padded representation of sentences\n",
    "      # Arguments: none\n",
    "      # Returns: tf.estimator.export.ServingInputReceiver\n",
    "    \"\"\"\n",
    "    feature_placeholder = tf.compat.v1.placeholder(tf.int16, [None, MAX_SEQUENCE_LENGTH])\n",
    "    features = feature_placeholder  # pass as-is\n",
    "    return tf.estimator.export.TensorServingInputReceiver(features, feature_placeholder)\n",
    "\n",
    "def get_embedding_matrix(word_index, embedding_path, embedding_dim):\n",
    "    \"\"\"\n",
    "    Takes embedding for generic voabulary and extracts the embeddings\n",
    "      matching the current vocabulary\n",
    "      The pre-trained embedding file is obtained from https://nlp.stanford.edu/projects/glove/\n",
    "      # Arguments:\n",
    "          word_index: dict, {key =word in vocabulary: value= integer mapped to that word}\n",
    "          embedding_path: string, location of the pre-trained embedding file on disk\n",
    "          embedding_dim: int, dimension of the embedding space\n",
    "      # Returns: numpy matrix of shape (vocabulary, embedding_dim) that contains the embedded\n",
    "          representation of each word in the vocabulary.\n",
    "    \"\"\"\n",
    "    # Read the pre-trained embedding file and get word to word \n",
    "    # vector mappings.\n",
    "    embedding_matrix_all = {}\n",
    "    \n",
    "    # Download if embedding file is in GCS\n",
    "    if embedding_path.startswith('gs://'):\n",
    "        download_from_gcs(\n",
    "            embedding_path,\n",
    "            destination='embedding.csv'\n",
    "        )\n",
    "        embedding_path = 'embedding.csv'\n",
    "        \n",
    "    with open(embedding_path) as f:\n",
    "        for line in f:\n",
    "            # Every line contains word followed by the vector value.\n",
    "            values = line.split()\n",
    "            word = values[0] # First item is word.\n",
    "            coefs = np.asarray(\n",
    "                values[1:], # Rest of the items are vector.\n",
    "                dtype='float32'\n",
    "            )\n",
    "            embedding_matrix_all[word] =coefs\n",
    "    \n",
    "    # Prepare embedding matrix with just the words in our \n",
    "    # word_index dictionary.\n",
    "    num_words = min(len(word_index) + 1, TOP_K)\n",
    "    embedding_matrix = np.zeros(\n",
    "        (num_words, embedding_dim)\n",
    "    )\n",
    "    \n",
    "    for word, i in word_index.items():\n",
    "        if i >= TOP_K:\n",
    "            continue\n",
    "        embedding_vector = embedding_matrix_all.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    \n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "def train_and_evaluate(output_dir, hparams):\n",
    "    \"\"\"\n",
    "    Main orchestrator. Responsible for calling all other functions in model.py\n",
    "      # Arguments:\n",
    "          output_dir: string, file path where training files will be written\n",
    "          hparams: dict, command line parameters passed from task.py\n",
    "      # Returns: nothing, kicks off training and evaluation\n",
    "    \"\"\"\n",
    "    # Ensure filewriter cache is clear for TensorBoard events file.\n",
    "    tf.compat.v1.summary.FileWriterCache.clear()\n",
    "    \n",
    "    # Load Data\n",
    "    ((train_texts, train_labels), (test_texts, test_labels)) = load_hacker_news_data(\n",
    "        hparams['train_data_path'], hparams['eval_data_path']\n",
    "    )\n",
    "    \n",
    "    # Create vocabulary from training corpus.\n",
    "    tokenizer = text.Tokenizer(num_words=TOP_K)\n",
    "    tokenizer.fit_on_texts(train_texts)\n",
    "    \n",
    "    # Save token dictionary to use during prediction time\n",
    "    pickle.dump(tokenizer, open('tokenizer.pickled', 'wb'))\n",
    "    \n",
    "    # Create estimator\n",
    "    run_config = tf.estimator.RunConfig(\n",
    "        save_checkpoints_steps=500\n",
    "    )\n",
    "    estimator = keras_estimator(\n",
    "        model_dir=output_dir,\n",
    "        config=run_config,\n",
    "        learning_rate=hparams['learning_rate'],\n",
    "        embedding_path=hparams['embedding_path'],\n",
    "        word_index=tokenizer.word_index\n",
    "    )\n",
    "    \n",
    "    # Create TrainSpec\n",
    "    train_steps = hparams['num_epochs'] * len(train_texts) / hparams['batch_size']\n",
    "    train_spec = tf.estimator.TrainSpec(\n",
    "        input_fn=input_fn(\n",
    "            train_texts,\n",
    "            train_labels,\n",
    "            tokenizer,\n",
    "            hparams['batch_size'],\n",
    "            mode=tf.estimator.ModeKeys.TRAIN\n",
    "        ),\n",
    "        max_steps=train_steps\n",
    "    )\n",
    "    \n",
    "    # Create EvalSpec\n",
    "    exporter = tf.estimator.LatestExporter(\n",
    "        name='exporter',\n",
    "        serving_input_receiver_fn=serving_input_fn\n",
    "    )\n",
    "    eval_spec = tf.estimator.EvalSpec(\n",
    "        input_fn=input_fn(\n",
    "            test_texts,\n",
    "            test_labels,\n",
    "            tokenizer,\n",
    "            hparams['batch_size'],\n",
    "            mode=tf.estimator.ModeKeys.EVAL\n",
    "        ),\n",
    "        steps=None,\n",
    "        exporters=exporter,\n",
    "        start_delay_secs=10,\n",
    "        throttle_secs=10\n",
    "    )\n",
    "    \n",
    "    # Start training\n",
    "    tf.estimator.train_and_evaluate(\n",
    "        estimator=estimator,\n",
    "        train_spec=train_spec,\n",
    "        eval_spec=eval_spec\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Locally (optional step)\n",
    "Let's make sure the code compiles by running locally for a fraction of an epoch.\n",
    "This may not work if you don't have all the packages installed locally for gcloud (such as in Colab).\n",
    "This is an optional step; move on to training on the cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-storage in /home/mujahid7292/anaconda3/lib/python3.8/site-packages (1.32.0)\n",
      "Requirement already satisfied: google-resumable-media<2.0dev,>=1.0.0 in /home/mujahid7292/anaconda3/lib/python3.8/site-packages (from google-cloud-storage) (1.1.0)\n",
      "Requirement already satisfied: google-auth<2.0dev,>=1.11.0 in /home/mujahid7292/anaconda3/lib/python3.8/site-packages (from google-cloud-storage) (1.22.1)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /home/mujahid7292/anaconda3/lib/python3.8/site-packages (from google-cloud-storage) (2.24.0)\n",
      "Requirement already satisfied: google-cloud-core<2.0dev,>=1.4.1 in /home/mujahid7292/anaconda3/lib/python3.8/site-packages (from google-cloud-storage) (1.4.3)\n",
      "Requirement already satisfied: six in /home/mujahid7292/anaconda3/lib/python3.8/site-packages (from google-resumable-media<2.0dev,>=1.0.0->google-cloud-storage) (1.15.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0; python_version >= \"3.5\" in /home/mujahid7292/anaconda3/lib/python3.8/site-packages (from google-resumable-media<2.0dev,>=1.0.0->google-cloud-storage) (1.0.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/mujahid7292/anaconda3/lib/python3.8/site-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage) (4.1.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.5\" in /home/mujahid7292/anaconda3/lib/python3.8/site-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage) (4.6)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/mujahid7292/anaconda3/lib/python3.8/site-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage) (0.2.8)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /home/mujahid7292/anaconda3/lib/python3.8/site-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage) (49.2.0.post20200714)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/mujahid7292/anaconda3/lib/python3.8/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/mujahid7292/anaconda3/lib/python3.8/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/mujahid7292/anaconda3/lib/python3.8/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/mujahid7292/anaconda3/lib/python3.8/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (1.25.9)\n",
      "Requirement already satisfied: google-api-core<2.0.0dev,>=1.19.0 in /home/mujahid7292/anaconda3/lib/python3.8/site-packages (from google-cloud-core<2.0dev,>=1.4.1->google-cloud-storage) (1.23.0)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /home/mujahid7292/anaconda3/lib/python3.8/site-packages (from google-crc32c<2.0dev,>=1.0; python_version >= \"3.5\"->google-resumable-media<2.0dev,>=1.0.0->google-cloud-storage) (1.14.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /home/mujahid7292/anaconda3/lib/python3.8/site-packages (from rsa<5,>=3.1.4; python_version >= \"3.5\"->google-auth<2.0dev,>=1.11.0->google-cloud-storage) (0.4.8)\n",
      "Requirement already satisfied: pytz in /home/mujahid7292/anaconda3/lib/python3.8/site-packages (from google-api-core<2.0.0dev,>=1.19.0->google-cloud-core<2.0dev,>=1.4.1->google-cloud-storage) (2020.1)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /home/mujahid7292/anaconda3/lib/python3.8/site-packages (from google-api-core<2.0.0dev,>=1.19.0->google-cloud-core<2.0dev,>=1.4.1->google-cloud-storage) (3.13.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /home/mujahid7292/anaconda3/lib/python3.8/site-packages (from google-api-core<2.0.0dev,>=1.19.0->google-cloud-core<2.0dev,>=1.4.1->google-cloud-storage) (1.52.0)\n",
      "Requirement already satisfied: pycparser in /home/mujahid7292/anaconda3/lib/python3.8/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0; python_version >= \"3.5\"->google-resumable-media<2.0dev,>=1.0.0->google-cloud-storage) (2.20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--output_dir=./txtcls_trained', '--train_data_path=./train.csv', '--eval_data_path=./eval.csv', '--num_epochs=5']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}\n",
      "2020-10-29 11:07:24.755336: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2020-10-29 11:07:24.777699: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2899885000 Hz\n",
      "2020-10-29 11:07:24.778099: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x557f9eae0850 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-10-29 11:07:24.778147: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2020-10-29 11:07:24.778358: I tensorflow/core/common_runtime/process_util.cc:147] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "INFO:tensorflow:Using the Keras model provided.\n",
      "INFO:tensorflow:Using config: {'_model_dir': './txtcls_trained', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mujahid7292/anaconda3/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/mujahid7292/anaconda3/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/media/mujahid7292/Data/GoogleDriveSandCorp2014/ML_With_TensorFlow_On_GCP/09.Sequence Models for Time Series and Natural Language Processing/Week_2/Lab_7_Text Classification using TensorFlow Or Keras on AI Platform/sequence/Practice/txtclsmodel/trainer/task.py\", line 60, in <module>\n",
      "    model.train_and_evaluate(output_dir, hparams)\n",
      "  File \"/media/mujahid7292/Data/GoogleDriveSandCorp2014/ML_With_TensorFlow_On_GCP/09.Sequence Models for Time Series and Natural Language Processing/Week_2/Lab_7_Text Classification using TensorFlow Or Keras on AI Platform/sequence/Practice/txtclsmodel/trainer/model.py\", line 338, in train_and_evaluate\n",
      "    input_fn=input_fn(\n",
      "  File \"/media/mujahid7292/Data/GoogleDriveSandCorp2014/ML_With_TensorFlow_On_GCP/09.Sequence Models for Time Series and Natural Language Processing/Week_2/Lab_7_Text Classification using TensorFlow Or Keras on AI Platform/sequence/Practice/txtclsmodel/trainer/model.py\", line 104, in input_fn\n",
      "    x = tokenizer.texts_to_sequnces(texts)\n",
      "AttributeError: 'Tokenizer' object has no attribute 'texts_to_sequnces'\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'pip install google-cloud-storage\\nrm -rf txtcls_trained\\ngcloud ai-platform local train \\\\\\n   --module-name=trainer.task \\\\\\n   --package-path=./txtclsmodel/trainer \\\\\\n   -- \\\\\\n   --output_dir=./txtcls_trained \\\\\\n   --train_data_path=./train.csv \\\\\\n   --eval_data_path=./eval.csv \\\\\\n   --num_epochs=5\\n'' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-1e24ed98914d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bash'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pip install google-cloud-storage\\nrm -rf txtcls_trained\\ngcloud ai-platform local train \\\\\\n   --module-name=trainer.task \\\\\\n   --package-path=./txtclsmodel/trainer \\\\\\n   -- \\\\\\n   --output_dir=./txtcls_trained \\\\\\n   --train_data_path=./train.csv \\\\\\n   --eval_data_path=./eval.csv \\\\\\n   --num_epochs=5\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2369\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2370\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2371\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2372\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mnamed_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshebang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# write a basic docstring:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-110>\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_error\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_close\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'pip install google-cloud-storage\\nrm -rf txtcls_trained\\ngcloud ai-platform local train \\\\\\n   --module-name=trainer.task \\\\\\n   --package-path=./txtclsmodel/trainer \\\\\\n   -- \\\\\\n   --output_dir=./txtcls_trained \\\\\\n   --train_data_path=./train.csv \\\\\\n   --eval_data_path=./eval.csv \\\\\\n   --num_epochs=5\\n'' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip install google-cloud-storage\n",
    "rm -rf txtcls_trained\n",
    "gcloud ai-platform local train \\\n",
    "   --module-name=trainer.task \\\n",
    "   --package-path=./txtclsmodel/trainer \\\n",
    "   -- \\\n",
    "   --output_dir=./txtcls_trained \\\n",
    "   --train_data_path=./train.csv \\\n",
    "   --eval_data_path=./eval.csv \\\n",
    "   --num_epochs=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
